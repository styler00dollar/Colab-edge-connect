{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-edge-connect.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "nsHuihTpErN4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEkV-m5FEJoM"
      },
      "source": [
        "# edge-connect with differentiable augmentation\n",
        "edge-connect: [knazeri/edge-connect](https://github.com/knazeri/edge-connect)\n",
        "\n",
        "Yukariins fork: [Yukariin/edge-connect](https://github.com/Yukariin/edge-connect)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "My fork: [styler00dollar/Colab-edge-connect](https://github.com/styler00dollar/Colab-edge-connect)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPLdOlecCkFa"
      },
      "source": [
        "# check gpu\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL5SvTn5BdBV",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-edge-connect\n",
        "!pip install numpy\n",
        "!pip install scipy==1.1\n",
        "!pip install future\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install pyaml\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsHuihTpErN4"
      },
      "source": [
        "# Test with pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW0V5LkRB4ej",
        "cellView": "form"
      },
      "source": [
        "#@title Download models\n",
        "%cd /content/Colab-edge-connect\n",
        "!pip install gdown\n",
        "!mkdir checkpoint_places\n",
        "%cd checkpoint_places\n",
        "# /checkpoints broken\n",
        "# places\n",
        "!gdown --id 1gesVuuYMtlWSQRR2JE5eO0QZHskYRfqv\n",
        "!gdown --id 1_oYnmK7kppXqka9UUsHrZB4gWE4ouSgT\n",
        "!gdown --id 1M-r_ds4VZJnUqViDMofd4-Fy8-q2aeKJ\n",
        "!gdown --id 1G8lXquU3eREfs8KorFpFC8N4YmTQRksF\n",
        "%cd ..\n",
        "!mkdir checkpoint_celeba\n",
        "%cd checkpoint_celeba\n",
        "!gdown --id 1wy0pEaXTqmya2yeLwWFmTBf4ICexCdce\n",
        "!gdown --id 1hqZRjnqZBGnSTtGJRHXEvvdGVICUGa7u\n",
        "!gdown --id 17FemN4FAKpS5-8Dos582IrOiSCZNDOAO\n",
        "!gdown --id 15mH1ZHMf83q3woBHFELr_TptSRGc5g5j\n",
        "%cd ..\n",
        "!mkdir checkpoint_street\n",
        "%cd checkpoint_street\n",
        "!gdown --id 1ORF2uN4lB3F6YndPm1ny8VIDrsWQBwUS\n",
        "!gdown --id 1EwHK8YjcpO-X3xhmeo2dtqGvtY5vOMMj\n",
        "!gdown --id 1AWxB8AwTOrlOmAUho3IQQlmawtp3y8gZ\n",
        "!gdown --id 12Ua8oQwk0iLdYgrb08bqBhfyiBIumQEK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM4GOSz3CzJl",
        "cellView": "form"
      },
      "source": [
        "#@title dummy config\n",
        "%%writefile /content/Colab-edge-connect/config.yml\n",
        "MODE: 1             # 1: train, 2: test, 3: eval\n",
        "MODEL: 1            # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
        "MASK: 3             # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
        "EDGE: 1             # 1: canny, 2: external\n",
        "NMS: 1              # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
        "SEED: 10            # random seed\n",
        "GPU: [0]            # list of gpu ids\n",
        "DEBUG: 0            # turns on debugging mode\n",
        "VERBOSE: 0          # turns on verbose mode in the output console\n",
        "\n",
        "TRAIN_FLIST: ./datasets/places2_train.flist\n",
        "VAL_FLIST: ./datasets/places2_val.flist\n",
        "TEST_FLIST: ./datasets/places2_test.flist\n",
        "\n",
        "TRAIN_EDGE_FLIST: ./datasets/places2_edges_train.flist\n",
        "VAL_EDGE_FLIST: ./datasets/places2_edges_val.flist\n",
        "TEST_EDGE_FLIST: ./datasets/places2_edges_test.flist\n",
        "\n",
        "TRAIN_MASK_FLIST: ./datasets/masks_train.flist\n",
        "VAL_MASK_FLIST: ./datasets/masks_val.flist\n",
        "TEST_MASK_FLIST: ./datasets/masks_test.flist\n",
        "\n",
        "LR: 0.0001                    # learning rate\n",
        "D2G_LR: 0.1                   # discriminator/generator learning rate ratio\n",
        "BETA1: 0.0                    # adam optimizer beta1\n",
        "BETA2: 0.9                    # adam optimizer beta2\n",
        "BATCH_SIZE: 8                 # input batch size for training\n",
        "INPUT_SIZE: 256               # input image size for training 0 for original size\n",
        "SIGMA: 2                      # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
        "MAX_ITERS: 2e6                # maximum number of iterations to train the model\n",
        "\n",
        "EDGE_THRESHOLD: 0.5           # edge detection threshold\n",
        "L1_LOSS_WEIGHT: 1             # l1 loss weight\n",
        "FM_LOSS_WEIGHT: 10            # feature-matching loss weight\n",
        "STYLE_LOSS_WEIGHT: 250        # style loss weight\n",
        "CONTENT_LOSS_WEIGHT: 0.1      # perceptual loss weight\n",
        "INPAINT_ADV_LOSS_WEIGHT: 0.1  # adversarial loss weight\n",
        "\n",
        "GAN_LOSS: nsgan               # nsgan | lsgan | hinge\n",
        "GAN_POOL_SIZE: 0              # fake images pool size\n",
        "\n",
        "SAVE_INTERVAL: 1000           # how many iterations to wait before saving model (0: never)\n",
        "SAMPLE_INTERVAL: 1000         # how many iterations to wait before sampling (0: never)\n",
        "SAMPLE_SIZE: 12               # number of images to sample\n",
        "EVAL_INTERVAL: 0              # how many iterations to wait before model evaluation (0: never)\n",
        "LOG_INTERVAL: 10              # how many iterations to wait before logging training status (0: never)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29t2_4dwGVPM"
      },
      "source": [
        "Currently default paths are ```/content/image.png``` and ```/content/mask.png```. Currently it's not supported that you change paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgnGR9NAUqJg",
        "cellView": "form"
      },
      "source": [
        "#@title Image and mask need to be dividable by 4, this code does fix wrong images \n",
        "import cv2\n",
        "import numpy\n",
        "path_inpainting = '/content/image.png' #@param {type:\"string\"}\n",
        "path_mask = '/content/mask.png' #@param {type:\"string\"}\n",
        "image=cv2.imread(path_mask)\n",
        "image_size0 = numpy.floor(image.shape[0]/4)\n",
        "image_size1 = numpy.floor(image.shape[1]/4)\n",
        "image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "ret,image=cv2.threshold(image,254,255,cv2.THRESH_BINARY)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_mask, image)\n",
        "\n",
        "image=cv2.imread(path_inpainting)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_inpainting, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WHBATxfdl-",
        "cellView": "form"
      },
      "source": [
        "#@title print shape\n",
        "import cv2\n",
        "image = cv2.imread(path_inpainting)\n",
        "print(image.shape)\n",
        "image = cv2.imread(path_mask)\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvHUtBwBri8",
        "cellView": "form"
      },
      "source": [
        "#@title Test Inpainting (result will be ```image.png```, the same filename you used as input)\n",
        "%cd /content/Colab-edge-connect\n",
        "!python test.py \\\n",
        "  --model 3 \\\n",
        "  --checkpoints /content/Colab-edge-connect/checkpoint_places \\\n",
        "  --input /content/image.png \\\n",
        "  --mask /content/mask.png \\\n",
        "  --output /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ard4kblETXdL"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpN03bmG3Ja"
      },
      "source": [
        "Interesting stuff:\n",
        "- New pytorch versions won't work with the original code. This fork fixes it.\n",
        "- The ```.tflist``` simply lists filepaths for images.\n",
        "- It supports blocks as inpainting method, but random/custom masks need to be manually downloaded and input with a ```.tflist``` as well. Two example datasets are linked in the original github.\n",
        "- [Model 4 is not recommended](https://github.com/knazeri/edge-connect/issues/144). You should probably use model 3.\n",
        "- [Resuming and using a model as pretrained is being done by simply starting training while the models are in the specified checkpoint path.](https://github.com/knazeri/edge-connect/issues/54). Just make sure to use the default names. Further info in my fork README.\n",
        "- Do not use Style loss + AMP. Will result in Nan errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSPXmERCWpJ4",
        "cellView": "form"
      },
      "source": [
        "#@title Create empty folders\n",
        "!mkdir /content/training-checkpoints/\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/\n",
        "!mkdir /content/mask_train/\n",
        "!mkdir /content/mask_val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H--bAmNAJdms"
      },
      "source": [
        "Input all your data.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKhSZMdoH0WM",
        "cellView": "form"
      },
      "source": [
        "#@title modify paths inside prepare.py to create file list\n",
        "%%writefile /content/Colab-edge-connect/prepare.py\n",
        "#!/usr/bin/python\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from random import shuffle\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--is_shuffled', default='1', type=int,\n",
        "                    help='Needed to shuffle')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_filename = 'train.flist'\n",
        "    validation_filename = 'val.flist'\n",
        "\n",
        "    train_path = '/home/path/'\n",
        "    val_path = '/home/path/'\n",
        "\n",
        "    training_file_names = []\n",
        "    validation_file_names = []\n",
        "\n",
        "    training_folder = os.listdir(train_path)\n",
        "\n",
        "    for training_item in training_folder:\n",
        "        training_item = train_path + \"/\" + training_item\n",
        "        training_file_names.append(training_item)\n",
        "\n",
        "    validation_folder = os.listdir(val_path)\n",
        "\n",
        "    for validation_item in validation_folder:\n",
        "        validation_item = val_path + \"/\" + validation_item\n",
        "        validation_file_names.append(validation_item)\n",
        "\n",
        "    # print all file paths\n",
        "    for i in training_file_names:\n",
        "        print(i)\n",
        "    for i in validation_file_names:\n",
        "        print(i)\n",
        "\n",
        "    # This would print all the files and directories\n",
        "\n",
        "    # shuffle file names if set\n",
        "    if args.is_shuffled == 1:\n",
        "        shuffle(training_file_names)\n",
        "        shuffle(validation_file_names)\n",
        "\n",
        "    # make output file if not existed\n",
        "    if not os.path.exists(train_filename):\n",
        "        os.mknod(train_filename)\n",
        "\n",
        "    if not os.path.exists(validation_filename):\n",
        "        os.mknod(validation_filename)\n",
        "\n",
        "    # write to file\n",
        "    fo = open(train_filename, \"w\")\n",
        "    fo.write(\"\\n\".join(training_file_names))\n",
        "    fo.close()\n",
        "\n",
        "    fo = open(validation_filename, \"w\")\n",
        "    fo.write(\"\\n\".join(validation_file_names))\n",
        "    fo.close()\n",
        "\n",
        "    # print process\n",
        "    print(\"Written file is: \", train_filename, \", is_shuffle: \", args.is_shuffled)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkRkDL9zH9NY",
        "cellView": "form"
      },
      "source": [
        "#@title create file list\n",
        "%cd /content/Colab-edge-connect\n",
        "!python prepare.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tglKPF-IesFt",
        "cellView": "form"
      },
      "source": [
        "#@title Training config\n",
        "%%writefile /content/training-checkpoints/config.yml\n",
        "MODE: 1             # 1: train, 2: test, 3: eval\n",
        "MODEL: 3            # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
        "MASK: 5             # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
        "EDGE: 1             # 1: canny, 2: external\n",
        "NMS: 1              # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
        "SEED: 10            # random seed\n",
        "GPU: [0]            # list of gpu ids\n",
        "DEBUG: 0            # turns on debugging mode\n",
        "VERBOSE: 0          # turns on verbose mode in the output console\n",
        "\n",
        "# input\n",
        "# edge-connect default: 'input_size', 'center_crop'\n",
        "RESIZE_MODE: 'random_resize'     # 'input_size', 'random_resize', 'input_size_and_random_downscale' # keeps aspect ratio\n",
        "CROP_MODE: 'random_crop'      # 'random_crop', 'center_crop' # center_crop assumes that the smaller axis is the INPUT_SIZE. \n",
        "FLIP_MODE: ['horizontal_flip', 'vertical_flip'] # ['horizontal_flip', 'vertical_flip']\n",
        "# ratio\n",
        "INPUT_SIZE_AND_RANDOM_DOWNSCALE_RATIO: 0.5\n",
        "HORIZONTAL_FLIP_RATIO: 0.5\n",
        "VERTICAL_FLIP_RATIO: 0.5\n",
        "\n",
        "MOSAIC_TEST: 0                # Currently experimental. Inputs randomly resized image, which is overlayed with mask to the generator. Preview images during training won't show the real inputs. Currently 256x256 hardcoded.\n",
        "USE_AMP: 1                    # Mixed precision training. Currently experimental. Will show a lot of Nan/Inf errors, but it seems to train fine.\n",
        "\n",
        "BATCH_SIZE: 1                 # input batch size for training\n",
        "INPUT_SIZE: 256               # input image size for training 0 for original size\n",
        "\n",
        "# Original Default: ['Adversarial', 'Perceptual', 'Style', 'DEFAULT_L1', 'DEFAULT_GAN']\n",
        "DISCRIMINATOR: 'pixel'       # default, pixel, patch\n",
        "DISCRIMINATOR_CALC: 'MSELoss'  # None, BCEWithLogitsLoss, MSELoss\n",
        "GENERATOR_CALC: 'MSELoss'      # None, BCEWithLogitsLoss, MSELoss\n",
        "\n",
        "# Options: ['Adversarial', 'Perceptual', 'Style', 'DEFAULT_L1', 'NEW_L1', 'DEFAULT_GAN', 'NEW_GAN', 'HFEN', 'TV', 'ElasticLoss', 'RelativeL1', 'L1CosineSim', 'ClipL1', 'FFT', 'OF', 'GP', 'CP', 'Contextual']\n",
        "# Warning: Currently, AMP + Style will result in Nan errors\n",
        "GENERATOR_LOSS: ['Adversarial', 'Perceptual', 'DEFAULT_L1', 'NEW_L1', 'DEFAULT_GAN', 'HFEN', 'TV', 'ElasticLoss', 'RelativeL1', 'L1CosineSim', 'ClipL1', 'FFT', 'OF', 'GP', 'CP', 'Contextual'] \n",
        "\n",
        "# default loss\n",
        "INPAINT_ADV_LOSS_WEIGHT: 0.1  # adversarial loss weight # maybe depricated\n",
        "CONTENT_LOSS_WEIGHT: 0.1      # perceptual loss weight\n",
        "STYLE_LOSS_WEIGHT: 250        # style loss weight\n",
        "L1_LOSS_WEIGHT: 1             # l1 loss weight\n",
        "\n",
        "DEFAULT_GAN_LOSS: nsgan       # nsgan | lsgan | hinge\n",
        "GAN_POOL_SIZE: 0              # fake images pool size\n",
        "\n",
        "# new loss functions (values not tested)\n",
        "DISCRIMINATOR_FAKE_LOSS_WEIGHT: 0.1\n",
        "DISCRIMINATOR_REAL_LOSS_WEIGHT: 0.1\n",
        "GENERATOR_CALC_WEIGHT: 0.1\n",
        "\n",
        "NEW_GAN_WEIGHT: 5e-3\n",
        "NEW_GAN_LOSS: 'hinge'         #vanilla, lsgan, srpgan, nsgan, BCE, hinge, wgan-gp (Only hinge seems not to crash, added GAN does not seem to work properly. This option is not really recommended.)\n",
        "\n",
        "L1_WEIGHT: 0.01               # using new L1\n",
        "HFEN_WEIGHT: 0.1              # high frequency error norm (HFEN) weight\n",
        "TV_WEIGHT: 0.000001           # total variation loss weight\n",
        "ElasticLoss_WEIGHT: 0.01\n",
        "RelativeL1_WEIGHT: 0.01 \n",
        "L1CosineSim_WEIGHT: 0.01 \n",
        "ClipL1_WEIGHT: 0.01 \n",
        "FFT_WEIGHT: 0.01 \n",
        "OF_WEIGHT: 0.01               # Overflow loss weight\n",
        "GP_WEIGHT: 0.01               # Gradient Profile (GP) loss weight\n",
        "CP_WEIGHT: 0.01               # Color Profile (CP) loss weight\n",
        "Contextual_WEIGHT: 10.01 \n",
        "HFEN_TYPE: 'Charbonnier' #L1, MSE, Charbonnier, Elastic, Relative, L1CosineSim\n",
        "\n",
        "\n",
        "TRAIN_FLIST: /content/train/train.tflist\n",
        "VAL_FLIST: /content/val/val.tflist\n",
        "TEST_FLIST: /content/val/val.tflist\n",
        "\n",
        "TRAIN_EDGE_FLIST: NULL\n",
        "VAL_EDGE_FLIST: NULL\n",
        "TEST_EDGE_FLIST: NULL\n",
        "\n",
        "TRAIN_MASK_FLIST: /content/mask_train/mask_train.tflist\n",
        "VAL_MASK_FLIST: /content/mask_val/mask_val.tflist\n",
        "TEST_MASK_FLIST: /content/mask_val/mask_val.tflist\n",
        "\n",
        "LR: 0.0001                    # learning rate\n",
        "D2G_LR: 0.1                   # discriminator/generator learning rate ratio\n",
        "BETA1: 0.0                    # adam optimizer beta1\n",
        "BETA2: 0.9                    # adam optimizer beta2\n",
        "SIGMA: 2                      # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
        "MAX_ITERS: 1000000            # maximum number of iterations to train the model\n",
        "\n",
        "EDGE_THRESHOLD: 0.5           # edge detection threshold\n",
        "FM_LOSS_WEIGHT: 10            # feature-matching loss weight (used in EdgeModel)\n",
        "\n",
        "\n",
        "# saving\n",
        "SAVE_INTERVAL: 5000           # how many iterations to wait before saving model (0: never)\n",
        "SAMPLE_INTERVAL: 500          # how many iterations to wait before sampling (0: never)\n",
        "SAMPLE_SIZE: 1                # number of images to sample\n",
        "EVAL_INTERVAL: 0              # how many iterations to wait before model evaluation (0: never)\n",
        "LOG_INTERVAL: 1              # how many iterations to wait before logging training status (0: never)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUxqJC30WlZW"
      },
      "source": [
        "# Train model\n",
        "%cd /content/Colab-edge-connect\n",
        "!python train.py --model 3 --checkpoints /content/training-checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaRu1BLcJah"
      },
      "source": [
        "- This will overwrite your original ```image.png```\n",
        "- Make sure the dimension is dividable by 4.\n",
        "- The default filenames for models are ```InpaintingModel_dis.pth``` and ```InpaintingModel_gen.pth```. Other names will be ignored.\n",
        "- You may or may not need to flip the image. The needed code is below.\n",
        "- ```image.png``` will be overwritten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKarRtcpc3CY",
        "cellView": "form"
      },
      "source": [
        "#@title Image and mask need to be dividable by 4, this code does fix wrong images \n",
        "import cv2\n",
        "import numpy\n",
        "path_inpainting = '/content/image.png' #@param {type:\"string\"}\n",
        "path_mask = '/content/mask.png' #@param {type:\"string\"}\n",
        "image=cv2.imread(path_mask)\n",
        "image_size0 = numpy.floor(image.shape[0]/4)\n",
        "image_size1 = numpy.floor(image.shape[1]/4)\n",
        "image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "ret,image=cv2.threshold(image,254,255,cv2.THRESH_BINARY)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_mask, image)\n",
        "\n",
        "image=cv2.imread(path_inpainting)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_inpainting, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Xshcidb0rr",
        "cellView": "form"
      },
      "source": [
        "#@title Flip image\n",
        "import cv2\n",
        "filename = '/content/image.png' #@param {type:\"string\"}\n",
        "image = cv2.imread(filename)\n",
        "image = cv2.flip(image, 1)\n",
        "cv2.imwrite(filename, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InsJhcpA6Bhc",
        "cellView": "form"
      },
      "source": [
        "#@title Test model\n",
        "%cd /content/Colab-edge-connect\n",
        "checkpoints = '/content/training-checkpoints/' #@param {type:\"string\"}\n",
        "input = '/content/image.png' #@param {type:\"string\"}\n",
        "mask = '/content/mask.png' #@param {type:\"string\"}\n",
        "output = '/content/' #@param {type:\"string\"}\n",
        "!python test.py \\\n",
        "  --model 3 \\\n",
        "  --checkpoints {checkpoints} \\\n",
        "  --input {input} \\\n",
        "  --mask {mask} \\\n",
        "  --output {output}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1bri1_MiQs7"
      },
      "source": [
        "-----------------------------------------------------\n",
        "Splitted files (Depricated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFEUgOiaiTfy"
      },
      "source": [
        "!mkdir /content/Colab-edge-connect/src/vic/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO1C5GiliSIE",
        "cellView": "form"
      },
      "source": [
        "#@title vic/loss.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/loss.py\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/loss.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/loss.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#TODO: change this file to loss_fns.py?\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#import pdb\n",
        "\n",
        "#from models.modules.architectures.perceptual import VGG_Model\n",
        "from .perceptual import VGG_Model\n",
        "\n",
        "#from models.modules.architectures.video import optical_flow_warp\n",
        "\n",
        "from .filters import *\n",
        "from .colors import *\n",
        "from .common import norm, denorm\n",
        "\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = y.size()\n",
        "        loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
        "        return loss/(c*b*h*w)\n",
        "    \n",
        "\n",
        "# Define GAN loss: [vanilla | lsgan | wgan-gp | srpgan/nsgan | hinge]\n",
        "# https://tuatini.me/creating-and-shipping-deep-learning-models-into-production/\n",
        "class GANLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Adversarial loss\n",
        "    https://arxiv.org/abs/1711.10337\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.gan_type = gan_type.lower()\n",
        "        self.real_label_val = real_label_val\n",
        "        self.fake_label_val = fake_label_val\n",
        "\n",
        "        if self.gan_type == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.gan_type == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.gan_type == 'srpgan' or self.gan_type == 'nsgan':\n",
        "            self.loss = nn.BCELoss()\n",
        "        elif self.gan_type == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        elif self.gan_type == 'wgan-gp':\n",
        "\n",
        "            def wgan_loss(input, target):\n",
        "                # target is boolean\n",
        "                return -1 * input.mean() if target else input.mean()\n",
        "\n",
        "            self.loss = wgan_loss\n",
        "        else:\n",
        "            raise NotImplementedError('GAN type [{:s}] is not found'.format(self.gan_type))\n",
        "\n",
        "    def get_target_label(self, input, target_is_real):\n",
        "        if self.gan_type == 'wgan-gp':\n",
        "            return target_is_real\n",
        "        if target_is_real:\n",
        "            return torch.empty_like(input).fill_(self.real_label_val) #torch.ones_like(d_sr_out)\n",
        "        else:\n",
        "            return torch.empty_like(input).fill_(self.fake_label_val) #torch.zeros_like(d_sr_out)\n",
        "\n",
        "    def forward(self, input, target_is_real, is_disc = None):\n",
        "        if self.gan_type == 'hinge': #TODO: test\n",
        "            if is_disc:\n",
        "                input = -input if target_is_real else input\n",
        "                return self.loss(1 + input).mean()\n",
        "            else:\n",
        "                return (-input).mean()\n",
        "        else:\n",
        "            target_label = self.get_target_label(input, target_is_real)\n",
        "            loss = self.loss(input, target_label)\n",
        "            return loss\n",
        "\n",
        "\n",
        "class GradientPenaltyLoss(nn.Module):\n",
        "    def __init__(self, device=torch.device('cpu')):\n",
        "        super(GradientPenaltyLoss, self).__init__()\n",
        "        self.register_buffer('grad_outputs', torch.Tensor())\n",
        "        self.grad_outputs = self.grad_outputs.to(device)\n",
        "\n",
        "    def get_grad_outputs(self, input):\n",
        "        if self.grad_outputs.size() != input.size():\n",
        "            self.grad_outputs.resize_(input.size()).fill_(1.0)\n",
        "        return self.grad_outputs\n",
        "\n",
        "    def forward(self, interp, interp_crit):\n",
        "        grad_outputs = self.get_grad_outputs(interp_crit)\n",
        "        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp, \\\n",
        "            grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n",
        "        grad_interp_norm = grad_interp.norm(2, dim=1)\n",
        "\n",
        "        loss = ((grad_interp_norm - 1)**2).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class HFENLoss(nn.Module): # Edge loss with pre_smooth\n",
        "    \"\"\"Calculates high frequency error norm (HFEN) between target and \n",
        "     prediction used to quantify the quality of reconstruction of edges \n",
        "     and fine features. \n",
        "     \n",
        "     Uses a rotationally symmetric LoG (Laplacian of Gaussian) filter to \n",
        "     capture edges. The original filter kernel is of size 15×15 pixels, \n",
        "     and has a standard deviation of 1.5 pixels.\n",
        "     ks = 2 * int(truncate * sigma + 0.5) + 1, so use truncate=4.5\n",
        "     \n",
        "     HFEN is computed as the norm of the result obtained by LoG filtering the \n",
        "     difference between the reconstructed and reference images.\n",
        "\n",
        "    [1]: Ravishankar and Bresler: MR Image Reconstruction From Highly\n",
        "    Undersampled k-Space Data by Dictionary Learning, 2011\n",
        "        https://ieeexplore.ieee.org/document/5617283\n",
        "    [2]: Han et al: Image Reconstruction Using Analysis Model Prior, 2016\n",
        "        https://www.hindawi.com/journals/cmmm/2016/7571934/\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    img1 : torch.Tensor or torch.autograd.Variable\n",
        "        Predicted image\n",
        "    img2 : torch.Tensor or torch.autograd.Variable\n",
        "        Target image\n",
        "    norm: if true, follows [2], who define a normalized version of HFEN.\n",
        "        If using RelativeL1 criterion, it's already normalized. \n",
        "    \"\"\"\n",
        "    def __init__(self, loss_f=None, kernel='log', kernel_size=15, sigma = 2.5, norm = False): #1.4 ~ 1.5\n",
        "        super(HFENLoss, self).__init__()\n",
        "        # can use different criteria\n",
        "        self.criterion = loss_f\n",
        "        self.norm = norm\n",
        "        #can use different kernels like DoG instead:\n",
        "        if kernel == 'dog':\n",
        "            kernel = get_dog_kernel(kernel_size, sigma)\n",
        "        else:\n",
        "            kernel = get_log_kernel(kernel_size, sigma)\n",
        "        self.filter = load_filter(kernel=kernel, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        self.filter.to(img1.device)\n",
        "        # HFEN loss\n",
        "        log1 = self.filter(img1)\n",
        "        log2 = self.filter(img2)\n",
        "        hfen_loss = self.criterion(log1, log2)\n",
        "        if self.norm:\n",
        "            hfen_loss /= img2.norm()\n",
        "        return hfen_loss\n",
        "\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_type='tv', p = 1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        assert p in [1, 2]\n",
        "        self.p = p\n",
        "        self.tv_type = tv_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        img_shape = x.shape\n",
        "        if len(img_shape) == 3 or len(img_shape) == 4:\n",
        "            if self.tv_type == 'dtv':\n",
        "                dy, dx, dp, dn  = get_4dim_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = (dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes) + dp.abs().sum(dim=reduce_axes) + dn.abs().sum(dim=reduce_axes)) # Calculates the TV loss for each image in the batch\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes) + torch.pow(dp,2).sum(dim=reduce_axes) + torch.pow(dn,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/(2.0*batch_size) # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "            else: #'tv'\n",
        "                dy, dx  = get_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes)\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/batch_size # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "        else:\n",
        "            raise ValueError(\"Expected input tensor to be of ndim 3 or 4, but got \" + str(len(img_shape)))\n",
        "    \n",
        "\n",
        "class GradientLoss(nn.Module):\n",
        "    def __init__(self, loss_f = None, reduction='mean', gradientdir='2d'): #2d or 4d\n",
        "        super(GradientLoss, self).__init__()\n",
        "        self.criterion = loss_f\n",
        "        self.gradientdir = gradientdir\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        if self.gradientdir == '4d':\n",
        "            inputdy, inputdx, inputdp, inputdn = get_4dim_image_gradients(input)\n",
        "            targetdy, targetdx, targetdp, targetdn = get_4dim_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy) + \\\n",
        "                    self.criterion(inputdp, targetdp) + self.criterion(inputdn, targetdn))/4\n",
        "        else: #'2d'\n",
        "            inputdy, inputdx = get_image_gradients(input)\n",
        "            targetdy, targetdx = get_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy))/2\n",
        "\n",
        "\n",
        "class ElasticLoss(nn.Module):\n",
        "    def __init__(self, a=0.2, reduction='mean'): #a=0.5 default\n",
        "        super(ElasticLoss, self).__init__()\n",
        "        self.alpha = torch.FloatTensor([a, 1 - a]) #.to('cuda:0')\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if not isinstance(input, tuple):\n",
        "            input = (input,)\n",
        "\n",
        "        for i in range(len(input)):\n",
        "            l2 = F.mse_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[0])\n",
        "            l1 = F.l1_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[1])\n",
        "            loss = l1 + l2\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#TODO: change to RelativeNorm and set criterion as an input argument, could be any basic criterion\n",
        "class RelativeL1(nn.Module):\n",
        "    '''\n",
        "    Comparing to the regular L1, introducing the division by |c|+epsilon \n",
        "    better models the human vision system’s sensitivity to variations\n",
        "    in the dark areas. (where epsilon = 0.01, to prevent values of 0 in the\n",
        "    denominator)\n",
        "    '''\n",
        "    def __init__(self, eps=.01, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.criterion = torch.nn.L1Loss(reduction=reduction)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        base = target + self.eps\n",
        "        return self.criterion(input/base, target/base)\n",
        "\n",
        "\n",
        "class L1CosineSim(nn.Module):\n",
        "    '''\n",
        "    https://github.com/dmarnerides/hdr-expandnet/blob/master/train.py\n",
        "    Can be used to replace L1 pixel loss, but includes a cosine similarity term \n",
        "    to ensure color correctness of the RGB vectors of each pixel.\n",
        "    lambda is a constant factor that adjusts the contribution of the cosine similarity term\n",
        "    It provides improved color stability, especially for low luminance values, which\n",
        "    are frequent in HDR images, since slight variations in any of the RGB components of these \n",
        "    low values do not contribute much totheL1loss, but they may however cause noticeable \n",
        "    color shifts. More in the paper: https://arxiv.org/pdf/1803.02266.pdf\n",
        "    '''\n",
        "    def __init__(self, loss_lambda=5, reduction='mean'):\n",
        "        super(L1CosineSim, self).__init__()\n",
        "        self.similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-20)\n",
        "        self.l1_loss = nn.L1Loss(reduction=reduction)\n",
        "        self.loss_lambda = loss_lambda\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        cosine_term = (1 - self.similarity(x, y)).mean()\n",
        "        return self.l1_loss(x, y) + self.loss_lambda * cosine_term\n",
        "\n",
        "\n",
        "class ClipL1(nn.Module):\n",
        "    '''\n",
        "    Clip L1 loss\n",
        "    From: https://github.com/HolmesShuan/AIM2020-Real-Super-Resolution/\n",
        "    ClipL1 Loss combines Clip function and L1 loss. self.clip_min sets the \n",
        "    gradients of well-trained pixels to zeros and clip_max works as a noise filter.\n",
        "    data range [0, 255]: (clip_min=0.0, clip_max=10.0), \n",
        "    for [0,1] set clip_min to 1/255=0.003921.\n",
        "    '''\n",
        "    def __init__(self, clip_min=0.0, clip_max=10.0):\n",
        "        super(ClipL1, self).__init__()\n",
        "        self.clip_max = clip_max\n",
        "        self.clip_min = clip_min\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        loss = torch.mean(torch.clamp(torch.abs(sr-hr), self.clip_min, self.clip_max))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MaskedL1Loss(nn.Module):\n",
        "    r\"\"\"Masked L1 loss constructor.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MaskedL1Loss, self, normalize_over_valid=False).__init__()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.normalize_over_valid = normalize_over_valid\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        r\"\"\"Masked L1 loss computation.\n",
        "        Args:\n",
        "            input (tensor): Input tensor.\n",
        "            target (tensor): Target tensor.\n",
        "            mask (tensor): Mask to be applied to the output loss.\n",
        "        Returns:\n",
        "            (tensor): Loss value.\n",
        "        \"\"\"\n",
        "        mask = mask.expand_as(input)\n",
        "        loss = self.criterion(input * mask, target * mask)\n",
        "        if self.normalize_over_valid:\n",
        "            # The loss has been averaged over all pixels.\n",
        "            # Only average over regions which are valid.\n",
        "            loss = loss * torch.numel(mask) / (torch.sum(mask) + 1e-6)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MultiscalePixelLoss(nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss(), scale = 5):\n",
        "        super(MultiscalePixelLoss, self).__init__()\n",
        "        self.criterion = loss_f\n",
        "        self.downsample = nn.AvgPool2d(2, stride=2, count_include_pad=False)\n",
        "        self.weights = [1, 0.5, 0.25, 0.125, 0.125]\n",
        "        self.weights = self.weights[:scale]\n",
        "\n",
        "    def forward(self, input, target, mask=None):\n",
        "        loss = 0\n",
        "        if mask is not None:\n",
        "            mask = mask.expand(-1, input.size()[1], -1, -1)\n",
        "        for i in range(len(self.weights)):\n",
        "            if mask is not None:\n",
        "                loss += self.weights[i] * self.criterion(input * mask, target * mask)\n",
        "            else:\n",
        "                loss += self.weights[i] * self.criterion(input, target)\n",
        "            if i != len(self.weights) - 1:\n",
        "                input = self.downsample(input)\n",
        "                target = self.downsample(target)\n",
        "                if mask is not None:\n",
        "                    mask = self.downsample(mask)\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Frequency loss \n",
        "# https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/fft.py\n",
        "class FFTloss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean'):\n",
        "        super(FFTloss, self).__init__()\n",
        "        self.criterion = loss_f(reduction=reduction)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        zeros=torch.zeros(img1.size()).to(img1.device)\n",
        "        return self.criterion(torch.fft(torch.stack((img1,zeros),-1),2),torch.fft(torch.stack((img2,zeros),-1),2))\n",
        "\n",
        "\n",
        "class OFLoss(torch.nn.Module):\n",
        "    '''\n",
        "    Overflow loss\n",
        "    Only use if the image range is in [0,1]. (This solves the SPL brightness problem\n",
        "    and can be useful in other cases as well)\n",
        "    https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/brelu.py\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(OFLoss, self).__init__()\n",
        "\n",
        "    def forward(self, img1):\n",
        "        img_clamp = img1.clamp(0,1)\n",
        "        b,c,h,w = img1.shape\n",
        "        return torch.log((img1 - img_clamp).abs() + 1).sum()/b/c/h/w\n",
        "\n",
        "\"\"\"\n",
        "class OFR_loss(torch.nn.Module):\n",
        "    '''\n",
        "    Optical flow reconstruction loss (for video)\n",
        "    https://github.com/LongguangWang/SOF-VSR/blob/master/TIP/data_utils.py\n",
        "    '''\n",
        "    def __init__(self, reg_weight=0.1):\n",
        "        super(OFR_loss, self).__init__()\n",
        "        self.regularization = L1_regularization()\n",
        "        self.reg_weight = reg_weight #lambda3\n",
        "\n",
        "    def forward(self, x0, x1, optical_flow):\n",
        "        warped = optical_flow_warp(x0, optical_flow)\n",
        "        loss = torch.mean(torch.abs(x1 - warped)) + self.reg_weight * self.regularization(optical_flow)\n",
        "        return loss\n",
        "\"\"\"\n",
        "class L1_regularization(torch.nn.Module):\n",
        "    # TODO: This is TVLoss/regularization, modify to reuse existing loss. Used by OFR_loss()\n",
        "    def __init__(self):\n",
        "        super(L1_regularization, self).__init__()\n",
        "\n",
        "    def forward(self, image):\n",
        "        b, _, h, w = image.size()\n",
        "        reg_x_1 = image[:, :, 0:h-1, 0:w-1] - image[:, :, 1:, 0:w-1]\n",
        "        reg_y_1 = image[:, :, 0:h-1, 0:w-1] - image[:, :, 0:h-1, 1:]\n",
        "        reg_L1 = torch.abs(reg_x_1) + torch.abs(reg_y_1)\n",
        "        return torch.sum(reg_L1) / (b*(h-1)*(w-1))\n",
        "\n",
        "\n",
        "\n",
        "#TODO: testing\n",
        "# Color loss \n",
        "class ColorLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(ColorLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "#TODO: testing\n",
        "# Averaging Downscale loss \n",
        "class AverageLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(AverageLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Spatial Profile Loss\n",
        "########################\n",
        "\n",
        "class GPLoss(nn.Module):\n",
        "    '''\n",
        "    https://github.com/ssarfraz/SPL/blob/master/SPL_Loss/\n",
        "    Gradient Profile (GP) loss\n",
        "    The image gradients in each channel can easily be computed \n",
        "    by simple 1-pixel shifted image differences from itself. \n",
        "    '''\n",
        "    def __init__(self, trace=False, spl_denorm=False):\n",
        "        super(GPLoss, self).__init__()\n",
        "        self.spl_denorm = spl_denorm\n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm == True:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        input_h, input_v = get_image_gradients(input)\n",
        "        ref_h, ref_v = get_image_gradients(reference)\n",
        "\n",
        "        trace_v = self.trace(input_v,ref_v)\n",
        "        trace_h = self.trace(input_h,ref_h)\n",
        "        return trace_v + trace_h\n",
        "\n",
        "class CPLoss(nn.Module):\n",
        "    '''\n",
        "    Color Profile (CP) loss\n",
        "    '''\n",
        "    def __init__(self, rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False):\n",
        "        super(CPLoss, self).__init__()\n",
        "        self.rgb = rgb\n",
        "        self.yuv = yuv\n",
        "        self.yuvgrad = yuvgrad\n",
        "        self.spl_denorm = spl_denorm\n",
        "        self.yuv_denorm = yuv_denorm\n",
        "        \n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "            self.trace_YUV = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "            self.trace_YUV = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # self.spl_denorm=False when your inputs and outputs are in [0,1] range already\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        total_loss= 0\n",
        "        if self.rgb:\n",
        "            total_loss += self.trace(input,reference)\n",
        "        if self.yuv:\n",
        "            # rgb_to_yuv() needs images in [0,1] range to work\n",
        "            if not self.spl_denorm and self.yuv_denorm:\n",
        "                input = denorm(input)\n",
        "                reference = denorm(reference)\n",
        "            input_yuv = rgb_to_yuv(input)\n",
        "            reference_yuv = rgb_to_yuv(reference)\n",
        "            total_loss += self.trace(input_yuv,reference_yuv)\n",
        "        if self.yuvgrad:\n",
        "            input_h, input_v = get_image_gradients(input_yuv)\n",
        "            ref_h, ref_v = get_image_gradients(reference_yuv)\n",
        "\n",
        "            total_loss +=  self.trace(input_v,ref_v)\n",
        "            total_loss +=  self.trace(input_h,ref_h)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "## Spatial Profile Loss (SPL) with trace\n",
        "class SPL_ComputeWithTrace(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial Profile Loss (SPL)\n",
        "    Both loss versions equate to the cosine similarity of rows/columns. \n",
        "    'SPL_ComputeWithTrace()' uses the trace (sum over the diagonal) of matrix multiplication \n",
        "    of L2-normalized input/target rows/columns.\n",
        "    Slow implementation of the trace loss using the same formula as stated in the paper. \n",
        "    In principle, we compute the loss between a source and target image by considering such \n",
        "    pattern differences along the image x and y-directions. Considering a row or a column \n",
        "    spatial profile of an image as a vector, we can compute the similarity between them in \n",
        "    this induced vector space. Formally, this similarity is measured over each image channel ’c’.\n",
        "    The first term computes similarity among row profiles and the second among column profiles \n",
        "    of an image pair (x, y) of size H ×W. These image pixels profiles are L2-normalized to \n",
        "    have a normalized cosine similarity loss.\n",
        "    \"\"\"\n",
        "    def __init__(self,weight = [1.,1.,1.]): # The variable 'weight' was originally intended to weigh color channels differently. In our experiments, we found that an equal weight between all channels gives the best results. As such, this variable is a leftover from that time and can be removed.\n",
        "        super(SPL_ComputeWithTrace, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = 0\n",
        "        b = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(input.shape[1]):\n",
        "                a += torch.trace(torch.matmul(F.normalize(input[i,j,:,:],p=2,dim=1),torch.t(F.normalize(reference[i,j,:,:],p=2,dim=1))))/input.shape[2]*self.weight[j]\n",
        "                b += torch.trace(torch.matmul(torch.t(F.normalize(input[i,j,:,:],p=2,dim=0)),F.normalize(reference[i,j,:,:],p=2,dim=0)))/input.shape[3]*self.weight[j]\n",
        "        a = -torch.sum(a)/input.shape[0]\n",
        "        b = -torch.sum(b)/input.shape[0]\n",
        "        return a+b\n",
        "\n",
        "## Spatial Profile Loss (SPL) without trace, prefered\n",
        "class SPLoss(nn.Module):\n",
        "    ''' \n",
        "    Spatial Profile Loss (SPL)\n",
        "    'SPLoss()' L2-normalizes the rows/columns, performs piece-wise multiplication \n",
        "    of the two tensors and then sums along the corresponding axes. This variant \n",
        "    needs less operations since it can be performed batchwise.\n",
        "    Note: SPLoss() makes image results too bright, when using images in the [0,1] \n",
        "    range and no activation as output of the Generator.\n",
        "    SPL_ComputeWithTrace() does not have this problem, but results are very blurry. \n",
        "    Adding the Overflow Loss fixes this problem.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SPLoss, self).__init__()\n",
        "        #self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = torch.sum(torch.sum(F.normalize(input, p=2, dim=2) * F.normalize(reference, p=2, dim=2),dim=2, keepdim=True))\n",
        "        b = torch.sum(torch.sum(F.normalize(input, p=2, dim=3) * F.normalize(reference, p=2, dim=3),dim=3, keepdim=True))\n",
        "        return -(a + b) / (input.size(2) * input.size(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Contextual Loss\n",
        "########################\n",
        "\n",
        "DIS_TYPES = ['cosine', 'l1', 'l2']\n",
        "\n",
        "class Contextual_Loss(nn.Module):\n",
        "    '''\n",
        "    Contextual loss for unaligned images (https://arxiv.org/abs/1803.02077)\n",
        "\n",
        "    https://github.com/roimehrez/contextualLoss\n",
        "    https://github.com/S-aiueo32/contextual_loss_pytorch\n",
        "    https://github.com/z-bingo/Contextual-Loss-PyTorch\n",
        "\n",
        "    layers_weights: is a dict, e.g., {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    crop_quarter: boolean\n",
        "    '''\n",
        "    def __init__(self, layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type: str = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg: bool = True, net: str = 'vgg19', calc_type: str =  'regular'):\n",
        "        super(Contextual_Loss, self).__init__()\n",
        "\n",
        "        assert band_width > 0, 'band_width parameter must be positive.'\n",
        "        assert distance_type in DIS_TYPES,\\\n",
        "            f'select a distance type from {DIS_TYPES}.'\n",
        "\n",
        "        listen_list = []\n",
        "        self.layers_weights = {}\n",
        "        try:\n",
        "            listen_list = layers_weights.keys()\n",
        "            self.layers_weights = layers_weights\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.crop_quarter = crop_quarter\n",
        "        self.distanceType = distance_type\n",
        "        self.max_1d_size = max_1d_size\n",
        "        self.b = b\n",
        "        self.band_width = band_width #self.h = h, #sigma\n",
        "        \n",
        "        if use_vgg:\n",
        "            self.vgg_model = VGG_Model(listen_list=listen_list, net=net)\n",
        "\n",
        "        if calc_type == 'bilateral':\n",
        "            self.calculate_loss = self.bilateral_CX_Loss\n",
        "        elif calc_type == 'symetric':\n",
        "            self.calculate_loss = self.symetric_CX_Loss\n",
        "        else: #if calc_type == 'regular':\n",
        "            self.calculate_loss = self.calculate_CX_Loss\n",
        "\n",
        "    def forward(self, images, gt):\n",
        "        device = images.device\n",
        "        \n",
        "        if hasattr(self, 'vgg_model'):\n",
        "            assert images.shape[1] == 3 and gt.shape[1] == 3,\\\n",
        "                'VGG model takes 3 channel images.'\n",
        "            \n",
        "            loss = 0\n",
        "            vgg_images = self.vgg_model(images)\n",
        "            vgg_images = {k: v.clone().to(device) for k, v in vgg_images.items()}\n",
        "            vgg_gt = self.vgg_model(gt)\n",
        "            vgg_gt = {k: v.to(device) for k, v in vgg_gt.items()}\n",
        "\n",
        "            for key in self.layers_weights.keys():\n",
        "                if self.crop_quarter:\n",
        "                    vgg_images[key] = self._crop_quarters(vgg_images[key])\n",
        "                    vgg_gt[key] = self._crop_quarters(vgg_gt[key])\n",
        "\n",
        "                N, C, H, W = vgg_images[key].size()\n",
        "                if H*W > self.max_1d_size**2:\n",
        "                    vgg_images[key] = self._random_pooling(vgg_images[key], output_1d_size=self.max_1d_size)\n",
        "                    vgg_gt[key] = self._random_pooling(vgg_gt[key], output_1d_size=self.max_1d_size)\n",
        "\n",
        "                loss_t = self.calculate_loss(vgg_images[key], vgg_gt[key])\n",
        "                loss += loss_t * self.layers_weights[key]\n",
        "                # del vgg_images[key], vgg_gt[key]\n",
        "        #TODO: without VGG it runs, but results are not looking right\n",
        "        else:\n",
        "            if self.crop_quarter:\n",
        "                images = self._crop_quarters(images)\n",
        "                gt = self._crop_quarters(gt)\n",
        "\n",
        "            N, C, H, W = images.size()\n",
        "            if H*W > self.max_1d_size**2:\n",
        "                images = self._random_pooling(images, output_1d_size=self.max_1d_size)\n",
        "                gt = self._random_pooling(gt, output_1d_size=self.max_1d_size)\n",
        "\n",
        "            loss = self.calculate_loss(images, gt)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_sampling(tensor, n, indices):\n",
        "        N, C, H, W = tensor.size()\n",
        "        S = H * W\n",
        "        tensor = tensor.view(N, C, S)\n",
        "        device=tensor.device\n",
        "        if indices is None:\n",
        "            indices = torch.randperm(S)[:n].contiguous().type_as(tensor).long()\n",
        "            indices = indices.clamp(indices.min(), tensor.shape[-1]-1) #max = indices.max()-1\n",
        "            indices = indices.view(1, 1, -1).expand(N, C, -1)\n",
        "        indices = indices.to(device)\n",
        "\n",
        "        res = torch.gather(tensor, index=indices, dim=-1)\n",
        "        return res, indices\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_pooling(feats, output_1d_size=100):\n",
        "        single_input = type(feats) is torch.Tensor\n",
        "\n",
        "        if single_input:\n",
        "            feats = [feats]\n",
        "\n",
        "        N, C, H, W = feats[0].size()\n",
        "        feats_sample, indices = Contextual_Loss._random_sampling(feats[0], output_1d_size**2, None)\n",
        "        res = [feats_sample]\n",
        "\n",
        "        for i in range(1, len(feats)):\n",
        "            feats_sample, _ = Contextual_Loss._random_sampling(feats[i], -1, indices)\n",
        "            res.append(feats_sample)\n",
        "\n",
        "        res = [feats_sample.view(N, C, output_1d_size, output_1d_size) for feats_sample in res]\n",
        "\n",
        "        if single_input:\n",
        "            return res[0]\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def _crop_quarters(feature_tensor):\n",
        "        N, fC, fH, fW = feature_tensor.size()\n",
        "        quarters_list = []\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), round(fW / 2):])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, round(fW / 2):])\n",
        "\n",
        "        feature_tensor = torch.cat(quarters_list, dim=0)\n",
        "        return feature_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L2(I_features, T_features):\n",
        "        \"\"\"\n",
        "        Calculating the distance between each feature of I and T\n",
        "        :param I_features:\n",
        "        :param T_features:\n",
        "        :return: raw_distance: [N, C, H, W, H*W], each element of which is the distance between I and T at each position\n",
        "        \"\"\"\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "        #\n",
        "        square_I = torch.sum(Ivecs*Ivecs, dim=1, keepdim=False)\n",
        "        square_T = torch.sum(Tvecs*Tvecs, dim=1, keepdim=False)\n",
        "        # raw_distance\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec, s_I, s_T = Ivecs[i, ...], Tvecs[i, ...], square_I[i, ...], square_T[i, ...]\n",
        "            # matrix multiplication\n",
        "            AB = Ivec.permute(1, 0) @ Tvec\n",
        "            dist = s_I.view(-1, 1) + s_T.view(1, -1) - 2*AB\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        raw_distance = torch.clamp(raw_distance, 0.0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L1(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec = Ivecs[i, ...], Tvecs[i, ...]\n",
        "            dist = torch.sum(\n",
        "                torch.abs(Ivec.view(C, -1, 1) - Tvec.view(C, 1, -1)), dim=0, keepdim=False\n",
        "            )\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_dotP(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        # prepare feature before calculating cosine distance\n",
        "        # mean shifting by channel-wise mean of `y`.\n",
        "        mean_T = T_features.mean(dim=(0, 2, 3), keepdim=True)        \n",
        "        I_features = I_features - mean_T\n",
        "        T_features = T_features - mean_T\n",
        "\n",
        "        # L2 channelwise normalization\n",
        "        I_features = F.normalize(I_features, p=2, dim=1)\n",
        "        T_features = F.normalize(T_features, p=2, dim=1)\n",
        "        \n",
        "        N, C, H, W = I_features.size()\n",
        "        cosine_dist = []\n",
        "        # work seperatly for each example in dim 1\n",
        "        for i in range(N):\n",
        "            # channel-wise vectorization\n",
        "            T_features_i = T_features[i].view(1, 1, C, H*W).permute(3, 2, 0, 1).contiguous() # 1CHW --> 11CP, with P=H*W\n",
        "            I_features_i = I_features[i].unsqueeze(0)\n",
        "            dist = F.conv2d(I_features_i, T_features_i).permute(0, 2, 3, 1).contiguous()\n",
        "            #cosine_dist.append(dist) # back to 1CHW\n",
        "            #TODO: temporary hack to workaround AMP bug:\n",
        "            cosine_dist.append(dist.to(torch.float32)) # back to 1CHW\n",
        "        cosine_dist = torch.cat(cosine_dist, dim=0)\n",
        "        cosine_dist = (1 - cosine_dist) / 2\n",
        "        cosine_dist = cosine_dist.clamp(min=0.0)\n",
        "\n",
        "        return cosine_dist\n",
        "\n",
        "    #compute_relative_distance\n",
        "    @staticmethod\n",
        "    def _calculate_relative_distance(raw_distance, epsilon=1e-5):\n",
        "        \"\"\"\n",
        "        Normalizing the distances first as Eq. (2) in paper\n",
        "        :param raw_distance:\n",
        "        :param epsilon:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        div = torch.min(raw_distance, dim=-1, keepdim=True)[0]\n",
        "        relative_dist = raw_distance / (div + epsilon) # Eq 2\n",
        "        return relative_dist\n",
        "\n",
        "    def symetric_CX_Loss(self, I_features, T_features):\n",
        "        loss = (self.calculate_CX_Loss(T_features, I_features) + self.calculate_CX_Loss(I_features, T_features)) / 2\n",
        "        return loss #score\n",
        "\n",
        "    def bilateral_CX_Loss(self, I_features, T_features, weight_sp: float = 0.1):\n",
        "        def compute_meshgrid(shape):\n",
        "            N, C, H, W = shape\n",
        "            rows = torch.arange(0, H, dtype=torch.float32) / (H + 1)\n",
        "            cols = torch.arange(0, W, dtype=torch.float32) / (W + 1)\n",
        "\n",
        "            feature_grid = torch.meshgrid(rows, cols)\n",
        "            feature_grid = torch.stack(feature_grid).unsqueeze(0)\n",
        "            feature_grid = torch.cat([feature_grid for _ in range(N)], dim=0)\n",
        "\n",
        "            return feature_grid\n",
        "\n",
        "        # spatial loss\n",
        "        grid = compute_meshgrid(I_features.shape).to(T_features.device)\n",
        "        raw_distance = Contextual_Loss._create_using_L2(grid, grid) # calculate raw distance\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_sp = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # feature loss\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_feat = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # combined loss\n",
        "        cx_combine = (1. - weight_sp) * cx_feat + weight_sp * cx_sp\n",
        "        k_max_NC, _ = torch.max(cx_combine, dim=2, keepdim=True)\n",
        "        cx = k_max_NC.mean(dim=1)\n",
        "        cx_loss = torch.mean(-torch.log(cx + 1e-5))\n",
        "        return cx_loss\n",
        "\n",
        "    def calculate_CX_Loss(self, I_features, T_features):\n",
        "        device = I_features.device\n",
        "        T_features = T_features.to(device)\n",
        "\n",
        "        if torch.sum(torch.isnan(I_features)) == torch.numel(I_features) or torch.sum(torch.isinf(I_features)) == torch.numel(I_features):\n",
        "            print(I_features)\n",
        "            raise ValueError('NaN or Inf in I_features')\n",
        "        if torch.sum(torch.isnan(T_features)) == torch.numel(T_features) or torch.sum(\n",
        "                torch.isinf(T_features)) == torch.numel(T_features):\n",
        "            print(T_features)\n",
        "            raise ValueError('NaN or Inf in T_features')\n",
        "\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        if torch.sum(torch.isnan(raw_distance)) == torch.numel(raw_distance) or torch.sum(\n",
        "                torch.isinf(raw_distance)) == torch.numel(raw_distance):\n",
        "            print(raw_distance)\n",
        "            raise ValueError('NaN or Inf in raw_distance')\n",
        "\n",
        "        # normalizing the distances\n",
        "        relative_distance = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        if torch.sum(torch.isnan(relative_distance)) == torch.numel(relative_distance) or torch.sum(\n",
        "                torch.isinf(relative_distance)) == torch.numel(relative_distance):\n",
        "            print(relative_distance)\n",
        "            raise ValueError('NaN or Inf in relative_distance')\n",
        "        del raw_distance\n",
        "\n",
        "        #compute_sim()\n",
        "        # where h>0 is a band-width parameter\n",
        "        exp_distance = torch.exp((self.b - relative_distance) / self.band_width) # Eq(3)\n",
        "        if torch.sum(torch.isnan(exp_distance)) == torch.numel(exp_distance) or torch.sum(\n",
        "                torch.isinf(exp_distance)) == torch.numel(exp_distance):\n",
        "            print(exp_distance)\n",
        "            raise ValueError('NaN or Inf in exp_distance')\n",
        "        del relative_distance\n",
        "        \n",
        "        # Similarity\n",
        "        contextual_sim = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "        if torch.sum(torch.isnan(contextual_sim)) == torch.numel(contextual_sim) or torch.sum(\n",
        "                torch.isinf(contextual_sim)) == torch.numel(contextual_sim):\n",
        "            print(contextual_sim)\n",
        "            raise ValueError('NaN or Inf in contextual_sim')\n",
        "        del exp_distance\n",
        "        \n",
        "        #contextual_loss()\n",
        "        max_gt_sim = torch.max(torch.max(contextual_sim, dim=1)[0], dim=1)[0] # Eq(1)\n",
        "        del contextual_sim\n",
        "        CS = torch.mean(max_gt_sim, dim=1)\n",
        "        CX_loss = torch.mean(-torch.log(CS)) # Eq(5)\n",
        "        if torch.isnan(CX_loss):\n",
        "            raise ValueError('NaN in computing CX_loss')\n",
        "        return CX_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VekVQxdItvfw",
        "cellView": "form"
      },
      "source": [
        "#@title vic/common.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/common.py\n",
        "\"\"\"\n",
        "BasicSR/codes/dataops/common.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/dataops/common.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import logging\n",
        "\n",
        "import copy\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "#from dataops.colors import *\n",
        "from .colors import *\n",
        "#from dataops.debug import tmp_vis, describe_numpy, describe_tensor\n",
        "\n",
        "####################\n",
        "# Files & IO\n",
        "####################\n",
        "\n",
        "###################### get image path list ######################\n",
        "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.dng', '.DNG', '.webp','.npy', '.NPY']\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def _get_paths_from_images(path):\n",
        "    '''get image path list from image folder'''\n",
        "    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n",
        "    images = []\n",
        "    for dirpath, _, fnames in sorted(os.walk(path)):\n",
        "        for fname in sorted(fnames):\n",
        "            if is_image_file(fname):\n",
        "                img_path = os.path.join(dirpath, fname)\n",
        "                images.append(img_path)\n",
        "    assert images, '{:s} has no valid image file'.format(path)\n",
        "    return images\n",
        "\n",
        "\n",
        "def _get_paths_from_lmdb(dataroot):\n",
        "    '''get image path list from lmdb'''\n",
        "    import lmdb\n",
        "    env = lmdb.open(dataroot, readonly=True, lock=False, readahead=False, meminit=False)\n",
        "    keys_cache_file = os.path.join(dataroot, '_keys_cache.p')\n",
        "    logger = logging.getLogger('base')\n",
        "    if os.path.isfile(keys_cache_file):\n",
        "        logger.info('Read lmdb keys from cache: {}'.format(keys_cache_file))\n",
        "        keys = pickle.load(open(keys_cache_file, \"rb\"))\n",
        "    else:\n",
        "        with env.begin(write=False) as txn:\n",
        "            logger.info('Creating lmdb keys cache: {}'.format(keys_cache_file))\n",
        "            keys = [key.decode('ascii') for key, _ in txn.cursor()]\n",
        "        pickle.dump(keys, open(keys_cache_file, 'wb'))\n",
        "    paths = sorted([key for key in keys if not key.endswith('.meta')])\n",
        "    return env, paths\n",
        "\n",
        "\n",
        "def get_image_paths(data_type, dataroot):\n",
        "    '''get image path list\n",
        "    support lmdb or image files'''\n",
        "    env, paths = None, None\n",
        "    if dataroot is not None:\n",
        "        if data_type == 'lmdb':\n",
        "            env, paths = _get_paths_from_lmdb(dataroot)\n",
        "        elif data_type == 'img':\n",
        "            paths = sorted(_get_paths_from_images(dataroot))\n",
        "        else:\n",
        "            raise NotImplementedError('data_type [{:s}] is not recognized.'.format(data_type))\n",
        "    return env, paths\n",
        "\n",
        "\n",
        "###################### read images ######################\n",
        "def _read_lmdb_img(env, path):\n",
        "    with env.begin(write=False) as txn:\n",
        "        buf = txn.get(path.encode('ascii'))\n",
        "        buf_meta = txn.get((path + '.meta').encode('ascii')).decode('ascii')\n",
        "    img_flat = np.frombuffer(buf, dtype=np.uint8)\n",
        "    H, W, C = [int(s) for s in buf_meta.split(',')]\n",
        "    img = img_flat.reshape(H, W, C)\n",
        "    return img\n",
        "\n",
        "\n",
        "def read_img(env, path, out_nc=3, fix_channels=True):\n",
        "    '''\n",
        "        Reads image using cv2 (rawpy if dng) or from lmdb by default\n",
        "        (can also use using PIL instead of cv2)\n",
        "    Arguments:\n",
        "        out_nc: Desired number of channels\n",
        "        fix_channels: changes the images to the desired number of channels\n",
        "    Output:\n",
        "        Numpy uint8, HWC, BGR, [0,255] by default \n",
        "    '''\n",
        "\n",
        "    img = None\n",
        "    if env is None:  # img\n",
        "        if(path[-3:].lower() == 'dng'): # if image is a DNG\n",
        "            import rawpy\n",
        "            with rawpy.imread(path) as raw:\n",
        "                img = raw.postprocess()\n",
        "        if(path[-3:].lower() == 'npy'): # if image is a NPY numpy array\n",
        "            with open(path, 'rb') as f:\n",
        "                img = np.load(f)\n",
        "        else: # else, if image can be read by cv2 \n",
        "            img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "        #TODO: add variable detecting if cv2 is not available and try PIL instead\n",
        "        # elif: # using PIL instead of OpenCV\n",
        "            # img = Image.open(path).convert('RGB')\n",
        "        # else: # For other images unrecognized by cv2\n",
        "            # import matplotlib.pyplot as plt\n",
        "            # img = (255*plt.imread(path)[:,:,:3]).astype('uint8')\n",
        "    else:\n",
        "        img = _read_lmdb_img(env, path)\n",
        "\n",
        "    # if not img:\n",
        "    #     raise ValueError(f\"Failed to read image: {path}\")\n",
        "\n",
        "    if fix_channels:\n",
        "        img = fix_img_channels(img, out_nc)\n",
        "\n",
        "    return img\n",
        "\n",
        "def fix_img_channels(img, out_nc):\n",
        "    '''\n",
        "        fix image channels to the expected number\n",
        "    '''\n",
        "\n",
        "    # if image has only 2 dimensions, add \"channel\" dimension (1)\n",
        "    if img.ndim == 2:\n",
        "        #img = img[..., np.newaxis] #alt\n",
        "        #img = np.expand_dims(img, axis=2)\n",
        "        img = np.tile(np.expand_dims(img, axis=2), (1, 1, 3))\n",
        "    # special case: properly remove alpha channel \n",
        "    if out_nc == 3 and img.shape[2] == 4: \n",
        "        img = bgra2rgb(img)\n",
        "    # remove all extra channels \n",
        "    elif img.shape[2] > out_nc: \n",
        "        img = img[:, :, :out_nc] \n",
        "    # if alpha is expected, add solid alpha channel\n",
        "    elif img.shape[2] == 3 and out_nc == 4:\n",
        "        img = np.dstack((img, np.full(img.shape[:-1], 255, dtype=np.uint8)))\n",
        "    return img\n",
        "\n",
        "\n",
        "####################\n",
        "# image processing\n",
        "# process on numpy image\n",
        "####################\n",
        "\n",
        "def bgra2rgb(img):\n",
        "    '''\n",
        "        cv2.cvtColor(img, cv2.COLOR_BGRA2BGR) has an issue removing the alpha channel,\n",
        "        this gets rid of wrong transparent colors that can harm training\n",
        "    '''\n",
        "    if img.shape[2] == 4:\n",
        "        #b, g, r, a = cv2.split((img*255).astype(np.uint8))\n",
        "        b, g, r, a = cv2.split((img.astype(np.uint8)))\n",
        "        b = cv2.bitwise_and(b, b, mask=a)\n",
        "        g = cv2.bitwise_and(g, g, mask=a)\n",
        "        r = cv2.bitwise_and(r, r, mask=a)\n",
        "        #return cv2.merge([b, g, r]).astype(np.float32)/255.\n",
        "        return cv2.merge([b, g, r])\n",
        "    return img\n",
        "\n",
        "def channel_convert(in_c, tar_type, img_list):\n",
        "    # conversion among BGR, gray and y\n",
        "    # Note: OpenCV uses inverted channels BGR, instead of RGB.\n",
        "    #  If images are loaded with something other than OpenCV,\n",
        "    #  check that the channels are in the correct order and use\n",
        "    #  the alternative conversion functions.\n",
        "    #if in_c == 4 and tar_type == 'RGB-A':  # BGRA to BGR, remove alpha channel\n",
        "        #return [cv2.cvtColor(img, cv2.COLOR_BGRA2BGR) for img in img_list]\n",
        "        #return [bgra2rgb(img) for img in img_list]\n",
        "    if in_c == 3 and tar_type == 'gray':  # BGR to gray\n",
        "        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n",
        "        return [np.expand_dims(img, axis=2) for img in gray_list]\n",
        "    elif in_c == 3 and tar_type == 'RGB-LAB': #RGB to LAB\n",
        "        return [cv2.cvtColor(img, cv2.COLOR_BGR2LAB) for img in img_list]\n",
        "    elif in_c == 3 and tar_type == 'LAB-RGB': #RGB to LAB\n",
        "        return [cv2.cvtColor(img, cv2.COLOR_LAB2BGR) for img in img_list]\n",
        "    elif in_c == 3 and tar_type == 'y':  # BGR to y\n",
        "        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n",
        "        return [np.expand_dims(img, axis=2) for img in y_list]\n",
        "    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR\n",
        "        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n",
        "    else:\n",
        "        return img_list\n",
        "\n",
        "def rgb2ycbcr(img, only_y=True):\n",
        "    '''same as matlab rgb2ycbcr\n",
        "    only_y: only return Y channel\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img_ = img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img_  *= 255.\n",
        "    # convert\n",
        "    if only_y:\n",
        "        rlt = np.dot(img_ , [65.481, 128.553, 24.966]) / 255.0 + 16.0\n",
        "    else:\n",
        "        rlt = np.matmul(img_ , [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n",
        "                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "\n",
        "def bgr2ycbcr(img, only_y=True, separate=False):\n",
        "    '''bgr version of matlab rgb2ycbcr\n",
        "    Python opencv library (cv2) cv2.COLOR_BGR2YCrCb has \n",
        "    different parameters with MATLAB color convertion.\n",
        "    only_y: only return Y channel\n",
        "    separate: if true, will returng the channels as \n",
        "        separate images\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img_ = img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img_  *= 255.\n",
        "    # convert\n",
        "    if only_y:\n",
        "        rlt = np.dot(img_ , [24.966, 128.553, 65.481]) / 255.0 + 16.0\n",
        "    else:\n",
        "        rlt = np.matmul(img_ , [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n",
        "                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n",
        "        # to make ycrcb like cv2\n",
        "        # rlt = rlt[:, :, (0, 2, 1)]\n",
        "    \n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    \n",
        "    if separate:\n",
        "        rlt = rlt.astype(in_img_type)\n",
        "        # y, cb, cr\n",
        "        return rlt[:, :, 0], rlt[:, :, 1], rlt[:, :, 2]\n",
        "    else:\n",
        "        return rlt.astype(in_img_type)\n",
        "\n",
        "'''\n",
        "def ycbcr2rgb_(img, only_y=True):\n",
        "    \"\"\"same as matlab ycbcr2rgb\n",
        "    (Note: this implementation is the original from BasicSR, but \n",
        "    appears to be for ycrcb, like cv2)\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    \"\"\"\n",
        "    in_img_type = img.dtype\n",
        "    img_ = img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img_  *= 255.\n",
        "    \n",
        "    # to make ycrcb like cv2\n",
        "    # rlt = rlt[:, :, (0, 2, 1)]\n",
        "\n",
        "    # convert\n",
        "    # original (for ycrcb):\n",
        "    rlt = np.matmul(img_ , [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n",
        "                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n",
        "\n",
        "    #alternative conversion:\n",
        "    # xform = np.array([[1, 0, 1.402], [1, -0.34414, -.71414], [1, 1.772, 0]])\n",
        "    # img_[:, :, [1, 2]] -= 128\n",
        "    # rlt = img_.dot(xform.T)\n",
        "    np.putmask(rlt, rlt > 255, 255)\n",
        "    np.putmask(rlt, rlt < 0, 0)\n",
        "    \n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "'''\n",
        "\n",
        "def ycbcr2rgb(img, only_y=True):\n",
        "    '''\n",
        "    bgr version of matlab ycbcr2rgb\n",
        "    Python opencv library (cv2) cv2.COLOR_YCrCb2BGR has \n",
        "    different parameters to MATLAB color convertion.\n",
        "\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img_ = img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img_  *= 255.\n",
        "    \n",
        "    # to make ycrcb like cv2\n",
        "    # rlt = rlt[:, :, (0, 2, 1)]\n",
        "\n",
        "    # convert\n",
        "    mat = np.array([[24.966, 128.553, 65.481],[112, -74.203, -37.797], [-18.214, -93.786, 112.0]])\n",
        "    mat = np.linalg.inv(mat.T) * 255\n",
        "    offset = np.array([[[16, 128, 128]]])\n",
        "\n",
        "    rlt = np.dot((img_ - offset), mat)\n",
        "    rlt = np.clip(rlt, 0, 255)\n",
        "    ## rlt = np.rint(rlt).astype('uint8')\n",
        "    \n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "\n",
        "'''\n",
        "#TODO: TMP RGB version, to check (PIL)\n",
        "def rgb2ycbcr(img_rgb):\n",
        "    ## the range of img_rgb should be (0, 1)\n",
        "    img_y = 0.257 * img_rgb[:, :, 0] + 0.504 * img_rgb[:, :, 1] + 0.098 * img_rgb[:, :, 2] + 16 / 255.0\n",
        "    img_cb = -0.148 * img_rgb[:, :, 0] - 0.291 * img_rgb[:, :, 1] + 0.439 * img_rgb[:, :, 2] + 128 / 255.0\n",
        "    img_cr = 0.439 * img_rgb[:, :, 0] - 0.368 * img_rgb[:, :, 1] - 0.071 * img_rgb[:, :, 2] + 128 / 255.0\n",
        "    return img_y, img_cb, img_cr\n",
        "\n",
        "#TODO: TMP RGB version, to check (PIL)\n",
        "def ycbcr2rgb(img_ycbcr):\n",
        "    ## the range of img_ycbcr should be (0, 1)\n",
        "    img_r = 1.164 * (img_ycbcr[:, :, 0] - 16 / 255.0) + 1.596 * (img_ycbcr[:, :, 2] - 128 / 255.0)\n",
        "    img_g = 1.164 * (img_ycbcr[:, :, 0] - 16 / 255.0) - 0.392 * (img_ycbcr[:, :, 1] - 128 / 255.0) - 0.813 * (img_ycbcr[:, :, 2] - 128 / 255.0)\n",
        "    img_b = 1.164 * (img_ycbcr[:, :, 0] - 16 / 255.0) + 2.017 * (img_ycbcr[:, :, 1] - 128 / 255.0)\n",
        "    img_r = img_r[:, :, np.newaxis]\n",
        "    img_g = img_g[:, :, np.newaxis]\n",
        "    img_b = img_b[:, :, np.newaxis]\n",
        "    img_rgb = np.concatenate((img_r, img_g, img_b), 2)\n",
        "    return img_rgb\n",
        "'''\n",
        "\n",
        "def modcrop(img_in, scale):\n",
        "    # img_in: Numpy, HWC or HW\n",
        "    img = np.copy(img_in)\n",
        "    if img.ndim == 2:\n",
        "        H, W = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r]\n",
        "    elif img.ndim == 3:\n",
        "        H, W, C = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r, :]\n",
        "    else:\n",
        "        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n",
        "    return img\n",
        "\n",
        "#TODO: this should probably be elsewhere (augmentations.py)\n",
        "def augment(img_list, hflip=True, rot=True):\n",
        "    # horizontal flip OR rotate\n",
        "    hflip = hflip and random.random() < 0.5\n",
        "    vflip = rot and random.random() < 0.5\n",
        "    rot90 = rot and random.random() < 0.5\n",
        "    #rot90n = rot and random.random() < 0.5\n",
        "\n",
        "    def _augment(img):\n",
        "        if hflip: img = np.flip(img, axis=1) #img[:, ::-1, :]\n",
        "        if vflip: img = np.flip(img, axis=0) #img[::-1, :, :]\n",
        "        #if rot90: img = img.transpose(1, 0, 2)\n",
        "        if rot90: img = np.rot90(img, 1) #90 degrees # In PIL: img.transpose(Image.ROTATE_90)\n",
        "        #if rot90n: img = np.rot90(img, -1) #-90 degrees\n",
        "        return img\n",
        "\n",
        "    return [_augment(img) for img in img_list]\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Normalization functions\n",
        "####################\n",
        "\n",
        "\n",
        "#TODO: Could also automatically detect the possible range with min and max, like in def ssim()\n",
        "def denorm(x, min_max=(-1.0, 1.0)):\n",
        "    '''\n",
        "        Denormalize from [-1,1] range to [0,1]\n",
        "        formula: xi' = (xi - mu)/sigma\n",
        "        Example: \"out = (x + 1.0) / 2.0\" for denorm \n",
        "            range (-1,1) to (0,1)\n",
        "        for use with proper act in Generator output (ie. tanh)\n",
        "    '''\n",
        "    out = (x - min_max[0]) / (min_max[1] - min_max[0])\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(0, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, 0, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "def norm(x): \n",
        "    #Normalize (z-norm) from [0,1] range to [-1,1]\n",
        "    out = (x - 0.5) * 2.0\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(-1, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, -1, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "\n",
        "####################\n",
        "# np and tensor conversions\n",
        "####################\n",
        "\n",
        "\n",
        "#2tensor\n",
        "def np2tensor(img, bgr2rgb=True, data_range=1., normalize=False, change_range=True, add_batch=True):\n",
        "    \"\"\"\n",
        "    Converts a numpy image array into a Tensor array.\n",
        "    Parameters:\n",
        "        img (numpy array): the input image numpy array\n",
        "        add_batch (bool): choose if new tensor needs batch dimension added \n",
        "    \"\"\"\n",
        "    if not isinstance(img, np.ndarray): #images expected to be uint8 -> 255\n",
        "        raise TypeError(\"Got unexpected object type, expected np.ndarray\")\n",
        "    #check how many channels the image has, then condition, like in my BasicSR. ie. RGB, RGBA, Gray\n",
        "    #if bgr2rgb:\n",
        "        #img = img[:, :, [2, 1, 0]] #BGR to RGB -> in numpy, if using OpenCV, else not needed. Only if image has colors.\n",
        "    if change_range:\n",
        "        if np.issubdtype(img.dtype, np.integer):\n",
        "            info = np.iinfo\n",
        "        elif np.issubdtype(img.dtype, np.floating):\n",
        "            info = np.finfo\n",
        "        img = img*data_range/info(img.dtype).max #uint8 = /255\n",
        "    img = torch.from_numpy(np.ascontiguousarray(np.transpose(img, (2, 0, 1)))).float() #\"HWC to CHW\" and \"numpy to tensor\"\n",
        "    if bgr2rgb:\n",
        "        if img.shape[0] == 3: #RGB\n",
        "            #BGR to RGB -> in tensor, if using OpenCV, else not needed. Only if image has colors.\n",
        "            img = bgr_to_rgb(img)\n",
        "        elif img.shape[0] == 4: #RGBA\n",
        "            #BGR to RGB -> in tensor, if using OpenCV, else not needed. Only if image has colors.)\n",
        "            img = bgra_to_rgba(img)\n",
        "    if add_batch:\n",
        "        img.unsqueeze_(0) # Add fake batch dimension = 1 . squeeze() will remove the dimensions of size 1\n",
        "    if normalize:\n",
        "        img = norm(img)\n",
        "    return img\n",
        "\n",
        "#2np\n",
        "def tensor2np(img, rgb2bgr=True, remove_batch=True, data_range=255, \n",
        "              denormalize=False, change_range=True, imtype=np.uint8):\n",
        "    \"\"\"\n",
        "    Converts a Tensor array into a numpy image array.\n",
        "    Parameters:\n",
        "        img (tensor): the input image tensor array\n",
        "            4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n",
        "        remove_batch (bool): choose if tensor of shape BCHW needs to be squeezed \n",
        "        denormalize (bool): Used to denormalize from [-1,1] range back to [0,1]\n",
        "        imtype (type): the desired type of the converted numpy array (np.uint8 \n",
        "            default)\n",
        "    Output: \n",
        "        img (np array): 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n",
        "    \"\"\"\n",
        "    if not isinstance(img, torch.Tensor):\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor\")\n",
        "    n_dim = img.dim()\n",
        "\n",
        "    #TODO: Check: could denormalize here in tensor form instead, but end result is the same\n",
        "    \n",
        "    img = img.float().cpu()  \n",
        "    \n",
        "    if n_dim == 4 or n_dim == 3:\n",
        "        #if n_dim == 4, has to convert to 3 dimensions, either removing batch or by creating a grid\n",
        "        if n_dim == 4 and remove_batch:\n",
        "            if img.shape[0] > 1:\n",
        "                # leave only the first image in the batch\n",
        "                img = img[0,...] \n",
        "            else:\n",
        "                # remove a fake batch dimension\n",
        "                img = img.squeeze()\n",
        "                # squeeze removes batch and channel of grayscale images (dimensions = 1)\n",
        "                if len(img.shape) < 3: \n",
        "                    #add back the lost channel dimension\n",
        "                    img = img.unsqueeze(dim=0)\n",
        "        # convert images in batch (BCHW) to a grid of all images (C B*H B*W)\n",
        "        else:\n",
        "            n_img = len(img)\n",
        "            img = make_grid(img, nrow=int(math.sqrt(n_img)), normalize=False)\n",
        "        \n",
        "        if img.shape[0] == 3 and rgb2bgr: #RGB\n",
        "            #RGB to BGR -> in tensor, if using OpenCV, else not needed. Only if image has colors.\n",
        "            img_np = rgb_to_bgr(img).numpy()\n",
        "        elif img.shape[0] == 4 and rgb2bgr: #RGBA\n",
        "            #RGBA to BGRA -> in tensor, if using OpenCV, else not needed. Only if image has colors.\n",
        "            img_np = rgba_to_bgra(img).numpy()\n",
        "        else:\n",
        "            img_np = img.numpy()\n",
        "        img_np = np.transpose(img_np, (1, 2, 0))  # \"CHW to HWC\" -> # HWC, BGR\n",
        "    elif n_dim == 2:\n",
        "        img_np = img.numpy()\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))\n",
        "\n",
        "    #if rgb2bgr:\n",
        "        #img_np = img_np[[2, 1, 0], :, :] #RGB to BGR -> in numpy, if using OpenCV, else not needed. Only if image has colors.\n",
        "    #TODO: Check: could denormalize in the begining in tensor form instead\n",
        "    if denormalize:\n",
        "        img_np = denorm(img_np) #denormalize if needed\n",
        "    if change_range:\n",
        "        img_np = np.clip(data_range*img_np,0,data_range).round() #clip to the data_range\n",
        "        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n",
        "    #has to be in range (0,255) before changing to np.uint8, else np.float32\n",
        "    return img_np.astype(imtype)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Prepare Images\n",
        "####################\n",
        "# https://github.com/sunreef/BlindSR/blob/master/src/image_utils.py\n",
        "def patchify_tensor(features, patch_size, overlap=10):\n",
        "    batch_size, channels, height, width = features.size()\n",
        "\n",
        "    effective_patch_size = patch_size - overlap\n",
        "    n_patches_height = (height // effective_patch_size)\n",
        "    n_patches_width = (width // effective_patch_size)\n",
        "\n",
        "    if n_patches_height * effective_patch_size < height:\n",
        "        n_patches_height += 1\n",
        "    if n_patches_width * effective_patch_size < width:\n",
        "        n_patches_width += 1\n",
        "\n",
        "    patches = []\n",
        "    for b in range(batch_size):\n",
        "        for h in range(n_patches_height):\n",
        "            for w in range(n_patches_width):\n",
        "                patch_start_height = min(h * effective_patch_size, height - patch_size)\n",
        "                patch_start_width = min(w * effective_patch_size, width - patch_size)\n",
        "                patches.append(features[b:b+1, :,\n",
        "                               patch_start_height: patch_start_height + patch_size,\n",
        "                               patch_start_width: patch_start_width + patch_size])\n",
        "    return torch.cat(patches, 0)\n",
        "\n",
        "def recompose_tensor(patches, full_height, full_width, overlap=10):\n",
        "\n",
        "    batch_size, channels, patch_size, _ = patches.size()\n",
        "    effective_patch_size = patch_size - overlap\n",
        "    n_patches_height = (full_height // effective_patch_size)\n",
        "    n_patches_width = (full_width // effective_patch_size)\n",
        "\n",
        "    if n_patches_height * effective_patch_size < full_height:\n",
        "        n_patches_height += 1\n",
        "    if n_patches_width * effective_patch_size < full_width:\n",
        "        n_patches_width += 1\n",
        "\n",
        "    n_patches = n_patches_height * n_patches_width\n",
        "    if batch_size % n_patches != 0:\n",
        "        print(\"Error: The number of patches provided to the recompose function does not match the number of patches in each image.\")\n",
        "    final_batch_size = batch_size // n_patches\n",
        "\n",
        "    blending_in = torch.linspace(0.1, 1.0, overlap)\n",
        "    blending_out = torch.linspace(1.0, 0.1, overlap)\n",
        "    middle_part = torch.ones(patch_size - 2 * overlap)\n",
        "    blending_profile = torch.cat([blending_in, middle_part, blending_out], 0)\n",
        "\n",
        "    horizontal_blending = blending_profile[None].repeat(patch_size, 1)\n",
        "    vertical_blending = blending_profile[:, None].repeat(1, patch_size)\n",
        "    blending_patch = horizontal_blending * vertical_blending\n",
        "\n",
        "    blending_image = torch.zeros(1, channels, full_height, full_width)\n",
        "    for h in range(n_patches_height):\n",
        "        for w in range(n_patches_width):\n",
        "            patch_start_height = min(h * effective_patch_size, full_height - patch_size)\n",
        "            patch_start_width = min(w * effective_patch_size, full_width - patch_size)\n",
        "            blending_image[0, :, patch_start_height: patch_start_height + patch_size, patch_start_width: patch_start_width + patch_size] += blending_patch[None]\n",
        "\n",
        "    recomposed_tensor = torch.zeros(final_batch_size, channels, full_height, full_width)\n",
        "    if patches.is_cuda:\n",
        "        blending_patch = blending_patch.cuda()\n",
        "        blending_image = blending_image.cuda()\n",
        "        recomposed_tensor = recomposed_tensor.cuda()\n",
        "    patch_index = 0\n",
        "    for b in range(final_batch_size):\n",
        "        for h in range(n_patches_height):\n",
        "            for w in range(n_patches_width):\n",
        "                patch_start_height = min(h * effective_patch_size, full_height - patch_size)\n",
        "                patch_start_width = min(w * effective_patch_size, full_width - patch_size)\n",
        "                recomposed_tensor[b, :, patch_start_height: patch_start_height + patch_size, patch_start_width: patch_start_width + patch_size] += patches[patch_index] * blending_patch\n",
        "                patch_index += 1\n",
        "    recomposed_tensor /= blending_image\n",
        "\n",
        "    return recomposed_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TODO: imresize could be an independent file (imresize.py)\n",
        "####################\n",
        "# Matlab imresize\n",
        "####################\n",
        "\n",
        "\n",
        "# These next functions are all interpolation methods. x is the distance from the left pixel center\n",
        "def cubic(x):\n",
        "    absx = torch.abs(x)\n",
        "    absx2 = absx**2\n",
        "    absx3 = absx**3\n",
        "    return (1.5*absx3 - 2.5*absx2 + 1) * ((absx <= 1).type_as(absx)) + \\\n",
        "        (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * (((absx > 1)*(absx <= 2)).type_as(absx))\n",
        "\n",
        "def box(x):\n",
        "    return ((-0.5 <= x) & (x < 0.5)) * 1.0\n",
        "\n",
        "def linear(x):\n",
        "    return (x + 1) * ((-1 <= x) & (x < 0)) + (1 - x) * ((0 <= x) & (x <= 1))\n",
        "\n",
        "def lanczos2(x):\n",
        "    return (((torch.sin(math.pi*x) * torch.sin(math.pi*x/2) + torch.finfo(torch.float32).eps) /\n",
        "             ((math.pi**2 * x**2 / 2) + torch.finfo(torch.float32).eps))\n",
        "            * (torch.abs(x) < 2))\n",
        "\n",
        "def lanczos3(x):\n",
        "    return (((torch.sin(math.pi*x) * torch.sin(math.pi*x/3) + torch.finfo(torch.float32).eps) /\n",
        "            ((math.pi**2 * x**2 / 3) + torch.finfo(torch.float32).eps))\n",
        "            * (torch.abs(x) < 3))\n",
        "\n",
        "\n",
        "def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n",
        "    if (scale < 1) and (antialiasing):\n",
        "        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n",
        "        kernel_width = kernel_width / scale\n",
        "\n",
        "    # Output-space coordinates\n",
        "    x = torch.linspace(1, out_length, out_length)\n",
        "\n",
        "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
        "    # in output space maps to 0.5 in input space, and 0.5+scale in output\n",
        "    # space maps to 1.5 in input space.\n",
        "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
        "\n",
        "    # What is the left-most pixel that can be involved in the computation?\n",
        "    left = torch.floor(u - kernel_width / 2)\n",
        "\n",
        "    # What is the maximum number of pixels that can be involved in the\n",
        "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
        "    # corresponding weights are all zero, it will be eliminated at the end\n",
        "    # of this function.\n",
        "    P = math.ceil(kernel_width) + 2\n",
        "\n",
        "    # The indices of the input pixels involved in computing the k-th output\n",
        "    # pixel are in row k of the indices matrix.\n",
        "    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n",
        "        1, P).expand(out_length, P)\n",
        "\n",
        "    # The weights used to compute the k-th output pixel are in row k of the\n",
        "    # weights matrix.\n",
        "    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n",
        "    # apply kernel\n",
        "    if (scale < 1) and (antialiasing):\n",
        "        weights = scale * kernel(distance_to_center * scale)\n",
        "    else:\n",
        "        weights = kernel(distance_to_center)\n",
        "    # Normalize the weights matrix so that each row sums to 1.\n",
        "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
        "    weights = weights / weights_sum.expand(out_length, P)\n",
        "\n",
        "    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n",
        "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
        "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 1, P - 2)\n",
        "        weights = weights.narrow(1, 1, P - 2)\n",
        "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 0, P - 2)\n",
        "        weights = weights.narrow(1, 0, P - 2)\n",
        "    weights = weights.contiguous()\n",
        "    indices = indices.contiguous()\n",
        "    sym_len_s = -indices.min() + 1\n",
        "    sym_len_e = indices.max() - in_length\n",
        "    indices = indices + sym_len_s - 1\n",
        "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
        "\n",
        "\n",
        "def imresize(img, scale, antialiasing=True, interpolation=None):\n",
        "    # The scale should be the same for H and W\n",
        "    # input: img: CHW RGB [0,1]\n",
        "    # output: CHW RGB [0,1] w/o round\n",
        "\n",
        "    in_C, in_H, in_W = img.size()\n",
        "    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n",
        "\n",
        "    # Choose interpolation method, each method has the matching kernel size\n",
        "    kernel, kernel_width = {\n",
        "        \"cubic\": (cubic, 4.0),\n",
        "        \"lanczos2\": (lanczos2, 4.0),\n",
        "        \"lanczos3\": (lanczos3, 6.0),\n",
        "        \"box\": (box, 1.0),\n",
        "        \"linear\": (linear, 2.0),\n",
        "        None: (cubic, 4.0)  # set default interpolation method as cubic\n",
        "    }.get(interpolation)\n",
        "\n",
        "    # Return the desired dimension order for performing the resize.  The\n",
        "    # strategy is to perform the resize first along the dimension with the\n",
        "    # smallest scale factor.\n",
        "    # Now we do not support this.\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n",
        "        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n",
        "    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n",
        "        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n",
        "    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n",
        "\n",
        "    sym_patch = img[:, :sym_len_Hs, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = img[:, -sym_len_He:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n",
        "    kernel_width = weights_H.size(1)\n",
        "    for i in range(out_H):\n",
        "        idx = int(indices_H[i][0])\n",
        "        out_1[0, i, :] = img_aug[0, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n",
        "        out_1[1, i, :] = img_aug[1, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n",
        "        out_1[2, i, :] = img_aug[2, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n",
        "    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :, :sym_len_Ws]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, :, -sym_len_We:]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n",
        "    kernel_width = weights_W.size(1)\n",
        "    for i in range(out_W):\n",
        "        idx = int(indices_W[i][0])\n",
        "        out_2[0, :, i] = out_1_aug[0, :, idx:idx + kernel_width].mv(weights_W[i])\n",
        "        out_2[1, :, i] = out_1_aug[1, :, idx:idx + kernel_width].mv(weights_W[i])\n",
        "        out_2[2, :, i] = out_1_aug[2, :, idx:idx + kernel_width].mv(weights_W[i])\n",
        "\n",
        "    return out_2\n",
        "\n",
        "\n",
        "def imresize_np(img, scale, antialiasing=True, interpolation=None):\n",
        "    # Now the scale should be the same for H and W\n",
        "    # input: img: Numpy, HWC BGR [0,1]\n",
        "    # output: HWC BGR [0,1] w/o round\n",
        "    \n",
        "    change_range = False\n",
        "    if img.max() > 1:\n",
        "        img_type = img.dtype\n",
        "        if np.issubdtype(img_type, np.integer):\n",
        "            info = np.iinfo\n",
        "        elif np.issubdtype(img_type, np.floating):\n",
        "            info = np.finfo\n",
        "        img = img/info(img_type).max\n",
        "        change_range = True\n",
        "\n",
        "    img = torch.from_numpy(img)\n",
        "\n",
        "    in_H, in_W, in_C = img.size()\n",
        "    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n",
        "\n",
        "    # Choose interpolation method, each method has the matching kernel size\n",
        "    kernel, kernel_width = {\n",
        "        \"cubic\": (cubic, 4.0),\n",
        "        \"lanczos2\": (lanczos2, 4.0),\n",
        "        \"lanczos3\": (lanczos3, 6.0),\n",
        "        \"box\": (box, 1.0),\n",
        "        \"linear\": (linear, 2.0),\n",
        "        None: (cubic, 4.0)  # set default interpolation method as cubic\n",
        "    }.get(interpolation)\n",
        "\n",
        "    # Return the desired dimension order for performing the resize.  The\n",
        "    # strategy is to perform the resize first along the dimension with the\n",
        "    # smallest scale factor.\n",
        "    # Now we do not support this.\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n",
        "        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n",
        "    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n",
        "        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n",
        "    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n",
        "\n",
        "    sym_patch = img[:sym_len_Hs, :, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n",
        "    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = img[-sym_len_He:, :, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n",
        "    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n",
        "    kernel_width = weights_H.size(1)\n",
        "    for i in range(out_H):\n",
        "        idx = int(indices_H[i][0])\n",
        "        out_1[i, :, 0] = img_aug[idx:idx + kernel_width, :, 0].transpose(0, 1).mv(weights_H[i])\n",
        "        out_1[i, :, 1] = img_aug[idx:idx + kernel_width, :, 1].transpose(0, 1).mv(weights_H[i])\n",
        "        out_1[i, :, 2] = img_aug[idx:idx + kernel_width, :, 2].transpose(0, 1).mv(weights_H[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n",
        "    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :sym_len_Ws, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, -sym_len_We:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n",
        "    kernel_width = weights_W.size(1)\n",
        "    for i in range(out_W):\n",
        "        idx = int(indices_W[i][0])\n",
        "        out_2[:, i, 0] = out_1_aug[:, idx:idx + kernel_width, 0].mv(weights_W[i])\n",
        "        out_2[:, i, 1] = out_1_aug[:, idx:idx + kernel_width, 1].mv(weights_W[i])\n",
        "        out_2[:, i, 2] = out_1_aug[:, idx:idx + kernel_width, 2].mv(weights_W[i])\n",
        "\n",
        "    out_2 = out_2.numpy().clip(0,1)\n",
        "    \n",
        "    if change_range:\n",
        "        out_2 = out_2*info(img_type).max #uint8 = 255\n",
        "        out_2 = out_2.astype(img_type)\n",
        "\n",
        "    return out_2\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # test imresize function\n",
        "    # read images\n",
        "    img = cv2.imread('test.png')\n",
        "    img = img * 1.0 / 255\n",
        "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n",
        "    # imresize\n",
        "    scale = 1 / 4\n",
        "    import time\n",
        "    total_time = 0\n",
        "    for i in range(10):\n",
        "        start_time = time.time()\n",
        "        rlt = imresize(img, scale, antialiasing=True)\n",
        "        use_time = time.time() - start_time\n",
        "        total_time += use_time\n",
        "    print('average time: {}'.format(total_time / 10))\n",
        "\n",
        "    import torchvision.utils\n",
        "    torchvision.utils.save_image(\n",
        "        (rlt * 255).round() / 255, 'rlt.png', nrow=1, padding=0, normalize=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lQVdNHypBOo",
        "cellView": "form"
      },
      "source": [
        "#@title perceptual.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/perceptual.py\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/architectures/perceptual.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/perceptual.py\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torchvision\n",
        "import torchvision.models.vgg as vgg\n",
        "import torchvision.models.resnet as resnet\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "####################\n",
        "# Perceptual Network\n",
        "####################\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    r\"\"\"\n",
        "    Perceptual loss, VGG-based\n",
        "    https://arxiv.org/abs/1603.08155\n",
        "    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 feature_layer=34,\n",
        "                 use_bn=False,\n",
        "                 use_input_norm=True,\n",
        "                 device=torch.device('cpu'),\n",
        "                 z_norm=False): #Note: PPON uses cuda instead of CPU\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        if use_bn:\n",
        "            model = vgg.vgg19_bn(pretrained=True)\n",
        "        else:\n",
        "            model = vgg.vgg19(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device) \n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)                 \n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "# VGG 19 layers to listen to\n",
        "vgg_layer19 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'conv_3_4': 16, 'pool_3': 18, 'conv_4_1': 19, 'conv_4_2': 21, 'conv_4_3': 23, 'conv_4_4': 25, 'pool_4': 27, 'conv_5_1': 28, 'conv_5_2': 30, 'conv_5_3': 32, 'conv_5_4': 34, 'pool_5': 36\n",
        "}\n",
        "vgg_layer_inv19 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'conv_3_4', 18: 'pool_3', 19: 'conv_4_1', 21: 'conv_4_2', 23: 'conv_4_3', 25: 'conv_4_4', 27: 'pool_4', 28: 'conv_5_1', 30: 'conv_5_2', 32: 'conv_5_3', 34: 'conv_5_4', 36: 'pool_5'\n",
        "}\n",
        "# VGG 16 layers to listen to\n",
        "vgg_layer16 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'pool_3': 16, 'conv_4_1': 17, 'conv_4_2': 19, 'conv_4_3': 21, 'pool_4': 23, 'conv_5_1': 24, 'conv_5_2': 26, 'conv_5_3': 28, 'pool_5': 30\n",
        "}\n",
        "vgg_layer_inv16 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'pool_3', 17: 'conv_4_1', 19: 'conv_4_2', 21: 'conv_4_3', 23: 'pool_4', 24: 'conv_5_1', 26: 'conv_5_2', 28: 'conv_5_3', 30: 'pool_5'\n",
        "}\n",
        "\n",
        "class VGG_Model(nn.Module):\n",
        "    \"\"\"\n",
        "        A VGG model with listerners in the layers. \n",
        "        Will return a dictionary of outputs that correspond to the \n",
        "        layers set in \"listen_list\".\n",
        "    \"\"\"\n",
        "    def __init__(self, listen_list=None, net='vgg19', use_input_norm=True, z_norm=False):\n",
        "        super(VGG_Model, self).__init__()\n",
        "        #vgg = vgg16(pretrained=True)\n",
        "        if net == 'vgg19':\n",
        "            vgg_net = vgg.vgg19(pretrained=True)\n",
        "            vgg_layer = vgg_layer19\n",
        "            self.vgg_layer_inv = vgg_layer_inv19\n",
        "        elif net == 'vgg16':\n",
        "            vgg_net = vgg.vgg16(pretrained=True)\n",
        "            vgg_layer = vgg_layer16\n",
        "            self.vgg_layer_inv = vgg_layer_inv16\n",
        "        self.vgg_model = vgg_net.features\n",
        "        self.use_input_norm = use_input_norm\n",
        "        # image normalization\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485-1]], [[0.456-1]], [[0.406-1]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229*2]], [[0.224*2]], [[0.225*2]]], requires_grad=False)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485]], [[0.456]], [[0.406]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229]], [[0.224]], [[0.225]]], requires_grad=False)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "\n",
        "        vgg_dict = vgg_net.state_dict()\n",
        "        vgg_f_dict = self.vgg_model.state_dict()\n",
        "        vgg_dict = {k: v for k, v in vgg_dict.items() if k in vgg_f_dict}\n",
        "        vgg_f_dict.update(vgg_dict)\n",
        "        # no grad\n",
        "        for p in self.vgg_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        if listen_list == []:\n",
        "            self.listen = []\n",
        "        else:\n",
        "            self.listen = set()\n",
        "            for layer in listen_list:\n",
        "                self.listen.add(vgg_layer[layer])\n",
        "        self.features = OrderedDict()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean.detach()) / self.std.detach()\n",
        "\n",
        "        for index, layer in enumerate(self.vgg_model):\n",
        "            x = layer(x)\n",
        "            if index in self.listen:\n",
        "                self.features[self.vgg_layer_inv[index]] = x\n",
        "        return self.features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(nn.Module):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = resnet.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MINCNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MINCNet, self).__init__()\n",
        "        self.ReLU = nn.ReLU(True)\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv31 = nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.maxpool3 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv41 = nn.Conv2d(256, 512, 3, 1, 1)\n",
        "        self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.maxpool4 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv51 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ReLU(self.conv11(x))\n",
        "        out = self.ReLU(self.conv12(out))\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.ReLU(self.conv21(out))\n",
        "        out = self.ReLU(self.conv22(out))\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.ReLU(self.conv31(out))\n",
        "        out = self.ReLU(self.conv32(out))\n",
        "        out = self.ReLU(self.conv33(out))\n",
        "        out = self.maxpool3(out)\n",
        "        out = self.ReLU(self.conv41(out))\n",
        "        out = self.ReLU(self.conv42(out))\n",
        "        out = self.ReLU(self.conv43(out))\n",
        "        out = self.maxpool4(out)\n",
        "        out = self.ReLU(self.conv51(out))\n",
        "        out = self.ReLU(self.conv52(out))\n",
        "        out = self.conv53(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class MINCFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, \\\n",
        "                device=torch.device('cpu')):\n",
        "        super(MINCFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.features = MINCNet()\n",
        "        self.features.load_state_dict(\n",
        "            torch.load('../experiments/pretrained_models/VGG16minc_53.pth'), strict=True)\n",
        "        self.features.eval()\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8EeEjwNjUFm",
        "cellView": "form"
      },
      "source": [
        "#@title vic/filters.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/filters.py\n",
        "\"\"\"\n",
        "BasicSR/codes/dataops/filters.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/dataops/filters.py\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "    Multiple image filters used by different functions. Can also be used as augmentations.\n",
        "'''\n",
        "\n",
        "import numbers\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from dataops.common import denorm\n",
        "\n",
        "\n",
        "def get_kernel_size(sigma = 6):\n",
        "    '''\n",
        "        Get optimal gaussian kernel size according to sigma * 6 criterion \n",
        "        (must return an int)\n",
        "        Alternative from Matlab: kernel_size=2*np.ceil(3*sigma)+1\n",
        "        https://stackoverflow.com/questions/3149279/optimal-sigma-for-gaussian-filtering-of-an-image\n",
        "    '''\n",
        "    kernel_size = np.ceil(sigma*6)\n",
        "    return kernel_size\n",
        "\n",
        "def get_kernel_sigma(kernel_size = 5):\n",
        "    '''\n",
        "        Get optimal gaussian kernel sigma (variance) according to kernel_size/6 \n",
        "        Alternative from Matlab: sigma = (kernel_size-1)/6\n",
        "    '''\n",
        "    return kernel_size/6.0\n",
        "\n",
        "def get_kernel_mean(kernel_size = 5):\n",
        "    '''\n",
        "        Get gaussian kernel mean\n",
        "    '''\n",
        "    return (kernel_size - 1) / 2.0\n",
        "\n",
        "def kernel_conv_w(kernel, channels: int =3):\n",
        "    '''\n",
        "        Reshape a H*W kernel to 2d depthwise convolutional \n",
        "            weight (for loading in a Conv2D)\n",
        "    '''\n",
        "\n",
        "    # Dynamic window expansion. expand() does not copy memory, needs contiguous()\n",
        "    kernel = kernel.expand(channels, 1, *kernel.size()).contiguous()\n",
        "    return kernel\n",
        "\n",
        "#@torch.jit.script\n",
        "def get_gaussian_kernel1d(kernel_size: int,\n",
        "                sigma: float = 1.5, \n",
        "                #channel: int = None,\n",
        "                force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns 1-D Gaussian filter kernel coefficients.\n",
        "\n",
        "    Args:\n",
        "        kernel_size (int): filter/window size. It should be odd and positive.\n",
        "        sigma (float): gaussian standard deviation, sigma of normal distribution\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 1D tensor with 1D gaussian filter coefficients.\n",
        "\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size})`\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> get_gaussian_kernel1d(3, 2.5)\n",
        "        tensor([0.3243, 0.3513, 0.3243])\n",
        "\n",
        "        >>> get_gaussian_kernel1d(5, 1.5)\n",
        "        tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])\n",
        "    \"\"\"\n",
        "        \n",
        "    if (not isinstance(kernel_size, int) or (\n",
        "            (kernel_size % 2 == 0) and not force_even) or (\n",
        "            kernel_size <= 0)):\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be an odd positive integer. \"\n",
        "            \"Got {}\".format(kernel_size)\n",
        "        )\n",
        "\n",
        "    if kernel_size % 2 == 0:\n",
        "        x = torch.arange(kernel_size).float() - kernel_size // 2    \n",
        "        x = x + 0.5\n",
        "        gauss = torch.exp((-x.pow(2.0) / float(2 * sigma ** 2)))\n",
        "    else: #much faster\n",
        "        gauss = torch.Tensor([np.exp(-(x - kernel_size//2)**2/float(2*sigma**2)) for x in range(kernel_size)])\n",
        "\n",
        "    gauss /= gauss.sum()\n",
        "    \n",
        "    return gauss\n",
        "\n",
        "#To get the kernel coefficients\n",
        "def get_gaussian_kernel2d(\n",
        "        #kernel_size: Tuple[int, int],\n",
        "        kernel_size,\n",
        "        #sigma: Tuple[float, float],\n",
        "        sigma,\n",
        "        force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns Gaussian filter matrix coefficients.\n",
        "         Modified with a faster kernel creation if the kernel size\n",
        "         is odd. \n",
        "    Args:\n",
        "        kernel_size (Tuple[int, int]): filter (window) sizes in the x and y \n",
        "         direction. Sizes should be odd and positive, unless force_even is\n",
        "         used.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "         direction.\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: 2D tensor with gaussian filter matrix coefficients.\n",
        "\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> get_gaussian_kernel2d((3, 3), (1.5, 1.5))\n",
        "        tensor([[0.0947, 0.1183, 0.0947],\n",
        "                [0.1183, 0.1478, 0.1183],\n",
        "                [0.0947, 0.1183, 0.0947]])\n",
        "\n",
        "        >>> get_gaussian_kernel2d((3, 5), (1.5, 1.5))\n",
        "        tensor([[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],\n",
        "                [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],\n",
        "                [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]])\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(kernel_size, (int, float)): \n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "    if isinstance(sigma, (int, float)): \n",
        "        sigma = (sigma, sigma)\n",
        "\n",
        "    if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be a tuple of length two. Got {}\".format(\n",
        "                kernel_size\n",
        "            )\n",
        "        )\n",
        "    if not isinstance(sigma, tuple) or len(sigma) != 2:\n",
        "        raise TypeError(\n",
        "            \"sigma must be a tuple of length two. Got {}\".format(sigma)\n",
        "        )\n",
        "    ksize_x, ksize_y = kernel_size\n",
        "    sigma_x, sigma_y = sigma\n",
        "    kernel_x: torch.Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)\n",
        "    kernel_y: torch.Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)\n",
        "    \n",
        "    kernel_2d: torch.Tensor = torch.matmul(\n",
        "        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t()\n",
        "    )\n",
        "    \n",
        "    return kernel_2d\n",
        "\n",
        "def get_gaussian_kernel(kernel_size=5, sigma=3, dim=2):\n",
        "    '''\n",
        "        This function can generate gaussian kernels in any dimension,\n",
        "            but its 3 times slower than get_gaussian_kernel2d()\n",
        "    Arguments:\n",
        "        kernel_size (Tuple[int, int]): filter sizes in the x and y direction.\n",
        "            Sizes should be odd and positive.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "            direction.\n",
        "        dim: the image dimension (2D=2, 3D=3, etc)\n",
        "    Returns:\n",
        "        Tensor: tensor with gaussian filter matrix coefficients.\n",
        "    '''\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    kernel = 1\n",
        "    meshgrids = torch.meshgrid(\n",
        "        [\n",
        "            torch.arange(size, dtype=torch.float32)\n",
        "            for size in kernel_size\n",
        "        ]\n",
        "    )\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
        "        mean = (size - 1) / 2\n",
        "        kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
        "                  torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
        "\n",
        "    kernel = kernel / torch.sum(kernel)    \n",
        "    return kernel\n",
        "\n",
        "#TODO: could be modified to generate kernels in different dimensions\n",
        "def get_box_kernel(kernel_size: int = 5, dim=2):\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "\n",
        "    kx: float=  float(kernel_size[0])\n",
        "    ky: float=  float(kernel_size[1])\n",
        "    box_kernel = torch.Tensor(np.ones((kx, ky)) / (kx*ky))\n",
        "\n",
        "    return box_kernel\n",
        "\n",
        "\n",
        "\n",
        "#TODO: Can change HFEN to use either LoG, DoG or XDoG\n",
        "def get_log_kernel_5x5():\n",
        "    '''\n",
        "    This is a precomputed LoG kernel that has already been convolved with\n",
        "    Gaussian, for edge detection. \n",
        "    \n",
        "    http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html\n",
        "    http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "    https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "    The 2-D LoG can be approximated by a 5 by 5 convolution kernel such as:\n",
        "    weight_log = torch.Tensor([\n",
        "                    [0, 0, 1, 0, 0],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [1, 2, -16, 2, 1],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [0, 0, 1, 0, 0]\n",
        "                ])\n",
        "    This is an approximate to the LoG kernel with kernel size 5 and optimal \n",
        "    sigma ~6 (0.590155...).\n",
        "    '''\n",
        "    return torch.Tensor([\n",
        "                [0, 0, 1, 0, 0],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [1, 2, -16, 2, 1],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [0, 0, 1, 0, 0]\n",
        "            ])\n",
        "\n",
        "#dim is the image dimension (2D=2, 3D=3, etc), but for now the final_kernel is hardcoded to 2D images\n",
        "#Not sure if it would make sense in higher dimensions\n",
        "#Note: Kornia suggests their laplacian kernel can also be used to generate LoG kernel: \n",
        "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html\n",
        "def get_log_kernel2d(kernel_size=5, sigma=None, dim=2): #sigma=0.6; kernel_size=5\n",
        "    \n",
        "    #either kernel_size or sigma are required:\n",
        "    if not kernel_size and sigma:\n",
        "        kernel_size = get_kernel_size(sigma)\n",
        "        kernel_size = [kernel_size] * dim #note: should it be [kernel_size] or [kernel_size-1]? look below \n",
        "    elif kernel_size and not sigma:\n",
        "        sigma = get_kernel_sigma(kernel_size)\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size-1] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    grids = torch.meshgrid([torch.arange(-size//2,size//2+1,1) for size in kernel_size])\n",
        "\n",
        "    kernel = 1\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, grids):\n",
        "        kernel *= torch.exp(-(mgrid**2/(2.*std**2)))\n",
        "    \n",
        "    #TODO: For now hardcoded to 2 dimensions, test to make it work in any dimension\n",
        "    final_kernel = (kernel) * ((grids[0]**2 + grids[1]**2) - (2*sigma[0]*sigma[1])) * (1/((2*math.pi)*(sigma[0]**2)*(sigma[1]**2)))\n",
        "    \n",
        "    #TODO: Test if normalization has to be negative (the inverted peak should not make a difference)\n",
        "    final_kernel = -final_kernel / torch.sum(final_kernel)\n",
        "    \n",
        "    return final_kernel\n",
        "\n",
        "def get_log_kernel(kernel_size: int = 5, sigma: float = None, dim: int = 2):\n",
        "    '''\n",
        "        Returns a Laplacian of Gaussian (LoG) kernel. If the kernel is known, use it,\n",
        "        else, generate a kernel with the parameters provided (slower)\n",
        "    '''\n",
        "    if kernel_size ==5 and not sigma and dim == 2: \n",
        "        return get_log_kernel_5x5()\n",
        "    else:\n",
        "        return get_log_kernel2d(kernel_size, sigma, dim)\n",
        "\n",
        "#TODO: use\n",
        "# Implementation of binarize operation (for edge detectors)\n",
        "def binarize(bin_img, threshold):\n",
        "  #bin_img = img > threshold\n",
        "  bin_img[bin_img < threshold] = 0.\n",
        "  return bin_img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_laplacian_kernel_3x3(alt=False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a laplacian kernel of 3x3\n",
        "            https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "            http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "        \n",
        "        This is called a negative Laplacian because the central peak is negative. \n",
        "        It is just as appropriate to reverse the signs of the elements, using \n",
        "        -1s and a +4, to get a positive Laplacian. It doesn't matter:\n",
        "\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [0,  -1, 0],\n",
        "                                    [-1, 4, -1],\n",
        "                                    [0,  -1, 0]\n",
        "                                ])\n",
        "\n",
        "        Alternative Laplacian kernel as produced by Kornia (this is positive Laplacian,\n",
        "        like: https://kornia.readthedocs.io/en/latest/filters.html\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [-1, -1, -1],\n",
        "                                    [-1,  8, -1],\n",
        "                                    [-1, -1, -1]\n",
        "                                ])\n",
        "\n",
        "    \"\"\"\n",
        "    if alt:\n",
        "        return torch.tensor([\n",
        "                    [-1, -1, -1],\n",
        "                    [-1,  8, -1],\n",
        "                    [-1, -1, -1]\n",
        "                ])\n",
        "    else:\n",
        "        return torch.tensor([\n",
        "                    [0, 1, 0],\n",
        "                    [1,-4, 1],\n",
        "                    [0, 1, 0],\n",
        "                ])\n",
        "\n",
        "def get_gradient_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a gradient kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "            kernel_gradient_v = [[0, -1, 0], \n",
        "                                 [0, 0, 0], \n",
        "                                 [0, 1, 0]]\n",
        "            kernel_gradient_h = [[0, 0, 0], \n",
        "                                 [-1, 0, 1], \n",
        "                                 [0, 0, 0]]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [0, 0, 0], \n",
        "                   [-1, 0, 1], \n",
        "                   [0, 0, 0],\n",
        "            ])\n",
        "\n",
        "def get_scharr_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a scharr kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-3, 0, 3],\n",
        "                   [-10,0,10],\n",
        "                   [-3, 0, 3],\n",
        "    ])\n",
        "\n",
        "def get_prewitt_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a prewitt kernel of 3x3\n",
        "            in x direction (transpose for y direction).\n",
        "        \n",
        "        Prewitt in x direction: This mask is called the \n",
        "            (vertical) Prewitt Edge Detector\n",
        "            prewitt_x= np.array([[-1, 0, 1],\n",
        "                                [-1, 0, 1],\n",
        "                                [-1, 0, 1]])\n",
        "        \n",
        "        Prewitt in y direction: This mask is called the \n",
        "            (horizontal) Prewitt Edge Detector\n",
        "            prewitt_y= np.array([[-1,-1,-1],\n",
        "                                 [0, 0, 0],\n",
        "                                 [1, 1, 1]])\n",
        "\n",
        "        Note that a Prewitt operator is a 1D box filter convolved with \n",
        "            a derivative operator \n",
        "            finite_diff = [-1, 0, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "    ])\n",
        "\n",
        "#https://github.com/kornia/kornia/blob/master/kornia/filters/kernels.py\n",
        "def get_sobel_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"Utility function that returns a sobel kernel of 3x3\n",
        "        sobel in x direction\n",
        "            sobel_x= np.array([[-1, 0, 1],\n",
        "                               [-2, 0, 2],\n",
        "                               [-1, 0, 1]])\n",
        "        sobel in y direction\n",
        "            sobel_y= np.array([[-1,-2,-1],\n",
        "                               [0, 0, 0],\n",
        "                               [1, 2, 1]])\n",
        "        \n",
        "        Note that a Sobel operator is a [1 2 1] filter convolved with \n",
        "            a derivative operator.\n",
        "            finite_diff = [1, 2, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "        [-1., 0., 1.],\n",
        "        [-2., 0., 2.],\n",
        "        [-1., 0., 1.],\n",
        "    ])\n",
        "\n",
        "#https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "def get_sobel_kernel_2d(kernel_size=3):\n",
        "    # get range\n",
        "    range = torch.linspace(-(kernel_size // 2), kernel_size // 2, kernel_size)\n",
        "    # compute a grid the numerator and the axis-distances\n",
        "    y, x = torch.meshgrid(range, range)\n",
        "    #Note: x is edge detector in x, y is edge detector in y, if not dividing by den\n",
        "    den = (x ** 2 + y ** 2)\n",
        "    #den[:, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    den[kernel_size // 2, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    #sobel_2D = x / den #produces kernel in range (0,1)\n",
        "    sobel_2D = 2*x / den #produces same kernel as kornia\n",
        "    return sobel_2D\n",
        "\n",
        "def get_sobel_kernel(kernel_size=3):\n",
        "    '''\n",
        "    Sobel kernel\n",
        "        https://en.wikipedia.org/wiki/Sobel_operator\n",
        "    Note: using the Sobel filters needs two kernels, one in X axis and one in Y \n",
        "        axis (which is the transpose of X), to get the gradients in both directions.\n",
        "        The same kernel can be used in both cases.\n",
        "    '''\n",
        "    if kernel_size==3:\n",
        "        return get_sobel_kernel_3x3()\n",
        "    else:\n",
        "        return get_sobel_kernel_2d(kernel_size)\n",
        "\n",
        "\n",
        "\n",
        "#To apply the 1D filter in X and Y axis (For SSIM)\n",
        "#@torch.jit.script\n",
        "def apply_1Dfilter(input, win, use_padding: bool=False):  \n",
        "    r\"\"\" Apply 1-D kernel to input in X and Y axes.\n",
        "         Separable filters like the Gaussian blur can be applied to \n",
        "         a two-dimensional image as two independent one-dimensional \n",
        "         calculations, so a 2-dimensional convolution operation can \n",
        "         be separated into two 1-dimensional filters. This reduces \n",
        "         the cost of computing the operator.\n",
        "           https://en.wikipedia.org/wiki/Separable_filter\n",
        "    Args:\n",
        "        input (torch.Tensor): a batch of tensors to be filtered\n",
        "        window (torch.Tensor): 1-D gauss kernel\n",
        "        use_padding: padding image before conv\n",
        "    Returns:\n",
        "        torch.Tensor: filtered tensors\n",
        "    \"\"\"\n",
        "    #N, C, H, W = input.shape\n",
        "    C = input.shape[1]\n",
        "    \n",
        "    padding = 0\n",
        "    if use_padding:\n",
        "        window_size = win.shape[3]\n",
        "        padding = window_size // 2\n",
        "\n",
        "    #same 1D filter for both axes    \n",
        "    out = F.conv2d(input, win, stride=1, padding=(0, padding), groups=C)\n",
        "    out = F.conv2d(out, win.transpose(2, 3), stride=1, padding=(padding, 0), groups=C)\n",
        "    return out\n",
        "\n",
        "#convenient alias\n",
        "apply_gaussian_filter = apply_1Dfilter\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use this in the initialization of class FilterX, so it can be used on \n",
        "# forward with an image (LoG, Gaussian, etc)\n",
        "def load_filter(kernel, kernel_size=3, in_channels=3, out_channels=3, \n",
        "                stride=1, padding=True, groups=3, dim: int =2, \n",
        "                requires_grad=False):\n",
        "    '''\n",
        "        Loads a kernel's coefficients into a Conv layer that \n",
        "            can be used to convolve an image with, by default, \n",
        "            for depthwise convolution\n",
        "        Can use nn.Conv1d, nn.Conv2d or nn.Conv3d, depending on\n",
        "            the dimension set in dim (1,2,3)\n",
        "        #From Pytorch Conv2D:\n",
        "            https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv2d\n",
        "            When `groups == in_channels` and `out_channels == K * in_channels`,\n",
        "            where `K` is a positive integer, this operation is also termed in\n",
        "            literature as depthwise convolution.\n",
        "             At groups= :attr:`in_channels`, each input channel is convolved with\n",
        "             its own set of filters, of size:\n",
        "             :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\n",
        "    '''\n",
        "\n",
        "    '''#TODO: check if this is necessary, probably not\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    '''\n",
        "\n",
        "    # Reshape to 2d depthwise convolutional weight\n",
        "    kernel = kernel_conv_w(kernel, in_channels)\n",
        "    assert(len(kernel.shape)==4 and kernel.shape[0]==in_channels)\n",
        "\n",
        "    if padding:\n",
        "        pad = compute_padding(kernel_size)\n",
        "    else:\n",
        "        pad = 0\n",
        "    \n",
        "    # create filter as convolutional layer\n",
        "    if dim == 1:\n",
        "        conv = nn.Conv1d\n",
        "    elif dim == 2:\n",
        "        conv = nn.Conv2d\n",
        "    elif dim == 3:\n",
        "        conv = nn.Conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported for convolution. \\\n",
        "            Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    filter = conv(in_channels=in_channels, out_channels=out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride, padding=padding, \n",
        "                        groups=groups, bias=False)\n",
        "    filter.weight.data = kernel\n",
        "    filter.weight.requires_grad = requires_grad\n",
        "    return filter\n",
        "\n",
        "\n",
        "def compute_padding(kernel_size):\n",
        "    '''\n",
        "        Computes padding tuple. For square kernels, pad can be an\n",
        "         int, else, a tuple with an element for each dimension\n",
        "    '''\n",
        "    # 4 or 6 ints:  (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    if isinstance(kernel_size, int):\n",
        "        return kernel_size//2\n",
        "    elif isinstance(kernel_size, list):\n",
        "        computed = [k // 2 for k in kernel_size]\n",
        "\n",
        "        out_padding = []\n",
        "\n",
        "        for i in range(len(kernel_size)):\n",
        "            computed_tmp = computed[-(i + 1)]\n",
        "            # for even kernels we need to do asymetric padding\n",
        "            if kernel_size[i] % 2 == 0:\n",
        "                padding = computed_tmp - 1\n",
        "            else:\n",
        "                padding = computed_tmp\n",
        "            out_padding.append(padding)\n",
        "            out_padding.append(computed_tmp)\n",
        "        return out_padding\n",
        "\n",
        "def normalize_kernel2d(input: torch.Tensor) -> torch.Tensor:\n",
        "    r\"\"\"Normalizes kernel.\n",
        "    \"\"\"\n",
        "    if len(input.size()) < 2:\n",
        "        raise TypeError(\"input should be at least 2D tensor. Got {}\"\n",
        "                        .format(input.size()))\n",
        "    norm: torch.Tensor = input.abs().sum(dim=-1).sum(dim=-1)\n",
        "    return input / (norm.unsqueeze(-1).unsqueeze(-1))\n",
        "\n",
        "def filter2D(input: torch.Tensor, kernel: torch.Tensor,\n",
        "             border_type: str = 'reflect', \n",
        "             dim: int =2,\n",
        "             normalized: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that convolves a tensor with a kernel.\n",
        "\n",
        "    The function applies a given kernel to a tensor. The kernel is applied\n",
        "    independently at each depth channel of the tensor. Before applying the\n",
        "    kernel, the function applies padding according to the specified mode so\n",
        "    that the output remains in the same shape.\n",
        "    Args:\n",
        "        input (torch.Tensor): the input tensor with shape of\n",
        "          :math:`(B, C, H, W)`.\n",
        "        kernel (torch.Tensor): the kernel to be convolved with the input\n",
        "          tensor. The kernel shape must be :math:`(1, kH, kW)`.\n",
        "        border_type (str): the padding mode to be applied before convolving.\n",
        "          The expected modes are: ``'constant'``, ``'reflect'``,\n",
        "          ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.\n",
        "        normalized (bool): If True, kernel will be L1 normalized.\n",
        "    Return:\n",
        "        torch.Tensor: the convolved tensor of same size and numbers of channels\n",
        "        as the input.\n",
        "    \"\"\"\n",
        "    if not isinstance(input, torch.Tensor):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(input)))\n",
        "\n",
        "    if not isinstance(kernel, torch.Tensor):\n",
        "        raise TypeError(\"Input kernel type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    if not isinstance(border_type, str):\n",
        "        raise TypeError(\"Input border_type is not string. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    #if not len(input.shape) == 4:\n",
        "        #raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"\n",
        "                         #.format(input.shape))\n",
        "\n",
        "    #if not len(kernel.shape) == 3:\n",
        "        #raise ValueError(\"Invalid kernel shape, we expect 1xHxW. Got: {}\"\n",
        "                         #.format(kernel.shape))\n",
        "\n",
        "    borders_list: List[str] = ['constant', 'reflect', 'replicate', 'circular']\n",
        "    if border_type not in borders_list:\n",
        "        raise ValueError(\"Invalid border_type, we expect the following: {0}.\"\n",
        "                         \"Got: {1}\".format(borders_list, border_type))\n",
        "\n",
        "    # prepare kernel\n",
        "    b, c, h, w = input.shape\n",
        "    tmp_kernel: torch.Tensor = kernel.unsqueeze(0).to(input.device).to(input.dtype)\n",
        "    if normalized:\n",
        "        tmp_kernel = normalize_kernel2d(tmp_kernel) \n",
        "    # pad the input tensor\n",
        "    height, width = tmp_kernel.shape[-2:]\n",
        "    padding_shape: List[int] = compute_padding((height, width))\n",
        "    input_pad: torch.Tensor = F.pad(input, padding_shape, mode=border_type)\n",
        "    b, c, hp, wp = input_pad.shape\n",
        "\n",
        "    tmp_kernel = tmp_kernel.expand(c, -1, -1, -1)\n",
        "\n",
        "    # convolve the tensor with the kernel.\n",
        "    if dim == 1:\n",
        "        conv = F.conv1d\n",
        "    elif dim == 2:\n",
        "        conv = F.conv2d\n",
        "        #TODO: this needs a review, the final sizes don't match with .view(b, c, h, w), (they are larger).\n",
        "            # using .view(b, c, -1, w) results in an output, but it's 3 times larger than it should be\n",
        "        '''\n",
        "        # if kernel_numel > 81 this is a faster algo\n",
        "        kernel_numel: int = height * width #kernel_numel = torch.numel(tmp_kernel[-1:])\n",
        "        if kernel_numel > 81:\n",
        "            return conv(input_pad.reshape(b * c, 1, hp, wp), tmp_kernel, padding=0, stride=1).view(b, c, h, w)\n",
        "        '''\n",
        "    elif dim == 3:\n",
        "        conv = F.conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    return conv(input_pad, tmp_kernel, groups=c, padding=0, stride=1)\n",
        "\n",
        "\n",
        "#TODO: make one class to receive any arbitrary kernel and others that are specific (like gaussian, etc)\n",
        "#class FilterX(nn.Module):\n",
        "  #def __init__(self, ..., kernel_type, dim: int=2):\n",
        "      #r\"\"\"\n",
        "      #Args:\n",
        "          #argument: ...\n",
        "      #\"\"\"\n",
        "      #super(filterXd, self).__init__()\n",
        "      #Here receive an pre-made kernel of any type, load as tensor or as\n",
        "      #convXd layer (class or functional)\n",
        "      # self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                #in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                #padding=pad, groups=image_channels)\n",
        "  #def forward:\n",
        "      #This would apply the filter that was initialized\n",
        "    \n",
        "\n",
        "\n",
        "class FilterLow(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, padding=True, \n",
        "                image_channels=3, include_pad=True, filter_type=None):\n",
        "        super(FilterLow, self).__init__()\n",
        "        \n",
        "        if padding:\n",
        "            pad = compute_padding(kernel_size)\n",
        "        else:\n",
        "            pad = 0\n",
        "        \n",
        "        if filter_type == 'gaussian':\n",
        "            sigma = get_kernel_sigma(kernel_size)\n",
        "            kernel = get_gaussian_kernel2d(kernel_size=kernel_size, sigma=sigma)\n",
        "            self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                    in_channels=image_channels, stride=stride, padding=pad)\n",
        "        #elif filter_type == '': #TODO... box? (the same as average) What else?\n",
        "        else:\n",
        "            self.filter = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, \n",
        "                    padding=pad, count_include_pad=include_pad)\n",
        "        self.recursions = recursions\n",
        "\n",
        "    def forward(self, img):\n",
        "        for i in range(self.recursions):\n",
        "            img = self.filter(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class FilterHigh(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, include_pad=True, \n",
        "            image_channels=3, normalize=True, filter_type=None, kernel=None):\n",
        "        super(FilterHigh, self).__init__()\n",
        "        \n",
        "        # if is standard freq. separator, will use the same LPF to remove LF from image\n",
        "        if filter_type=='gaussian' or filter_type=='average':\n",
        "            self.type = 'separator'\n",
        "            self.filter_low = FilterLow(recursions=1, kernel_size=kernel_size, stride=stride, \n",
        "                image_channels=image_channels, include_pad=include_pad, filter_type=filter_type)\n",
        "        # otherwise, can use any independent filter\n",
        "        else: #load any other filter for the high pass\n",
        "            self.type = 'independent'\n",
        "            #kernel and kernel_size should be provided. Options for edge detectors:\n",
        "            # In both dimensions: get_log_kernel, get_laplacian_kernel_3x3 \n",
        "            # and get_sobel_kernel\n",
        "            # Single dimension: get_prewitt_kernel_3x3, get_scharr_kernel_3x3 \n",
        "            # get_gradient_kernel_3x3 \n",
        "            if include_pad:\n",
        "                pad = compute_padding(kernel_size)\n",
        "            else:\n",
        "                pad = 0\n",
        "            self.filter_low = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                padding=pad, groups=image_channels)\n",
        "        self.recursions = recursions\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, img):\n",
        "        if self.type == 'separator':\n",
        "            if self.recursions > 1:\n",
        "                for i in range(self.recursions - 1):\n",
        "                    img = self.filter_low(img)\n",
        "            img = img - self.filter_low(img)\n",
        "        elif self.type == 'independent':\n",
        "            img = self.filter_low(img)\n",
        "        if self.normalize:\n",
        "            return denorm(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "#TODO: check how similar getting the gradient with get_gradient_kernel_3x3 is from the alternative displacing the image\n",
        "#ref from TF: https://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/python/ops/image_ops_impl.py#L3423\n",
        "def get_image_gradients(image):\n",
        "    \"\"\"Returns image gradients (dy, dx) for each color channel.\n",
        "    Both output tensors have the same shape as the input: [b, c, h, w]. \n",
        "    Places the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y). \n",
        "    That means that dy will always have zeros in the last row,\n",
        "    and dx will always have zeros in the last column.\n",
        "\n",
        "    This can be used to implement the anisotropic 2-D version of the \n",
        "    Total Variation formula:\n",
        "        https://en.wikipedia.org/wiki/Total_variation_denoising\n",
        "    (anisotropic is using l1, isotropic is using l2 norm)\n",
        "    \n",
        "    Arguments:\n",
        "        image: Tensor with shape [b, c, h, w].\n",
        "    Returns:\n",
        "        Pair of tensors (dy, dx) holding the vertical and horizontal image\n",
        "        gradients (1-step finite difference).  \n",
        "    Raises:\n",
        "      ValueError: If `image` is not a 3D image or 4D tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_shape = image.shape\n",
        "      \n",
        "    if len(image_shape) == 3:\n",
        "        # The input is a single image with shape [height, width, channels].\n",
        "        # Calculate the difference of neighboring pixel-values.\n",
        "        # The images are shifted one pixel along the height and width by slicing.\n",
        "        dx = image[:, 1:, :] - image[:, :-1, :] #pixel_dif2, f_v_1-f_v_2\n",
        "        dy = image[1:, :, :] - image[:-1, :, :] #pixel_dif1, f_h_1-f_h_2\n",
        "\n",
        "    elif len(image_shape) == 4:    \n",
        "        # Return tensors with same size as original image\n",
        "        #adds one pixel pad to the right and removes one pixel from the left\n",
        "        right = F.pad(image, [0, 1, 0, 0])[..., :, 1:]\n",
        "        #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "        bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] \n",
        "\n",
        "        #right and bottom have the same dimensions as image\n",
        "        dx, dy = right - image, bottom - image \n",
        "        \n",
        "        #this is required because otherwise results in the last column and row having \n",
        "        # the original pixels from the image\n",
        "        dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "        dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'image_gradients expects a 3D [h, w, c] or 4D tensor '\n",
        "          '[batch_size, c, h, w], not %s.', image_shape)\n",
        "\n",
        "    return dy, dx\n",
        "\n",
        "\n",
        "def get_4dim_image_gradients(image):\n",
        "    # Return tensors with same size as original image\n",
        "    # Place the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y).\n",
        "    right = F.pad(image, [0, 1, 0, 0])[..., :, 1:] #adds one pixel pad to the right and removes one pixel from the left\n",
        "    bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "    botright = F.pad(image, [0, 1, 0, 1])[..., 1:, 1:] #displaces in diagonal direction\n",
        "\n",
        "    dx, dy = right - image, bottom - image #right and bottom have the same dimensions as image\n",
        "    dn, dp = botright - image, right - bottom\n",
        "    #dp is positive diagonal (bottom left to top right)\n",
        "    #dn is negative diagonal (top left to bottom right)\n",
        "    \n",
        "    #this is required because otherwise results in the last column and row having \n",
        "    # the original pixels from the image\n",
        "    dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "    dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    dp[:, :, -1, :] = 0 # dp will always have zeros in the last row\n",
        "\n",
        "    return dy, dx, dp, dn\n",
        "\n",
        "#TODO: #https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "#TODO: https://link.springer.com/article/10.1007/s11220-020-00281-8\n",
        "def grad_orientation(grad_y, grad_x):\n",
        "    go = torch.atan(grad_y / grad_x)\n",
        "    go = go * (360 / np.pi) + 180 # convert to degree\n",
        "    go = torch.round(go / 45) * 45  # keep a split by 45\n",
        "    return go\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OtdTi2RlD-Q",
        "cellView": "form"
      },
      "source": [
        "#@title vic/colors.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/colors.py\n",
        "\"\"\"\n",
        "BasicSR/codes/dataops/colors.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/dataops/colors.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "'''\n",
        "Functions for color operations on tensors.\n",
        "If needed, there are more conversions that can be used:\n",
        "https://github.com/kornia/kornia/tree/master/kornia/color\n",
        "https://github.com/R08UST/Color_Conversion_pytorch/blob/master/differentiable_color_conversion/basic_op.py\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "def bgr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n",
        "    # flip image channels\n",
        "    out: torch.Tensor = image.flip(-3) #https://github.com/pytorch/pytorch/issues/229\n",
        "    #out: torch.Tensor = image[[2, 1, 0], :, :] #RGB to BGR #may be faster\n",
        "    return out\n",
        "\n",
        "def rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgr_to_rgb(), flip image channels\n",
        "    return bgr_to_rgb(image)\n",
        "\n",
        "def bgra_to_rgba(image: torch.Tensor) -> torch.Tensor:\n",
        "    out: torch.Tensor = image[[2, 1, 0, 3], :, :]\n",
        "    return out\n",
        "\n",
        "def rgba_to_bgra(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgra_to_rgba(), flip image channels\n",
        "    return bgra_to_rgba(image)\n",
        "\n",
        "def rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    r, g, b = torch.chunk(input, chunks=3, dim=-3)\n",
        "    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    #gray = rgb_to_yuv(input,consts='y')\n",
        "    return gray\n",
        "\n",
        "def bgr_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    input_rgb = bgr_to_rgb(input)\n",
        "    gray: torch.Tensor = rgb_to_grayscale(input_rgb)\n",
        "    #gray = rgb_to_yuv(input_rgb,consts='y')\n",
        "    return gray\n",
        "\n",
        "def grayscale_to_rgb(input: torch.Tensor) -> torch.Tensor:\n",
        "    #repeat the gray image to the three channels\n",
        "    rgb: torch.Tensor = input.repeat(3, *[1] * (input.dim() - 1))\n",
        "    return rgb\n",
        "\n",
        "def grayscale_to_bgr(input: torch.Tensor) -> torch.Tensor:\n",
        "    return grayscale_to_rgb(input)\n",
        "\n",
        "def rgb_to_ycbcr(input: torch.Tensor, consts='yuv'):\n",
        "    return rgb_to_yuv(input, consts == 'ycbcr')\n",
        "\n",
        "def rgb_to_yuv(input: torch.Tensor, consts='yuv'):\n",
        "    \"\"\"Converts one or more images from RGB to YUV.\n",
        "    Outputs a tensor of the same shape as the `input` image tensor, containing the YUV\n",
        "    value of the pixels.\n",
        "    The output is only well defined if the value in images are in [0,1].\n",
        "    Y′CbCr is often confused with the YUV color space, and typically the terms YCbCr \n",
        "    and YUV are used interchangeably, leading to some confusion. The main difference \n",
        "    is that YUV is analog and YCbCr is digital: https://en.wikipedia.org/wiki/YCbCr\n",
        "    Args:\n",
        "      input: 2-D or higher rank. Image data to convert. Last dimension must be\n",
        "        size 3. (Could add additional channels, ie, AlphaRGB = AlphaYUV)\n",
        "      consts: YUV constant parameters to use. BT.601 or BT.709. Could add YCbCr\n",
        "        https://en.wikipedia.org/wiki/YUV\n",
        "    Returns:\n",
        "      images: images tensor with the same shape as `input`.\n",
        "    \"\"\"\n",
        "    \n",
        "    #channels = input.shape[0]\n",
        "    \n",
        "    if consts == 'BT.709': # HDTV YUV\n",
        "        Wr = 0.2126\n",
        "        Wb = 0.0722\n",
        "        Wg = 1 - Wr - Wb #0.7152\n",
        "        Uc = 0.539\n",
        "        Vc = 0.635\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "    elif consts == 'ycbcr': # Alt. BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.564 #(b-y) #cb\n",
        "        Vc = 0.713 #(r-y) #cr\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    elif consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Ur = -0.147\n",
        "        Ug = -0.289\n",
        "        Ub = 0.436\n",
        "        Vr = 0.615\n",
        "        Vg = -0.515\n",
        "        Vb = -0.100\n",
        "        #delta: float = 0.0\n",
        "    elif consts == 'y': #returns only Y channel, same as rgb_to_grayscale()\n",
        "        #Note: torchvision uses ITU-R 601-2: Wr = 0.2989, Wg = 0.5870, Wb = 0.1140\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "    else: # Default to 'BT.601', SDTV YUV\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.493 #0.492\n",
        "        Vc = 0.877\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "\n",
        "    r: torch.Tensor = input[..., 0, :, :]\n",
        "    g: torch.Tensor = input[..., 1, :, :]\n",
        "    b: torch.Tensor = input[..., 2, :, :]\n",
        "    #TODO\n",
        "    #r, g, b = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    if consts == 'y':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        #(0.2989 * input[0] + 0.5870 * input[1] + 0.1140 * input[2]).to(img.dtype)\n",
        "        return y\n",
        "    elif consts == 'yuvK':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = Ur * r + Ug * g + Ub * b\n",
        "        v: torch.Tensor = Vr * r + Vg * g + Vb * b\n",
        "    else: #if consts == 'ycbcr' or consts == 'yuv' or consts == 'BT.709':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = (b - y) * Uc + delta #cb\n",
        "        v: torch.Tensor = (r - y) * Vc + delta #cr\n",
        "\n",
        "    if consts == 'uv': #returns only UV channels\n",
        "        return torch.stack((u, v), -3)\n",
        "    else:\n",
        "        return torch.stack((y, u, v), -3)\n",
        "\n",
        "def ycbcr_to_rgb(input: torch.Tensor):\n",
        "    return yuv_to_rgb(input, consts = 'ycbcr')\n",
        "\n",
        "def yuv_to_rgb(input: torch.Tensor, consts='yuv') -> torch.Tensor:\n",
        "    if consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 1.14 #1.402\n",
        "        Wb = 2.029 #1.772\n",
        "        Wgu = 0.396 #.344136\n",
        "        Wgv = 0.581 #.714136\n",
        "        delta: float = 0.0\n",
        "    elif consts == 'yuv' or consts == 'ycbcr': # BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 1.403 #1.402\n",
        "        Wb = 1.773 #1.772\n",
        "        Wgu = .344 #.344136\n",
        "        Wgv = .714 #.714136\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    \n",
        "    #Note: https://github.com/R08UST/Color_Conversion_pytorch/blob/75150c5fbfb283ae3adb85c565aab729105bbb66/differentiable_color_conversion/basic_op.py#L65 has u and v flipped\n",
        "    y: torch.Tensor = input[..., 0, :, :]\n",
        "    u: torch.Tensor = input[..., 1, :, :] #cb\n",
        "    v: torch.Tensor = input[..., 2, :, :] #cr\n",
        "    #TODO\n",
        "    #y, u, v = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    u_shifted: torch.Tensor = u - delta #cb\n",
        "    v_shifted: torch.Tensor = v - delta #cr\n",
        "\n",
        "    r: torch.Tensor = y + Wr * v_shifted\n",
        "    g: torch.Tensor = y - Wgv * v_shifted - Wgu * u_shifted\n",
        "    b: torch.Tensor = y + Wb * u_shifted\n",
        "    return torch.stack((r, g, b), -3) \n",
        "\n",
        "#Not tested:\n",
        "def rgb2srgb(imgs):\n",
        "    return torch.where(imgs<=0.04045,imgs/12.92,torch.pow((imgs+0.055)/1.055,2.4))\n",
        "\n",
        "#Not tested:\n",
        "def srgb2rgb(imgs):\n",
        "    return torch.where(imgs<=0.0031308,imgs*12.92,1.055*torch.pow((imgs),1/2.4)-0.055)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chXYGZi5lTVh",
        "cellView": "form"
      },
      "source": [
        "#@title vic/discriminators.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/discriminators.py\n",
        "\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/architectures/discriminators.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/discriminators.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from . import block as B\n",
        "from . import spectral_norm as SN\n",
        "\n",
        "\n",
        "####################\n",
        "# Discriminator\n",
        "####################\n",
        "\n",
        "\n",
        "# VGG style Discriminator\n",
        "class Discriminator_VGG(nn.Module):\n",
        "    def __init__(self, size, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG, self).__init__()\n",
        "\n",
        "        conv_blocks = []\n",
        "        conv_blocks.append(B.conv_block(  in_nc, base_nf, kernel_size=3, stride=1, norm_type=None, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "        conv_blocks.append(B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "\n",
        "        cur_size = size // 2\n",
        "        cur_nc = base_nf\n",
        "        while cur_size > 4:\n",
        "            out_nc = cur_nc * 2 if cur_nc < 512 else cur_nc\n",
        "            conv_blocks.append(B.conv_block(cur_nc, out_nc, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            conv_blocks.append(B.conv_block(out_nc, out_nc, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            cur_nc = out_nc\n",
        "            cur_size //= 2\n",
        "\n",
        "        self.features = B.sequential(*conv_blocks)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 96*96\n",
        "class Discriminator_VGG_96(nn.Module):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_96, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 96, 64\n",
        "        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 48, 64\n",
        "        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 24, 128\n",
        "        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 12, 256\n",
        "        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 6, 512\n",
        "        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 3, 512\n",
        "        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 128*128, Spectral Normalization\n",
        "class Discriminator_VGG_128_SN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator_VGG_128_SN, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        self.conv0 = SN.spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\n",
        "        self.conv1 = SN.spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\n",
        "        # 64, 64\n",
        "        self.conv2 = SN.spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\n",
        "        self.conv3 = SN.spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\n",
        "        # 32, 128\n",
        "        self.conv4 = SN.spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\n",
        "        self.conv5 = SN.spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\n",
        "        # 16, 256\n",
        "        self.conv6 = SN.spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\n",
        "        self.conv7 = SN.spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 8, 512\n",
        "        self.conv8 = SN.spectral_norm(nn.Conv2d(512, 512, 3, 1, 1))\n",
        "        self.conv9 = SN.spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 4, 512\n",
        "\n",
        "        # classifier\n",
        "        self.linear0 = SN.spectral_norm(nn.Linear(512 * 4 * 4, 100))\n",
        "        self.linear1 = SN.spectral_norm(nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.conv0(x))\n",
        "        x = self.lrelu(self.conv1(x))\n",
        "        x = self.lrelu(self.conv2(x))\n",
        "        x = self.lrelu(self.conv3(x))\n",
        "        x = self.lrelu(self.conv4(x))\n",
        "        x = self.lrelu(self.conv5(x))\n",
        "        x = self.lrelu(self.conv6(x))\n",
        "        x = self.lrelu(self.conv7(x))\n",
        "        x = self.lrelu(self.conv8(x))\n",
        "        x = self.lrelu(self.conv9(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lrelu(self.linear0(x))\n",
        "        x = self.linear1(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 128*128\n",
        "class Discriminator_VGG_128(nn.Module):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_128, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 64\n",
        "        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 128\n",
        "        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 256\n",
        "        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 4, 512\n",
        "        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 192*192\n",
        "class Discriminator_VGG_192(nn.Module): #vic in PPON is called Discriminator_192 \n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_192, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 192, 64\n",
        "        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode) # 3-->64\n",
        "        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->64, 96*96\n",
        "        # 96, 64\n",
        "        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->128\n",
        "        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->128, 48*48\n",
        "        # 48, 128\n",
        "        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->256\n",
        "        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->256, 24*24\n",
        "        # 24, 256\n",
        "        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->512\n",
        "        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 12*12\n",
        "        # 12, 512\n",
        "        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512\n",
        "        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 6*6\n",
        "        # 6, 512\n",
        "        conv10 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 3, 512\n",
        "        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1)) #vic PPON uses 128 and 128 instead of 100\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 256*256\n",
        "class Discriminator_VGG_256(nn.Module):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_256, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 256, 64\n",
        "        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 128, 64\n",
        "        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 128\n",
        "        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 256\n",
        "        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 512\n",
        "        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv10 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 4, 512\n",
        "        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "####################\n",
        "# Perceptual Network\n",
        "####################\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 feature_layer=34,\n",
        "                 use_bn=False,\n",
        "                 use_input_norm=True,\n",
        "                 device=torch.device('cpu'),\n",
        "                 z_norm=False): #Note: PPON uses cuda instead of CPU\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        if use_bn:\n",
        "            model = torchvision.models.vgg19_bn(pretrained=True)\n",
        "        else:\n",
        "            model = torchvision.models.vgg19(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device) \n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)                 \n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(nn.Module):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = torchvision.models.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MINCNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MINCNet, self).__init__()\n",
        "        self.ReLU = nn.ReLU(True)\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv31 = nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.maxpool3 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv41 = nn.Conv2d(256, 512, 3, 1, 1)\n",
        "        self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.maxpool4 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv51 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ReLU(self.conv11(x))\n",
        "        out = self.ReLU(self.conv12(out))\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.ReLU(self.conv21(out))\n",
        "        out = self.ReLU(self.conv22(out))\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.ReLU(self.conv31(out))\n",
        "        out = self.ReLU(self.conv32(out))\n",
        "        out = self.ReLU(self.conv33(out))\n",
        "        out = self.maxpool3(out)\n",
        "        out = self.ReLU(self.conv41(out))\n",
        "        out = self.ReLU(self.conv42(out))\n",
        "        out = self.ReLU(self.conv43(out))\n",
        "        out = self.maxpool4(out)\n",
        "        out = self.ReLU(self.conv51(out))\n",
        "        out = self.ReLU(self.conv52(out))\n",
        "        out = self.conv53(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class MINCFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, \\\n",
        "                device=torch.device('cpu')):\n",
        "        super(MINCFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.features = MINCNet()\n",
        "        self.features.load_state_dict(\n",
        "            torch.load('../experiments/pretrained_models/VGG16minc_53.pth'), strict=True)\n",
        "        self.features.eval()\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "#TODO\n",
        "# moved from models.modules.architectures.ASRResNet_arch, did not bring the self-attention layer\n",
        "# VGG style Discriminator with input size 128*128, with feature_maps extraction and self-attention\n",
        "class Discriminator_VGG_128_fea(nn.Module):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "         arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4):\n",
        "        super(Discriminator_VGG_128_fea, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        \n",
        "        # Self-Attention configuration\n",
        "        '''#TODO\n",
        "        self.self_attention = self_attention\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        '''\n",
        "        \n",
        "        # Remove BatchNorm2d if using spectral_norm\n",
        "        if spectral_norm:\n",
        "            norm_type = None\n",
        "        \n",
        "        self.conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        self.conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 64, 64\n",
        "        self.conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 32, 128\n",
        "        self.conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 16, 256\n",
        "        \n",
        "        '''#TODO\n",
        "        if self.self_attention:\n",
        "            self.FSA = SelfAttentionBlock(in_dim = base_nf*4, max_pool=self.max_pool, poolsize = self.poolsize, spectral_norm=spectral_norm)\n",
        "        '''\n",
        "\n",
        "        self.conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 8, 512\n",
        "        self.conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 4, 512\n",
        "        # self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            # conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    #TODO: modify to a listening dictionary like VGG_Model(), can select what maps to use\n",
        "    def forward(self, x, return_maps=False):\n",
        "        feature_maps = []\n",
        "        # x = self.features(x)\n",
        "        x = self.conv0(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv1(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv2(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv3(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv4(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv5(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv6(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv7(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv8(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv9(x)\n",
        "        feature_maps.append(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        if return_maps:\n",
        "            return [x, feature_maps]\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    r\"\"\"\n",
        "    PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1611.07004v3.pdf\n",
        "    https://arxiv.org/pdf/1803.07422.pdf\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        #self.getIntermFeat = getIntermFeat # not used for now\n",
        "        #use_sigmoid not used for now\n",
        "        #TODO: test if there are benefits by incorporating the use of intermediate features from pix2pixHD\n",
        "\n",
        "        use_bias = False\n",
        "        kw = 4\n",
        "        padw = 1 # int(np.ceil((kw-1.0)/2))\n",
        "\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MultiscaleDiscriminator(nn.Module):\n",
        "    r\"\"\"\n",
        "    Multiscale PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1711.11585.pdf\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n",
        "        \"\"\"Construct a pyramid of PatchGAN discriminators\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "            use_sigmoid     -- boolean to use sigmoid in patchGAN discriminators\n",
        "            num_D (int)     -- number of discriminators/downscales in the pyramid\n",
        "            getIntermFeat   -- boolean to get intermediate features (unused for now)\n",
        "        \"\"\"\n",
        "        super(MultiscaleDiscriminator, self).__init__()\n",
        "        self.num_D = num_D\n",
        "        self.n_layers = n_layers\n",
        "        self.getIntermFeat = getIntermFeat\n",
        "     \n",
        "        for i in range(num_D):\n",
        "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
        "            if getIntermFeat:                                \n",
        "                for j in range(n_layers+2):\n",
        "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
        "            else:\n",
        "                setattr(self, 'layer'+str(i), netD.model)\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def singleD_forward(self, model, input):\n",
        "        if self.getIntermFeat:\n",
        "            result = [input]\n",
        "            for i in range(len(model)):\n",
        "                result.append(model[i](result[-1]))\n",
        "            return result[1:]\n",
        "        else:\n",
        "            return [model(input)]\n",
        "\n",
        "    def forward(self, input):        \n",
        "        num_D = self.num_D\n",
        "        result = []\n",
        "        input_downsampled = input\n",
        "        for i in range(num_D):\n",
        "            if self.getIntermFeat:\n",
        "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
        "            else:\n",
        "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
        "            result.append(self.singleD_forward(model, input_downsampled))\n",
        "            if i != (num_D-1):\n",
        "                input_downsampled = self.downsample(input_downsampled)\n",
        "        return result\n",
        "\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        use_bias = False\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0CR8cUmwMOB",
        "cellView": "form"
      },
      "source": [
        "#@title vic/block.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/block.py\n",
        "\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/architectures/block.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/block.py\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from models.modules.architectures.convolutions.partialconv2d import PartialConv2d #TODO\n",
        "from .partialconv2d import PartialConv2d\n",
        "\n",
        "####################\n",
        "# Basic blocks\n",
        "####################\n",
        "\n",
        "# Swish activation funtion\n",
        "def swish_func(x, beta=1.0):\n",
        "    '''\n",
        "    \"Swish: a Self-Gated Activation Function\"\n",
        "    Searching for Activation Functions (https://arxiv.org/abs/1710.05941)\n",
        "    \n",
        "    If beta=1 applies the Sigmoid Linear Unit (SiLU) function element-wise\n",
        "    If beta=0, Swish becomes the scaled linear function (identity \n",
        "      activation) f(x) = x/2\n",
        "    As beta -> ∞, the sigmoid component converges to approach a 0-1 function\n",
        "      (unit step), and multiplying that by x gives us f(x)=2max(0,x), which \n",
        "      is the ReLU multiplied by a constant factor of 2, so Swish becomes like \n",
        "      the ReLU function.\n",
        "        \n",
        "    Including beta, Swish can be loosely viewed as a smooth function that \n",
        "      nonlinearly interpolate between identity (linear) and ReLU function.\n",
        "      The degree of interpolation can be controlled by the model if beta is \n",
        "      set as a trainable parameter.\n",
        "      \n",
        "    Alt: 1.78718727865 * (x * sigmoid(x) - 0.20662096414)\n",
        "    '''\n",
        "    \n",
        "    # In-place implementation, may consume less GPU memory: \n",
        "    \"\"\" \n",
        "    result = x.clone()\n",
        "    torch.sigmoid_(beta*x)\n",
        "    x *= result\n",
        "    return x\n",
        "    #\"\"\"\n",
        "    \n",
        "    # Normal out-of-place implementation:\n",
        "    #\"\"\"\n",
        "    return x * torch.sigmoid(beta*x)\n",
        "    #\"\"\"\n",
        "    \n",
        "# Swish module\n",
        "class Swish(nn.Module):\n",
        "    \n",
        "    __constants__ = ['beta', 'slope', 'inplace']\n",
        "    \n",
        "    def __init__(self, beta = 1.0, slope = 1.67653251702, inplace=False):\n",
        "        '''\n",
        "        Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        #self.beta = beta # user-defined beta parameter, non-trainable\n",
        "        #self.beta = beta * torch.nn.Parameter(torch.ones(1)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta = torch.nn.Parameter(torch.tensor(beta)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta.requiresGrad = True # set requiresGrad to true to make it trainable\n",
        "\n",
        "        self.slope = slope/2 # user-defined \"slope\", non-trainable\n",
        "        #self.slope = slope * torch.nn.Parameter(torch.ones(1)) # learnable slope parameter, create a tensor out of slope\n",
        "        #self.slope = torch.nn.Parameter(torch.tensor(slope)) # learnable slope parameter, create a tensor out of slope\n",
        "        #self.slope.requiresGrad = True # set requiresGrad to true to true to make it trainable\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        # Disabled, using inplace causes:\n",
        "        # \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"\n",
        "        if self.inplace:\n",
        "            input.mul_(torch.sigmoid(self.beta*input))\n",
        "            return 2 * self.slope * input\n",
        "        else:\n",
        "            return 2 * self.slope * swish_func(input, self.beta)\n",
        "        \"\"\"\n",
        "        return 2 * self.slope * swish_func(input, self.beta)\n",
        "        \n",
        "\n",
        "def act(act_type, inplace=True, neg_slope=0.2, n_prelu=1, beta=1.0):\n",
        "    # helper selecting activation\n",
        "    # neg_slope: for leakyrelu and init of prelu\n",
        "    # n_prelu: for p_relu num_parameters\n",
        "    # beta: for swish\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type == 'leakyrelu' or act_type == 'lrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    elif act_type == 'Tanh' or act_type == 'tanh' : # [-1, 1] range output\n",
        "        layer = nn.Tanh()\n",
        "    elif act_type == 'sigmoid': # [0, 1] range output\n",
        "        layer = nn.Sigmoid()\n",
        "    elif act_type == 'swish':\n",
        "        layer = Swish(beta=beta,inplace=inplace)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def norm(norm_type, nc):\n",
        "    # helper selecting normalization layer\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'batch':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def pad(pad_type, padding):\n",
        "    # helper selecting padding layer\n",
        "    # if padding is 'zero', do by conv layers\n",
        "    pad_type = pad_type.lower()\n",
        "    if padding == 0:\n",
        "        return None\n",
        "    if pad_type == 'reflect':\n",
        "        layer = nn.ReflectionPad2d(padding)\n",
        "    elif pad_type == 'replicate':\n",
        "        layer = nn.ReplicationPad2d(padding)\n",
        "    else:\n",
        "        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def get_valid_padding(kernel_size, dilation):\n",
        "    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    return padding\n",
        "\n",
        "\n",
        "class ConcatBlock(nn.Module):\n",
        "    # Concat the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ConcatBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.cat((x, self.sub(x)), dim=1)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = 'Identity .. \\n|'\n",
        "        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n",
        "        tmpstr = tmpstr + modstr\n",
        "        return tmpstr\n",
        "\n",
        "\n",
        "class ShortcutBlock(nn.Module):\n",
        "    #Elementwise sum the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ShortcutBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x + self.sub(x)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = 'Identity + \\n|'\n",
        "        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n",
        "        tmpstr = tmpstr + modstr\n",
        "        return tmpstr\n",
        "\n",
        "\n",
        "def sequential(*args):\n",
        "    # Flatten Sequential. It unwraps nn.Sequential.\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n",
        "               pad_type='zero', norm_type=None, act_type='relu', mode='CNA', convtype='Conv2D', \\\n",
        "               spectral_norm=False):\n",
        "    '''\n",
        "    Conv layer with padding, normalization, activation\n",
        "    mode: CNA --> Conv -> Norm -> Act\n",
        "        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n",
        "    '''\n",
        "    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wrong conv mode [{:s}]'.format(mode)\n",
        "    padding = get_valid_padding(kernel_size, dilation)\n",
        "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
        "    padding = padding if pad_type == 'zero' else 0\n",
        "    \n",
        "    if convtype=='PartialConv2D':\n",
        "        c = PartialConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    else: #default case is standard 'Conv2D':\n",
        "        c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "                dilation=dilation, bias=bias, groups=groups) #normal conv2d\n",
        "            \n",
        "    if spectral_norm:\n",
        "        c = nn.utils.spectral_norm(c)\n",
        "    \n",
        "    a = act(act_type) if act_type else None\n",
        "    if 'CNA' in mode:\n",
        "        n = norm(norm_type, out_nc) if norm_type else None\n",
        "        return sequential(p, c, n, a)\n",
        "    elif mode == 'NAC':\n",
        "        if norm_type is None and act_type is not None:\n",
        "            a = act(act_type, inplace=False)\n",
        "            # Important!\n",
        "            # input----ReLU(inplace)----Conv--+----output\n",
        "            #        |________________________|\n",
        "            # inplace ReLU will modify the input, therefore wrong output\n",
        "        n = norm(norm_type, in_nc) if norm_type else None\n",
        "        return sequential(n, a, p, c)\n",
        "\n",
        "\n",
        "####################\n",
        "# Useful blocks\n",
        "####################\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    '''\n",
        "    ResNet Block, 3-3 style\n",
        "    with extra residual scaling used in EDSR\n",
        "    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_nc, mid_nc, out_nc, kernel_size=3, stride=1, dilation=1, groups=1, \\\n",
        "            bias=True, pad_type='zero', norm_type=None, act_type='relu', mode='CNA', res_scale=1, convtype='Conv2D'):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        conv0 = conv_block(in_nc, mid_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n",
        "            norm_type, act_type, mode, convtype)\n",
        "        if mode == 'CNA':\n",
        "            act_type = None\n",
        "        if mode == 'CNAC':  # Residual path: |-CNAC-|\n",
        "            act_type = None\n",
        "            norm_type = None\n",
        "        conv1 = conv_block(mid_nc, out_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n",
        "            norm_type, act_type, mode, convtype)\n",
        "        # if in_nc != out_nc:\n",
        "        #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \\\n",
        "        #         None, None)\n",
        "        #     print('Need a projecter in ResNetBlock.')\n",
        "        # else:\n",
        "        #     self.project = lambda x:x\n",
        "        self.res = sequential(conv0, conv1)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.res(x).mul(self.res_scale)\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class ResidualDenseBlock_5C(nn.Module):\n",
        "    '''\n",
        "    Residual Dense Block\n",
        "    style: 5 convs\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    Modified options that can be used:\n",
        "        - \"Partial Convolution based Padding\" arXiv:1811.11718\n",
        "        - \"Spectral normalization\" arXiv:1802.05957\n",
        "        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C. {Rakotonirina} and A. {Rasoanaivo}\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(ResidualDenseBlock_5C, self).__init__()\n",
        "        # gc: growth channel, i.e. intermediate channels\n",
        "        \n",
        "        ## +\n",
        "        self.noise = GaussianNoise() if gaussian_noise else None\n",
        "        self.conv1x1 = conv1x1(nc, gc) if plus else None\n",
        "        ## +\n",
        "\n",
        "        self.conv1 = conv_block(nc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv_block(nc+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(nc+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv_block(nc+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "        if mode == 'CNA':\n",
        "            last_act = None\n",
        "        else:\n",
        "            last_act = act_type\n",
        "        self.conv5 = conv_block(nc+4*gc, nc, 3, stride, bias=bias, pad_type=pad_type, \\\n",
        "            norm_type=norm_type, act_type=last_act, mode=mode, convtype=convtype, \\\n",
        "            spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
        "        if self.conv1x1:\n",
        "            x2 = x2 + self.conv1x1(x) #+\n",
        "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
        "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
        "        if self.conv1x1:\n",
        "            x4 = x4 + x2 #+\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        if self.noise:\n",
        "            return self.noise(x5.mul(0.2) + x)\n",
        "        else:\n",
        "            return x5.mul(0.2) + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    '''\n",
        "    Residual in Residual Dense Block\n",
        "    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.RDB1 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                gaussian_noise=gaussian_noise, plus=plus)\n",
        "        self.RDB2 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                gaussian_noise=gaussian_noise, plus=plus)\n",
        "        self.RDB3 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
        "                norm_type, act_type, mode, convtype, spectral_norm=spectral_norm, \\\n",
        "                gaussian_noise=gaussian_noise, plus=plus)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.RDB1(x)\n",
        "        out = self.RDB2(out)\n",
        "        out = self.RDB3(out)\n",
        "        return out.mul(0.2) + x\n",
        "            \n",
        "\n",
        "\n",
        "#PPON\n",
        "class _ResBlock_32(nn.Module):\n",
        "    def __init__(self, nc=64):\n",
        "        super(_ResBlock_32, self).__init__()\n",
        "        self.c1 = conv_layer(nc, nc, 3, 1, 1)\n",
        "        self.d1 = conv_layer(nc, nc//2, 3, 1, 1)  # rate=1\n",
        "        self.d2 = conv_layer(nc, nc//2, 3, 1, 2)  # rate=2\n",
        "        self.d3 = conv_layer(nc, nc//2, 3, 1, 3)  # rate=3\n",
        "        self.d4 = conv_layer(nc, nc//2, 3, 1, 4)  # rate=4\n",
        "        self.d5 = conv_layer(nc, nc//2, 3, 1, 5)  # rate=5\n",
        "        self.d6 = conv_layer(nc, nc//2, 3, 1, 6)  # rate=6\n",
        "        self.d7 = conv_layer(nc, nc//2, 3, 1, 7)  # rate=7\n",
        "        self.d8 = conv_layer(nc, nc//2, 3, 1, 8)  # rate=8\n",
        "        self.act = act('lrelu')\n",
        "        self.c2 = conv_layer(nc * 4, nc, 1, 1, 1)  # 256-->64\n",
        "\n",
        "    def forward(self, input):\n",
        "        output1 = self.act(self.c1(input))\n",
        "        d1 = self.d1(output1)\n",
        "        d2 = self.d2(output1)\n",
        "        d3 = self.d3(output1)\n",
        "        d4 = self.d4(output1)\n",
        "        d5 = self.d5(output1)\n",
        "        d6 = self.d6(output1)\n",
        "        d7 = self.d7(output1)\n",
        "        d8 = self.d8(output1)\n",
        "\n",
        "        add1 = d1 + d2\n",
        "        add2 = add1 + d3\n",
        "        add3 = add2 + d4\n",
        "        add4 = add3 + d5\n",
        "        add5 = add4 + d6\n",
        "        add6 = add5 + d7\n",
        "        add7 = add6 + d8\n",
        "\n",
        "        combine = torch.cat([d1, add1, add2, add3, add4, add5, add6, add7], 1)\n",
        "        output2 = self.c2(self.act(combine))\n",
        "        output = input + output2.mul(0.2)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RRBlock_32(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RRBlock_32, self).__init__()\n",
        "        self.RB1 = _ResBlock_32()\n",
        "        self.RB2 = _ResBlock_32()\n",
        "        self.RB3 = _ResBlock_32()\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.RB1(input)\n",
        "        out = self.RB2(out)\n",
        "        out = self.RB3(out)\n",
        "        return out.mul(0.2) + input\n",
        "\n",
        "\n",
        "####################\n",
        "# Upsampler\n",
        "####################\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    #To prevent warning: nn.Upsample is deprecated\n",
        "    #https://discuss.pytorch.org/t/which-function-is-better-for-upsampling-upsampling-or-interpolate/21811/8\n",
        "    #From: https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#Upsample\n",
        "    #Alternative: https://discuss.pytorch.org/t/using-nn-function-interpolate-inside-nn-sequential/23588/2?u=ptrblck\n",
        "    \n",
        "    def __init__(self, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "        super(Upsample, self).__init__()\n",
        "        if isinstance(scale_factor, tuple):\n",
        "            self.scale_factor = tuple(float(factor) for factor in scale_factor)\n",
        "        else:\n",
        "            self.scale_factor = float(scale_factor) if scale_factor else None\n",
        "        self.mode = mode\n",
        "        self.size = size\n",
        "        self.align_corners = align_corners\n",
        "        #self.interp = nn.functional.interpolate\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "        #return self.interp(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        if self.scale_factor is not None:\n",
        "            info = 'scale_factor=' + str(self.scale_factor)\n",
        "        else:\n",
        "            info = 'size=' + str(self.size)\n",
        "        info += ', mode=' + self.mode\n",
        "        return info\n",
        "\n",
        "def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                        pad_type='zero', norm_type=None, act_type='relu', convtype='Conv2D'):\n",
        "    '''\n",
        "    Pixel shuffle layer\n",
        "    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n",
        "    Neural Network, CVPR17)\n",
        "    '''\n",
        "    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=None, act_type=None, convtype=convtype)\n",
        "    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "    n = norm(norm_type, out_nc) if norm_type else None\n",
        "    a = act(act_type) if act_type else None\n",
        "    return sequential(conv, pixel_shuffle, n, a)\n",
        "\n",
        "def upconv_blcok(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                pad_type='zero', norm_type=None, act_type='relu', mode='nearest', convtype='Conv2D'):\n",
        "    # Up conv\n",
        "    # described in https://distill.pub/2016/deconv-checkerboard/\n",
        "    #upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n",
        "    upsample = Upsample(scale_factor=upscale_factor, mode=mode) #Updated to prevent the \"nn.Upsample is deprecated\" Warning\n",
        "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type, convtype=convtype)\n",
        "    return sequential(upsample, conv)\n",
        "\n",
        "#PPON\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1):\n",
        "    padding = int((kernel_size - 1) / 2) * dilation\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=True, dilation=dilation, groups=groups)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "#ESRGANplus\n",
        "####################\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=False):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0, dtype=torch.float).to(torch.device('cuda'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x \n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "#TODO: Not used:\n",
        "# https://github.com/github-pengge/PyTorch-progressive_growing_of_gans/blob/master/models/base_model.py\n",
        "class minibatch_std_concat_layer(nn.Module):\n",
        "    def __init__(self, averaging='all'):\n",
        "        super(minibatch_std_concat_layer, self).__init__()\n",
        "        self.averaging = averaging.lower()\n",
        "        if 'group' in self.averaging:\n",
        "            self.n = int(self.averaging[5:])\n",
        "        else:\n",
        "            assert self.averaging in ['all', 'flat', 'spatial', 'none', 'gpool'], 'Invalid averaging mode'%self.averaging\n",
        "        self.adjusted_std = lambda x, **kwargs: torch.sqrt(torch.mean((x - torch.mean(x, **kwargs)) ** 2, **kwargs) + 1e-8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = list(x.size())\n",
        "        target_shape = copy.deepcopy(shape)\n",
        "        vals = self.adjusted_std(x, dim=0, keepdim=True)\n",
        "        if self.averaging == 'all':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.mean(vals, dim=1, keepdim=True)\n",
        "        elif self.averaging == 'spatial':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(vals, axis=[2,3], keepdim=True)             # torch.mean(torch.mean(vals, 2, keepdim=True), 3, keepdim=True)\n",
        "        elif self.averaging == 'none':\n",
        "            target_shape = [target_shape[0]] + [s for s in target_shape[1:]]\n",
        "        elif self.averaging == 'gpool':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(x, [0,2,3], keepdim=True)                   # torch.mean(torch.mean(torch.mean(x, 2, keepdim=True), 3, keepdim=True), 0, keepdim=True)\n",
        "        elif self.averaging == 'flat':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.FloatTensor([self.adjusted_std(x)])\n",
        "        else:                                                           # self.averaging == 'group'\n",
        "            target_shape[1] = self.n\n",
        "            vals = vals.view(self.n, self.shape[1]/self.n, self.shape[2], self.shape[3])\n",
        "            vals = mean(vals, axis=0, keepdim=True).view(1, self.n, 1, 1)\n",
        "        vals = vals.expand(*target_shape)\n",
        "        return torch.cat([x, vals], 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Fb3HFiwfLp",
        "cellView": "form"
      },
      "source": [
        "#@title vic/partialconv2d.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/partialconv2d.py\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/architectures/convolutions/partialconv2d.py (8-Nov-20)\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/convolutions/partialconv2d.py\n",
        "\"\"\"\n",
        "\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "#\n",
        "# Source: https://github.com/NVIDIA/partialconv/blob/master/models/partialconv2d.py\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, cuda\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False  \n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "            \n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "                        \n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                #make sure the value of self.mask_ratio for the entries in the interior (no need for padding) have value 1. If not, you replace with the line below.\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        # if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "        #     self.update_mask.to(input)\n",
        "        #     self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klWCIQLhv5_g",
        "cellView": "form"
      },
      "source": [
        "#@title vic/spectral_norm.py\n",
        "%%writefile /content/Colab-edge-connect/src/vic/spectral_norm.py\n",
        "\"\"\"\n",
        "BasicSR/codes/models/modules/architectures/spectral_norm.py\n",
        "https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/spectral_norm.py\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "Copy from pytorch github repo\n",
        "Spectral Normalization from https://arxiv.org/abs/1802.05957\n",
        "'''\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SpectralNorm(object):\n",
        "    def __init__(self, name='weight', n_power_iterations=1, dim=0, eps=1e-12):\n",
        "        self.name = name\n",
        "        self.dim = dim\n",
        "        if n_power_iterations <= 0:\n",
        "            raise ValueError('Expected n_power_iterations to be positive, but '\n",
        "                             'got n_power_iterations={}'.format(n_power_iterations))\n",
        "        self.n_power_iterations = n_power_iterations\n",
        "        self.eps = eps\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        u = getattr(module, self.name + '_u')\n",
        "        weight_mat = weight\n",
        "        if self.dim != 0:\n",
        "            # permute dim to front\n",
        "            weight_mat = weight_mat.permute(self.dim,\n",
        "                                            *[d for d in range(weight_mat.dim()) if d != self.dim])\n",
        "        height = weight_mat.size(0)\n",
        "        weight_mat = weight_mat.reshape(height, -1)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.n_power_iterations):\n",
        "                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n",
        "                # are the first left and right singular vectors.\n",
        "                # This power iteration produces approximations of `u` and `v`.\n",
        "                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n",
        "                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps)\n",
        "\n",
        "        sigma = torch.dot(u, torch.matmul(weight_mat, v))\n",
        "        weight = weight / sigma\n",
        "        return weight, u\n",
        "\n",
        "    def remove(self, module):\n",
        "        weight = getattr(module, self.name)\n",
        "        delattr(module, self.name)\n",
        "        delattr(module, self.name + '_u')\n",
        "        delattr(module, self.name + '_orig')\n",
        "        module.register_parameter(self.name, torch.nn.Parameter(weight))\n",
        "\n",
        "    def __call__(self, module, inputs):\n",
        "        if module.training:\n",
        "            weight, u = self.compute_weight(module)\n",
        "            setattr(module, self.name, weight)\n",
        "            setattr(module, self.name + '_u', u)\n",
        "        else:\n",
        "            r_g = getattr(module, self.name + '_orig').requires_grad\n",
        "            getattr(module, self.name).detach_().requires_grad_(r_g)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, name, n_power_iterations, dim, eps):\n",
        "        fn = SpectralNorm(name, n_power_iterations, dim, eps)\n",
        "        weight = module._parameters[name]\n",
        "        height = weight.size(dim)\n",
        "\n",
        "        u = normalize(weight.new_empty(height).normal_(0, 1), dim=0, eps=fn.eps)\n",
        "        delattr(module, fn.name)\n",
        "        module.register_parameter(fn.name + \"_orig\", weight)\n",
        "        # We still need to assign weight back as fn.name because all sorts of\n",
        "        # things may assume that it exists, e.g., when initializing weights.\n",
        "        # However, we can't directly assign as it could be an nn.Parameter and\n",
        "        # gets added as a parameter. Instead, we register weight.data as a\n",
        "        # buffer, which will cause weight to be included in the state dict\n",
        "        # and also supports nn.init due to shared storage.\n",
        "        module.register_buffer(fn.name, weight.data)\n",
        "        module.register_buffer(fn.name + \"_u\", u)\n",
        "\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "\n",
        "def spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None):\n",
        "    r\"\"\"Applies spectral normalization to a parameter in the given module.\n",
        "\n",
        "    .. math::\n",
        "         \\mathbf{W} &= \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n",
        "         \\sigma(\\mathbf{W}) &= \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n",
        "\n",
        "    Spectral normalization stabilizes the training of discriminators (critics)\n",
        "    in Generaive Adversarial Networks (GANs) by rescaling the weight tensor\n",
        "    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n",
        "    power iteration method. If the dimension of the weight tensor is greater\n",
        "    than 2, it is reshaped to 2D in power iteration method to get spectral\n",
        "    norm. This is implemented via a hook that calculates spectral norm and\n",
        "    rescales weight before every :meth:`~Module.forward` call.\n",
        "\n",
        "    See `Spectral Normalization for Generative Adversarial Networks`_ .\n",
        "\n",
        "    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "        n_power_iterations (int, optional): number of power iterations to\n",
        "            calculate spectal norm\n",
        "        eps (float, optional): epsilon for numerical stability in\n",
        "            calculating norms\n",
        "        dim (int, optional): dimension corresponding to number of outputs,\n",
        "            the default is 0, except for modules that are instances of\n",
        "            ConvTranspose1/2/3d, when it is 1\n",
        "\n",
        "    Returns:\n",
        "        The original module with the spectal norm hook\n",
        "\n",
        "    Example::\n",
        "\n",
        "        >>> m = spectral_norm(nn.Linear(20, 40))\n",
        "        Linear (20 -> 40)\n",
        "        >>> m.weight_u.size()\n",
        "        torch.Size([20])\n",
        "\n",
        "    \"\"\"\n",
        "    if dim is None:\n",
        "        if isinstance(\n",
        "                module,\n",
        "            (torch.nn.ConvTranspose1d, torch.nn.ConvTranspose2d, torch.nn.ConvTranspose3d)):\n",
        "            dim = 1\n",
        "        else:\n",
        "            dim = 0\n",
        "    SpectralNorm.apply(module, name, n_power_iterations, dim, eps)\n",
        "    return module\n",
        "\n",
        "\n",
        "def remove_spectral_norm(module, name='weight'):\n",
        "    r\"\"\"Removes the spectral normalization reparameterization from a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "\n",
        "    Example:\n",
        "        >>> m = spectral_norm(nn.Linear(40, 10))\n",
        "        >>> remove_spectral_norm(m)\n",
        "    \"\"\"\n",
        "    for k, hook in module._forward_pre_hooks.items():\n",
        "        if isinstance(hook, SpectralNorm) and hook.name == name:\n",
        "            hook.remove(module)\n",
        "            del module._forward_pre_hooks[k]\n",
        "            return module\n",
        "\n",
        "    raise ValueError(\"spectral_norm of '{}' not found in {}\".format(name, module))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8gykTkHmJVV",
        "cellView": "form"
      },
      "source": [
        "#@title diffaug.py\n",
        "%%writefile /content/Colab-edge-connect/src/diffaug.py\n",
        "\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "import torch\n",
        "import torch.nn.functional as F \n",
        "\n",
        "policy = 'color,translation,cutout' \n",
        "\n",
        "import torch.nn.functional as nnf\n",
        "import random\n",
        "\n",
        "#torch.autograd.set_detect_anomaly(True)\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4LMCSFnmcGG",
        "cellView": "form"
      },
      "source": [
        "#@title models.py\n",
        "%%writefile /content/Colab-edge-connect/src/models.py\n",
        "\n",
        "from .vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss\n",
        "from .vic.filters import *\n",
        "from .vic.colors import *\n",
        "from .vic.discriminators import *\n",
        "from .diffaug import *\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from .networks import InpaintGenerator, EdgeGenerator, Discriminator\n",
        "from .loss import AdversarialLoss, PerceptualLoss, StyleLoss\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, name, config):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.config = config\n",
        "        self.iteration = 0\n",
        "        self.mosaic_test = config.MOSAIC_TEST\n",
        "\n",
        "\t\t# loading previous weights\n",
        "        self.gen_weights_path = os.path.join(config.PATH, name + '_gen.pth')\n",
        "        self.dis_weights_path = os.path.join(config.PATH, name + '_dis.pth')\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.gen_weights_path):\n",
        "            print('Loading %s generator...' % self.name)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = torch.load(self.gen_weights_path)\n",
        "            else:\n",
        "                data = torch.load(self.gen_weights_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "            self.generator.load_state_dict(data['generator'])\n",
        "            self.iteration = data['iteration']\n",
        "\n",
        "        # load discriminator only when training\n",
        "        if self.config.MODE == 1 and os.path.exists(self.dis_weights_path):\n",
        "            print('Loading %s discriminator...' % self.name)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = torch.load(self.dis_weights_path)\n",
        "            else:\n",
        "                data = torch.load(self.dis_weights_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "            if self.config.DISCRIMINATOR == 'default':\n",
        "              self.discriminator.load_state_dict(data['discriminator'])\n",
        "            if self.config.DISCRIMINATOR == 'pixel':\n",
        "              self.PixelDiscriminator.load_state_dict(data['discriminator'])\n",
        "            if self.config.DISCRIMINATOR == 'patch':\n",
        "              self.NLayerDiscriminator.load_state_dict(data['discriminator'])\n",
        "\n",
        "    def save(self):\n",
        "        print('\\nsaving %s...\\n' % self.name)\n",
        "        torch.save({\n",
        "            'iteration': self.iteration,\n",
        "            'generator': self.generator.state_dict()\n",
        "        }, os.path.join(self.config.PATH, self.name + \"_\" + str(self.iteration) + \"_gen.pth\"))\n",
        "\n",
        "        if self.config.DISCRIMINATOR == 'default':\n",
        "          torch.save({\n",
        "              #'discriminator': self.discriminator.state_dict()\n",
        "              'discriminator': self.PixelDiscriminator.state_dict()\n",
        "          }, os.path.join(self.config.PATH, self.name + \"_\" + str(self.iteration) + \"_dis.pth\"))\n",
        "        if self.config.DISCRIMINATOR == 'pixel':\n",
        "          torch.save({\n",
        "              'discriminator': self.PixelDiscriminator.state_dict()\n",
        "          }, os.path.join(self.config.PATH, self.name + \"_\" + str(self.iteration) + \"_dis.pth\"))\n",
        "        if self.config.DISCRIMINATOR == 'patch':\n",
        "          torch.save({\n",
        "              'discriminator': self.NLayerDiscriminator.state_dict()\n",
        "          }, os.path.join(self.config.PATH, self.name + \"_\" + str(self.iteration) + \"_dis.pth\"))\n",
        "\n",
        "class EdgeModel(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super(EdgeModel, self).__init__('EdgeModel', config)\n",
        "\n",
        "        # generator input: [grayscale(1) + edge(1) + mask(1)]\n",
        "        # discriminator input: (grayscale(1) + edge(1))\n",
        "        generator = EdgeGenerator(use_spectral_norm=True)\n",
        "        discriminator = Discriminator(in_channels=2, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
        "        if len(config.GPU) > 1:\n",
        "            generator = nn.DataParallel(generator, config.GPU)\n",
        "            discriminator = nn.DataParallel(discriminator, config.GPU)\n",
        "        l1_loss = nn.L1Loss()\n",
        "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
        "\n",
        "        self.add_module('generator', generator)\n",
        "        self.add_module('discriminator', discriminator)\n",
        "\n",
        "        self.add_module('l1_loss', l1_loss)\n",
        "        self.add_module('adversarial_loss', adversarial_loss)\n",
        "\n",
        "        self.gen_optimizer = optim.Adam(\n",
        "            params=generator.parameters(),\n",
        "            lr=float(config.LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "        self.dis_optimizer = optim.Adam(\n",
        "            params=discriminator.parameters(),\n",
        "            lr=float(config.LR) * float(config.D2G_LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def process(self, images, edges, masks):\n",
        "        self.iteration += 1\n",
        "\n",
        "\n",
        "        # zero optimizers\n",
        "        self.gen_optimizer.zero_grad()\n",
        "        self.dis_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # process outputs\n",
        "        outputs = self(images, edges, masks)\n",
        "        gen_loss = 0\n",
        "        dis_loss = 0\n",
        "\n",
        "\n",
        "        # discriminator loss\n",
        "        dis_input_real = torch.cat((images, edges), dim=1)\n",
        "        dis_input_fake = torch.cat((images, outputs.detach()), dim=1)\n",
        "        #real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
        "        dis_real, dis_real_feat = self.discriminator(DiffAugment(dis_input_real, policy=policy))        # in: (grayscale(1) + edge(1))\n",
        "        dis_fake, dis_fake_feat = self.discriminator(DiffAugment(dis_input_fake, policy=policy))        # in: (grayscale(1) + edge(1))\n",
        "        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n",
        "        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n",
        "        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n",
        "\n",
        "\n",
        "        # generator adversarial loss\n",
        "        gen_input_fake = torch.cat((images, outputs), dim=1)\n",
        "        gen_fake, gen_fake_feat = self.discriminator(DiffAugment(gen_input_fake, policy=policy))         # in: (grayscale(1) + edge(1))\n",
        "        gen_gan_loss = self.adversarial_loss(gen_fake, True, False)\n",
        "        gen_loss += gen_gan_loss\n",
        "\n",
        "\n",
        "        # generator feature matching loss\n",
        "        gen_fm_loss = 0\n",
        "        for i in range(len(dis_real_feat)):\n",
        "            gen_fm_loss += self.l1_loss(gen_fake_feat[i], dis_real_feat[i].detach())\n",
        "        gen_fm_loss = gen_fm_loss * self.config.FM_LOSS_WEIGHT\n",
        "        gen_loss += gen_fm_loss\n",
        "\n",
        "\n",
        "        # create logs\n",
        "        logs = [\n",
        "            (\"l_d1\", dis_loss.item()),\n",
        "            (\"l_g1\", gen_gan_loss.item()),\n",
        "            (\"l_fm\", gen_fm_loss.item()),\n",
        "        ]\n",
        "\n",
        "        return outputs, gen_loss, dis_loss, logs\n",
        "\n",
        "    def forward(self, images, edges, masks):\n",
        "        edges_masked = (edges * (1 - masks))\n",
        "        images_masked = (images * (1 - masks)) + masks\n",
        "        inputs = torch.cat((images_masked, edges_masked, masks), dim=1)\n",
        "        outputs = self.generator(inputs)                                    # in: [grayscale(1) + edge(1) + mask(1)]\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, gen_loss=None, dis_loss=None):\n",
        "        if dis_loss is not None:\n",
        "            dis_loss.backward()\n",
        "        self.dis_optimizer.step()\n",
        "\n",
        "        if gen_loss is not None:\n",
        "            gen_loss.backward()\n",
        "        self.gen_optimizer.step()\n",
        "\n",
        "\n",
        "class InpaintingModel(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super(InpaintingModel, self).__init__('InpaintingModel', config)\n",
        "\n",
        "        # generator input: [rgb(3) + edge(1)]\n",
        "        # discriminator input: [rgb(3)]\n",
        "        generator = InpaintGenerator()\n",
        "        \"\"\"\n",
        "        if len(config.GPU) > 1:\n",
        "            generator = nn.DataParallel(generator, config.GPU)\n",
        "            discriminator = nn.DataParallel(discriminator , config.GPU)\n",
        "        \"\"\"\n",
        "        # original loss\n",
        "        l1_loss = nn.L1Loss()\n",
        "        perceptual_loss = PerceptualLoss()\n",
        "        style_loss = StyleLoss()\n",
        "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
        "\n",
        "        self.generator_loss = config.GENERATOR_LOSS\n",
        "\n",
        "        self.add_module('generator', generator)\n",
        "\n",
        "        if self.config.DISCRIMINATOR == 'default':\n",
        "          discriminator = Discriminator(in_channels=3, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
        "          self.add_module('discriminator', discriminator)\n",
        "\n",
        "        if self.config.DISCRIMINATOR == 'pixel':\n",
        "          _PixelDiscriminator = PixelDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "          self.add_module('PixelDiscriminator', _PixelDiscriminator)\n",
        "        \n",
        "        if self.config.DISCRIMINATOR == 'patch':\n",
        "          _NLayerDiscriminator = NLayerDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "          self.add_module('NLayerDiscriminator', _NLayerDiscriminator)\n",
        "\n",
        "        self.add_module('l1_loss', l1_loss)\n",
        "        self.add_module('perceptual_loss', perceptual_loss)\n",
        "        self.add_module('style_loss', style_loss)\n",
        "        self.add_module('adversarial_loss', adversarial_loss)\n",
        "\n",
        "        # new added loss\n",
        "        # CharbonnierLoss (L1) (already implemented?)\n",
        "        _CharbonnierLoss = CharbonnierLoss()\n",
        "        self.add_module('_CharbonnierLoss', _CharbonnierLoss)\n",
        "        # GANLoss (vanilla, lsgan, srpgan, nsgan, hinge, wgan-gp)\n",
        "        _GANLoss = GANLoss('vanilla', real_label_val=1.0, fake_label_val=0.0)\n",
        "        self.add_module('_GANLoss', _GANLoss)\n",
        "        # GradientPenaltyLoss\n",
        "        _GradientPenaltyLoss = GradientPenaltyLoss()\n",
        "        self.add_module('_GradientPenaltyLoss', _GradientPenaltyLoss)\n",
        "        # HFENLoss\n",
        "        #l_hfen_type = CharbonnierLoss() # nn.L1Loss(), nn.MSELoss(), CharbonnierLoss(), ElasticLoss(), RelativeL1(), L1CosineSim()\n",
        "        if self.config.HFEN_TYPE == 'L1':\n",
        "          l_hfen_type = nn.L1Loss()\n",
        "        if self.config.HFEN_TYPE == 'MSE': \n",
        "          l_hfen_type = nn.MSELoss()\n",
        "        if self.config.HFEN_TYPE == 'Charbonnier':\n",
        "          l_hfen_type = CharbonnierLoss()\n",
        "        if self.config.HFEN_TYPE == 'ElasticLoss':\n",
        "          l_hfen_type = ElasticLoss()\n",
        "        if self.config.HFEN_TYPE == 'RelativeL1':\n",
        "          l_hfen_type = RelativeL1()        \n",
        "        if self.config.HFEN_TYPE == 'L1CosineSim':\n",
        "          l_hfen_type = L1CosineSim()\n",
        "\n",
        "        _HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "        self.add_module('_HFENLoss', _HFENLoss)\n",
        "        # TVLoss\n",
        "        _TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "        self.add_module('_TVLoss', _TVLoss)\n",
        "        # GradientLoss\n",
        "        _GradientLoss = GradientLoss(loss_f = None, reduction='mean', gradientdir='2d')\n",
        "        self.add_module('_GradientLoss', _GradientLoss)\n",
        "        # ElasticLoss\n",
        "        _ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "        self.add_module('_ElasticLoss', _ElasticLoss)\n",
        "        # RelativeL1 (todo?)\n",
        "        _RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "        self.add_module('_RelativeL1', _RelativeL1)\n",
        "        # L1CosineSim\n",
        "        _L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "        self.add_module('_L1CosineSim', _L1CosineSim)\n",
        "        # ClipL1\n",
        "        _ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "        self.add_module('_ClipL1', _ClipL1)\n",
        "        # FFTloss\n",
        "        _FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "        self.add_module('_FFTloss', _FFTloss)\n",
        "        # OFLoss\n",
        "        _OFLoss = OFLoss()\n",
        "        self.add_module('_OFLoss', _OFLoss)\n",
        "        # ColorLoss (untested)\n",
        "        ds_f = torch.nn.AvgPool2d=((3, 3)) # kernel_size=5\n",
        "        _ColorLoss = ColorLoss(loss_f = torch.nn.L1Loss, reduction='mean', ds_f=ds_f)\n",
        "        self.add_module('_ColorLoss', _ColorLoss)\n",
        "        # GPLoss\n",
        "        _GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "        self.add_module('_GPLoss', _GPLoss)\n",
        "        # CPLoss (SPL_ComputeWithTrace, SPLoss)\n",
        "        _CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "        self.add_module('_CPLoss', _CPLoss)\n",
        "        # Contextual_Loss\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        _Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "        self.add_module('_Contextual_Loss', _Contextual_Loss)\n",
        "\n",
        "        \"\"\"\n",
        "        if self.config.DISCRIMINATOR == 'pixel':\n",
        "          pixel_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "          self.add_module('pixel_criterion', pixel_criterion)\n",
        "\n",
        "        if self.config.DISCRIMINATOR == 'patch':\n",
        "          patch_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "          self.add_module('patch_criterion', patch_criterion)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.config.DISCRIMINATOR_CALC == 'BCEWithLogitsLoss':\n",
        "          bce_criterion = nn.BCEWithLogitsLoss()\n",
        "          self.add_module('bce_criterion', bce_criterion)\n",
        "\n",
        "        if self.config.DISCRIMINATOR_CALC == 'MSELoss':\n",
        "          mse_criterion = nn.MSELoss()\n",
        "          self.add_module('mse_criterion', mse_criterion)\n",
        "\n",
        "        self.gen_optimizer = optim.Adam(\n",
        "            params=generator.parameters(),\n",
        "            lr=float(config.LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "        if self.config.DISCRIMINATOR == 'default':\n",
        "          self.dis_optimizer = optim.Adam(\n",
        "              params=discriminator.parameters(),\n",
        "              #params=self.PixelDiscriminator.parameters(),\n",
        "              lr=float(config.LR) * float(config.D2G_LR),\n",
        "              betas=(config.BETA1, config.BETA2)\n",
        "          )\n",
        "        if self.config.DISCRIMINATOR == 'pixel':\n",
        "          self.dis_optimizer = optim.Adam(\n",
        "              #params=discriminator.parameters(),\n",
        "              params=self.PixelDiscriminator.parameters(),\n",
        "              lr=float(config.LR) * float(config.D2G_LR),\n",
        "              betas=(config.BETA1, config.BETA2)\n",
        "          )\n",
        "\n",
        "      \n",
        "        if self.config.DISCRIMINATOR == 'patch':\n",
        "          self.dis_optimizer = optim.Adam(\n",
        "              #params=discriminator.parameters(),\n",
        "              params=self.NLayerDiscriminator.parameters(),\n",
        "              lr=float(config.LR) * float(config.D2G_LR),\n",
        "              betas=(config.BETA1, config.BETA2)\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.use_amp = config.USE_AMP\n",
        "\n",
        "    def process(self, images, edges, masks, mosaic_size=None):\n",
        "        self.iteration += 1\n",
        "\n",
        "        # zero optimizers\n",
        "        self.gen_optimizer.zero_grad()\n",
        "        self.dis_optimizer.zero_grad()\n",
        "\n",
        "        if(mosaic_size != None):\n",
        "          # resize image with random size. (256 currently hardcoded)\n",
        "          #mosaic_size = int(random.triangular(int(min(256*0.01, 256*0.01)), int(min(256*0.2, 256*0.2)), int(min(256*0.0625, 256*0.0625))))\n",
        "          images_mosaic = nnf.interpolate(images, size=(mosaic_size, mosaic_size), mode='nearest')\n",
        "          images_mosaic = nnf.interpolate(images_mosaic, size=(256, 256), mode='nearest')\n",
        "          images_mosaic = (images * (1 - masks).float()) + (images_mosaic * (masks).float())\n",
        "          outputs = self(images_mosaic, edges, masks)\n",
        "        else:\n",
        "          outputs = self(images, edges, masks)\n",
        "\n",
        "        if self.use_amp == 1:\n",
        "          with torch.cuda.amp.autocast(): \n",
        "            # process outputs\n",
        "            #outputs = self(images, edges, masks)\n",
        "            gen_loss = 0\n",
        "            dis_loss = 0\n",
        "\n",
        "\n",
        "            # discriminator loss\n",
        "            dis_input_real = images\n",
        "            dis_input_fake = outputs.detach()\n",
        "            #real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
        "\n",
        "            if self.config.DISCRIMINATOR == 'default':\n",
        "              dis_real, _ = self.discriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "              dis_fake, _ = self.discriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "            if self.config.DISCRIMINATOR == 'pixel':\n",
        "              dis_real = self.PixelDiscriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "              dis_fake = self.PixelDiscriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "            if self.config.DISCRIMINATOR == 'patch':\n",
        "              dis_real = self.NLayerDiscriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "              dis_fake = self.NLayerDiscriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "\n",
        "\n",
        "            if self.config.DISCRIMINATOR_CALC == 'BCEWithLogitsLoss':\n",
        "              dis_fake_loss = self.bce_criterion(dis_fake, torch.ones_like(dis_fake))\n",
        "              dis_real_loss = self.bce_criterion(dis_real, torch.zeros_like(dis_real))\n",
        "\n",
        "            if self.config.DISCRIMINATOR_CALC == 'MSELoss':\n",
        "              dis_fake_loss = self.mse_criterion(dis_fake, torch.ones_like(dis_fake))\n",
        "              dis_real_loss = self.mse_criterion(dis_real, torch.zeros_like(dis_real))\n",
        "\n",
        "\n",
        "            dis_loss += ((dis_real_loss * self.config.DISCRIMINATOR_REAL_LOSS_WEIGHT) + (dis_fake_loss * self.config.DISCRIMINATOR_FAKE_LOSS_WEIGHT)) / 2\n",
        "\n",
        "\n",
        "            # original generator loss\n",
        "            # generator adversarial loss\n",
        "            gen_input_fake = outputs\n",
        "            \n",
        "            if 'DEFAULT_GAN' in self.generator_loss:\n",
        "              if self.config.DISCRIMINATOR == 'default':\n",
        "                gen_fake, _ = self.discriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "              if self.config.DISCRIMINATOR == 'pixel':\n",
        "                gen_fake = self.PixelDiscriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "              if self.config.DISCRIMINATOR == 'patch':\n",
        "                gen_fake = self.NLayerDiscriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "\n",
        "              #gen_gan_loss = self.adversarial_loss(gen_fake, True, False) * self.config.INPAINT_ADV_LOSS_WEIGHT\n",
        "\n",
        "              if self.config.GENERATOR_CALC == 'BCEWithLogitsLoss':\n",
        "                gen_gan_loss = self.bce_criterion(gen_fake, torch.ones_like(gen_fake)) \n",
        "\n",
        "              if self.config.GENERATOR_CALC == 'MSELoss':\n",
        "                gen_gan_loss = self.mse_criterion(gen_fake, torch.ones_like(gen_fake))\n",
        "\n",
        "              gen_loss += gen_gan_loss * self.config.GENERATOR_CALC_WEIGHT\n",
        "\n",
        "            # generator l1 loss\n",
        "            if 'DEFAULT_L1' in self.generator_loss:\n",
        "              gen_l1_loss = self.l1_loss(outputs, images) * self.config.L1_LOSS_WEIGHT / torch.mean(masks)\n",
        "              gen_loss += gen_l1_loss\n",
        "\n",
        "            # generator perceptual loss\n",
        "            if 'Perceptual' in self.generator_loss:\n",
        "              gen_content_loss = self.perceptual_loss(outputs, images)\n",
        "              gen_content_loss = gen_content_loss * self.config.CONTENT_LOSS_WEIGHT\n",
        "              gen_loss += gen_content_loss\n",
        "\n",
        "            # generator style loss\n",
        "            if 'Style' in self.generator_loss:\n",
        "              gen_style_loss = self.style_loss(outputs * masks, images * masks)\n",
        "              gen_style_loss = gen_style_loss * self.config.STYLE_LOSS_WEIGHT\n",
        "              gen_loss += gen_style_loss\n",
        "\n",
        "\n",
        "            # new loss \n",
        "            # CharbonnierLoss (L1) (already implemented?)\n",
        "            if 'NEW_L1' in self.generator_loss:\n",
        "              gen_loss += self.config.L1_LOSS_WEIGHT * self._CharbonnierLoss(outputs, images)\n",
        "            \n",
        "            # GANLoss (vanilla, lsgan, srpgan, nsgan, hinge, wgan-gp)\n",
        "            if 'NEW_GAN' in self.generator_loss:\n",
        "              gen_loss += self.config.NEW_GAN_WEIGHT * self._GANLoss(outputs, images)\n",
        "            # GradientPenaltyLoss\n",
        "            #gen_loss += self._GradientPenaltyLoss(outputs, images, interp_crit) # not sure what interp_crit is\n",
        "            # HFENLoss\n",
        "            if 'HFEN' in self.generator_loss:\n",
        "              gen_loss += self.config.HFEN_WEIGHT * self._HFENLoss(outputs, images)\n",
        "            # TVLoss\n",
        "            if 'TV' in self.generator_loss:\n",
        "              gen_loss += self.config.TV_WEIGHT * self._TVLoss(outputs)\n",
        "            # GradientLoss\n",
        "            #gen_loss += self._GradientLoss(outputs, images) # TypeError: 'NoneType' object is not callable\n",
        "            # ElasticLoss\n",
        "            if 'ElasticLoss' in self.generator_loss:\n",
        "              gen_loss += self.config.ElasticLoss_WEIGHT * self._ElasticLoss(outputs, images)\n",
        "            # RelativeL1 (todo?)\n",
        "            if 'RelativeL1' in self.generator_loss:\n",
        "              gen_loss += self.config.RelativeL1_WEIGHT * self._RelativeL1(outputs, images)\n",
        "            # L1CosineSim\n",
        "            if 'L1CosineSim' in self.generator_loss:\n",
        "              gen_loss += self.config.L1CosineSim_WEIGHT * self._L1CosineSim(outputs, images)\n",
        "            # ClipL1\n",
        "            if 'ClipL1' in self.generator_loss:\n",
        "              gen_loss += self.config.ClipL1_WEIGHT * self._ClipL1(outputs, images)\n",
        "            # FFTloss\n",
        "            if 'FFT' in self.generator_loss:\n",
        "              gen_loss += self.config.FFT_WEIGHT * self._FFTloss(outputs, images)\n",
        "            # OFLoss\n",
        "            if 'OF' in self.generator_loss:\n",
        "              gen_loss += self.config.OF_WEIGHT * self._OFLoss(outputs)\n",
        "            # ColorLoss (untested)\n",
        "            #gen_loss += self._ColorLoss(outputs, images) # TypeError: 'NoneType' object is not callable\n",
        "            # GPLoss\n",
        "            if 'GP' in self.generator_loss:\n",
        "              gen_loss += self.config.GP_WEIGHT * self._GPLoss(outputs, images)\n",
        "            # CPLoss (SPL_ComputeWithTrace, SPLoss)\n",
        "            if 'CP' in self.generator_loss:\n",
        "              gen_loss += self.config.CP_WEIGHT * self._CPLoss(outputs, images)\n",
        "            # Contextual_Loss\n",
        "            if 'Contextual' in self.generator_loss:\n",
        "              gen_loss += self.config.Contextual_WEIGHT * self._Contextual_Loss(outputs, images)\n",
        "\n",
        "        else:\n",
        "          # process outputs\n",
        "          #outputs = self(images, edges, masks)\n",
        "          gen_loss = 0\n",
        "          dis_loss = 0\n",
        "\n",
        "\n",
        "          # discriminator loss\n",
        "          dis_input_real = images\n",
        "          dis_input_fake = outputs.detach()\n",
        "          #real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
        "\n",
        "          if self.config.DISCRIMINATOR == 'default':\n",
        "            dis_real, _ = self.discriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "            dis_fake, _ = self.discriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "          if self.config.DISCRIMINATOR == 'pixel':\n",
        "            dis_real = self.PixelDiscriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "            dis_fake = self.PixelDiscriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "          if self.config.DISCRIMINATOR == 'patch':\n",
        "            dis_real = self.NLayerDiscriminator(DiffAugment(dis_input_real, policy=policy))                    # in: [rgb(3)]\n",
        "            dis_fake = self.NLayerDiscriminator(DiffAugment(dis_input_fake, policy=policy))                    # in: [rgb(3)]\n",
        "\n",
        "\n",
        "\n",
        "          if self.config.DISCRIMINATOR_CALC == 'BCEWithLogitsLoss':\n",
        "            dis_fake_loss = self.bce_criterion(dis_fake, torch.ones_like(dis_fake))\n",
        "            dis_real_loss = self.bce_criterion(dis_real, torch.zeros_like(dis_real))\n",
        "\n",
        "          if self.config.DISCRIMINATOR_CALC == 'MSELoss':\n",
        "            dis_fake_loss = self.mse_criterion(dis_fake, torch.ones_like(dis_fake))\n",
        "            dis_real_loss = self.mse_criterion(dis_real, torch.zeros_like(dis_real))\n",
        "\n",
        "\n",
        "          dis_loss += ((dis_real_loss * self.config.DISCRIMINATOR_REAL_LOSS_WEIGHT) + (dis_fake_loss * self.config.DISCRIMINATOR_FAKE_LOSS_WEIGHT)) / 2\n",
        "\n",
        "\n",
        "          # original generator loss\n",
        "          # generator adversarial loss\n",
        "          gen_input_fake = outputs\n",
        "          \n",
        "          if 'DEFAULT_GAN' in self.generator_loss:\n",
        "            if self.config.DISCRIMINATOR == 'default':\n",
        "              gen_fake, _ = self.discriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "            if self.config.DISCRIMINATOR == 'pixel':\n",
        "              gen_fake = self.PixelDiscriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "            if self.config.DISCRIMINATOR == 'patch':\n",
        "              gen_fake = self.NLayerDiscriminator(DiffAugment(gen_input_fake, policy=policy))                  # in: [rgb(3)]\n",
        "\n",
        "            #gen_gan_loss = self.adversarial_loss(gen_fake, True, False) * self.config.INPAINT_ADV_LOSS_WEIGHT\n",
        "\n",
        "            if self.config.GENERATOR_CALC == 'BCEWithLogitsLoss':\n",
        "              gen_gan_loss = self.bce_criterion(gen_fake, torch.ones_like(gen_fake)) \n",
        "\n",
        "            if self.config.GENERATOR_CALC == 'MSELoss':\n",
        "              gen_gan_loss = self.mse_criterion(gen_fake, torch.ones_like(gen_fake))\n",
        "\n",
        "            gen_loss += gen_gan_loss * self.config.GENERATOR_CALC_WEIGHT\n",
        "\n",
        "          # generator l1 loss\n",
        "          if 'DEFAULT_L1' in self.generator_loss:\n",
        "            gen_l1_loss = self.l1_loss(outputs, images) * self.config.L1_LOSS_WEIGHT / torch.mean(masks)\n",
        "            gen_loss += gen_l1_loss\n",
        "\n",
        "          # generator perceptual loss\n",
        "          if 'Perceptual' in self.generator_loss:\n",
        "            gen_content_loss = self.perceptual_loss(outputs, images)\n",
        "            gen_content_loss = gen_content_loss * self.config.CONTENT_LOSS_WEIGHT\n",
        "            gen_loss += gen_content_loss\n",
        "\n",
        "          # generator style loss\n",
        "          if 'Style' in self.generator_loss:\n",
        "            gen_style_loss = self.style_loss(outputs * masks, images * masks)\n",
        "            gen_style_loss = gen_style_loss * self.config.STYLE_LOSS_WEIGHT\n",
        "            gen_loss += gen_style_loss\n",
        "\n",
        "\n",
        "          # new loss \n",
        "          # CharbonnierLoss (L1) (already implemented?)\n",
        "          if 'NEW_L1' in self.generator_loss:\n",
        "            gen_loss += self.config.L1_LOSS_WEIGHT * self._CharbonnierLoss(outputs, images)\n",
        "          \n",
        "          # GANLoss (vanilla, lsgan, srpgan, nsgan, hinge, wgan-gp)\n",
        "          if 'NEW_GAN' in self.generator_loss:\n",
        "            gen_loss += self.config.NEW_GAN_WEIGHT * self._GANLoss(outputs, images)\n",
        "          # GradientPenaltyLoss\n",
        "          #gen_loss += self._GradientPenaltyLoss(outputs, images, interp_crit) # not sure what interp_crit is\n",
        "          # HFENLoss\n",
        "          if 'HFEN' in self.generator_loss:\n",
        "            gen_loss += self.config.HFEN_WEIGHT * self._HFENLoss(outputs, images)\n",
        "          # TVLoss\n",
        "          if 'TV' in self.generator_loss:\n",
        "            gen_loss += self.config.TV_WEIGHT * self._TVLoss(outputs)\n",
        "          # GradientLoss\n",
        "          #gen_loss += self._GradientLoss(outputs, images) # TypeError: 'NoneType' object is not callable\n",
        "          # ElasticLoss\n",
        "          if 'ElasticLoss' in self.generator_loss:\n",
        "            gen_loss += self.config.ElasticLoss_WEIGHT * self._ElasticLoss(outputs, images)\n",
        "          # RelativeL1 (todo?)\n",
        "          if 'RelativeL1' in self.generator_loss:\n",
        "            gen_loss += self.config.RelativeL1_WEIGHT * self._RelativeL1(outputs, images)\n",
        "          # L1CosineSim\n",
        "          if 'L1CosineSim' in self.generator_loss:\n",
        "            gen_loss += self.config.L1CosineSim_WEIGHT * self._L1CosineSim(outputs, images)\n",
        "          # ClipL1\n",
        "          if 'ClipL1' in self.generator_loss:\n",
        "            gen_loss += self.config.ClipL1_WEIGHT * self._ClipL1(outputs, images)\n",
        "          # FFTloss\n",
        "          if 'FFT' in self.generator_loss:\n",
        "            gen_loss += self.config.FFT_WEIGHT * self._FFTloss(outputs, images)\n",
        "          # OFLoss\n",
        "          if 'OF' in self.generator_loss:\n",
        "            gen_loss += self.config.OF_WEIGHT * self._OFLoss(outputs)\n",
        "          # ColorLoss (untested)\n",
        "          #gen_loss += self._ColorLoss(outputs, images) # TypeError: 'NoneType' object is not callable\n",
        "          # GPLoss\n",
        "          if 'GP' in self.generator_loss:\n",
        "            gen_loss += self.config.GP_WEIGHT * self._GPLoss(outputs, images)\n",
        "          # CPLoss (SPL_ComputeWithTrace, SPLoss)\n",
        "          if 'CP' in self.generator_loss:\n",
        "            gen_loss += self.config.CP_WEIGHT * self._CPLoss(outputs, images)\n",
        "          # Contextual_Loss\n",
        "          if 'Contextual' in self.generator_loss:\n",
        "            gen_loss += self.config.Contextual_WEIGHT * self._Contextual_Loss(outputs, images)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        # create logs\n",
        "        logs = [\n",
        "            (\"l_d2\", dis_loss.item()),\n",
        "            (\"l_g2\", gen_gan_loss.item()),\n",
        "            (\"l_l1\", gen_l1_loss.item()),\n",
        "            (\"l_per\", gen_content_loss.item()),\n",
        "            (\"l_sty\", gen_style_loss.item()),\n",
        "        ]\n",
        "        \"\"\"\n",
        "        logs = [] # txt logs currently unsupported\n",
        "        return outputs, gen_loss, dis_loss, logs\n",
        "\n",
        "    def forward(self, images, edges, masks):\n",
        "        if (self.mosaic_test == 1):\n",
        "          # mosaic test\n",
        "          images_masked = images\n",
        "        else:\n",
        "          images_masked = (images * (1 - masks).float()) + masks\n",
        "\n",
        "        inputs = torch.cat((images_masked, edges), dim=1)\n",
        "        outputs = self.generator(inputs)                                    # in: [rgb(3) + edge(1)]\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, gen_loss=None, dis_loss=None):\n",
        "        #dis_loss.backward(retain_graph = True)\n",
        "        #gen_loss.backward()\n",
        "        scaler.scale(dis_loss).backward(retain_graph = True) \n",
        "        scaler.scale(gen_loss).backward() \n",
        "\n",
        "        #self.gen_optimizer.step()\n",
        "        #self.dis_optimizer.step()\n",
        "        scaler.step(self.gen_optimizer) \n",
        "        scaler.step(self.dis_optimizer) \n",
        "\n",
        "        scaler.update() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}