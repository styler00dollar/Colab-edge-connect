{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-edge-connect.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEkV-m5FEJoM",
        "colab_type": "text"
      },
      "source": [
        "# edge-connect with differentiable augmentation\n",
        "edge-connect: [knazeri/edge-connect](https://github.com/knazeri/edge-connect)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPLdOlecCkFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check gpu\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL5SvTn5BdBV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install\n",
        "!git clone https://github.com/knazeri/edge-connect.git\n",
        "%cd edge-connect\n",
        "!pip install -r requirements.txt\n",
        "#!bash ./scripts/download_model.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsHuihTpErN4",
        "colab_type": "text"
      },
      "source": [
        "# Test with pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW0V5LkRB4ej",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download models\n",
        "%cd /content/edge-connect\n",
        "!pip install gdown\n",
        "!mkdir checkpoint_places\n",
        "%cd checkpoint_places\n",
        "# /checkpoints broken\n",
        "# places\n",
        "!gdown --id 1gesVuuYMtlWSQRR2JE5eO0QZHskYRfqv\n",
        "!gdown --id 1_oYnmK7kppXqka9UUsHrZB4gWE4ouSgT\n",
        "!gdown --id 1M-r_ds4VZJnUqViDMofd4-Fy8-q2aeKJ\n",
        "!gdown --id 1G8lXquU3eREfs8KorFpFC8N4YmTQRksF\n",
        "%cd ..\n",
        "!mkdir checkpoint_celeba\n",
        "%cd checkpoint_celeba\n",
        "!gdown --id 1wy0pEaXTqmya2yeLwWFmTBf4ICexCdce\n",
        "!gdown --id 1hqZRjnqZBGnSTtGJRHXEvvdGVICUGa7u\n",
        "!gdown --id 17FemN4FAKpS5-8Dos582IrOiSCZNDOAO\n",
        "!gdown --id 15mH1ZHMf83q3woBHFELr_TptSRGc5g5j\n",
        "%cd ..\n",
        "!mkdir checkpoint_street\n",
        "%cd checkpoint_street\n",
        "!gdown --id 1ORF2uN4lB3F6YndPm1ny8VIDrsWQBwUS\n",
        "!gdown --id 1EwHK8YjcpO-X3xhmeo2dtqGvtY5vOMMj\n",
        "!gdown --id 1AWxB8AwTOrlOmAUho3IQQlmawtp3y8gZ\n",
        "!gdown --id 12Ua8oQwk0iLdYgrb08bqBhfyiBIumQEK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM4GOSz3CzJl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title dummy config\n",
        "%%writefile /content/edge-connect/config.yml\n",
        "MODE: 1             # 1: train, 2: test, 3: eval\n",
        "MODEL: 1            # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
        "MASK: 3             # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
        "EDGE: 1             # 1: canny, 2: external\n",
        "NMS: 1              # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
        "SEED: 10            # random seed\n",
        "GPU: [0]            # list of gpu ids\n",
        "DEBUG: 0            # turns on debugging mode\n",
        "VERBOSE: 0          # turns on verbose mode in the output console\n",
        "\n",
        "TRAIN_FLIST: ./datasets/places2_train.flist\n",
        "VAL_FLIST: ./datasets/places2_val.flist\n",
        "TEST_FLIST: ./datasets/places2_test.flist\n",
        "\n",
        "TRAIN_EDGE_FLIST: ./datasets/places2_edges_train.flist\n",
        "VAL_EDGE_FLIST: ./datasets/places2_edges_val.flist\n",
        "TEST_EDGE_FLIST: ./datasets/places2_edges_test.flist\n",
        "\n",
        "TRAIN_MASK_FLIST: ./datasets/masks_train.flist\n",
        "VAL_MASK_FLIST: ./datasets/masks_val.flist\n",
        "TEST_MASK_FLIST: ./datasets/masks_test.flist\n",
        "\n",
        "LR: 0.0001                    # learning rate\n",
        "D2G_LR: 0.1                   # discriminator/generator learning rate ratio\n",
        "BETA1: 0.0                    # adam optimizer beta1\n",
        "BETA2: 0.9                    # adam optimizer beta2\n",
        "BATCH_SIZE: 8                 # input batch size for training\n",
        "INPUT_SIZE: 256               # input image size for training 0 for original size\n",
        "SIGMA: 2                      # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
        "MAX_ITERS: 2e6                # maximum number of iterations to train the model\n",
        "\n",
        "EDGE_THRESHOLD: 0.5           # edge detection threshold\n",
        "L1_LOSS_WEIGHT: 1             # l1 loss weight\n",
        "FM_LOSS_WEIGHT: 10            # feature-matching loss weight\n",
        "STYLE_LOSS_WEIGHT: 250        # style loss weight\n",
        "CONTENT_LOSS_WEIGHT: 0.1      # perceptual loss weight\n",
        "INPAINT_ADV_LOSS_WEIGHT: 0.1  # adversarial loss weight\n",
        "\n",
        "GAN_LOSS: nsgan               # nsgan | lsgan | hinge\n",
        "GAN_POOL_SIZE: 0              # fake images pool size\n",
        "\n",
        "SAVE_INTERVAL: 1000           # how many iterations to wait before saving model (0: never)\n",
        "SAMPLE_INTERVAL: 1000         # how many iterations to wait before sampling (0: never)\n",
        "SAMPLE_SIZE: 12               # number of images to sample\n",
        "EVAL_INTERVAL: 0              # how many iterations to wait before model evaluation (0: never)\n",
        "LOG_INTERVAL: 10              # how many iterations to wait before logging training status (0: never)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29t2_4dwGVPM",
        "colab_type": "text"
      },
      "source": [
        "Currently default paths are ```/content/image.png``` and ```/content/mask.png```. Currently it's not supported that you change paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgnGR9NAUqJg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Image and mask need to be dividable by 4, this code does fix wrong images \n",
        "import cv2\n",
        "import numpy\n",
        "path_inpainting = '/content/image.png' #@param {type:\"string\"}\n",
        "path_mask = '/content/mask.png' #@param {type:\"string\"}\n",
        "image=cv2.imread(path_mask)\n",
        "image_size0 = numpy.floor(image.shape[0]/4)\n",
        "image_size1 = numpy.floor(image.shape[1]/4)\n",
        "image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "ret,image=cv2.threshold(image,254,255,cv2.THRESH_BINARY)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_mask, image)\n",
        "\n",
        "image=cv2.imread(path_inpainting)\n",
        "image = cv2.resize(image, (int(image_size1*4), int(image_size0*4)), cv2.INTER_NEAREST)\n",
        "cv2.imwrite(path_inpainting, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WHBATxfdl-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title print shape\n",
        "import cv2\n",
        "image = cv2.imread(path_inpainting)\n",
        "print(image.shape)\n",
        "image = cv2.imread(path_mask)\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvHUtBwBri8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test Inpainting (result will be ```image.png```, the same filename you used as input)\n",
        "%cd /content/edge-connect\n",
        "!python test.py \\\n",
        "  --model 3 \\\n",
        "  --checkpoints /content/edge-connect/checkpoint_places \\\n",
        "  --input /content/image.png \\\n",
        "  --mask /content/mask.png \\\n",
        "  --output /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ard4kblETXdL",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpN03bmG3Ja",
        "colab_type": "text"
      },
      "source": [
        "Interesting stuff:\n",
        "- New pytorch versions won't work. Pytorch 1.0 makes problems. Pytorch 1.1 seems to work fine.\n",
        "- The ```.tflist``` simply lists filepaths for images.\n",
        "- It supports blocks as inpainting method, but random/custom masks need to be manually downloaded and input with a ```.tflist``` as well. Two example datasets are linked in the original github.\n",
        "- [Model 4 is not recommended](https://github.com/knazeri/edge-connect/issues/144). You should probably use model 3.\n",
        "- [Resuming and using a model as pretrained is being done by simply starting training while the models are in the specified checkpoint path.](https://github.com/knazeri/edge-connect/issues/54)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAdscM1moO-C",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Training config\n",
        "%%writefile /content/training-checkpoints/config.yml\n",
        "MODE: 1             # 1: train, 2: test, 3: eval\n",
        "MODEL: 3            # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
        "MASK: 3             # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
        "EDGE: 1             # 1: canny, 2: external\n",
        "NMS: 1              # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
        "SEED: 10            # random seed\n",
        "GPU: [0]            # list of gpu ids\n",
        "DEBUG: 0            # turns on debugging mode\n",
        "VERBOSE: 0          # turns on verbose mode in the output console\n",
        "\n",
        "TRAIN_FLIST: /content/train/train.tflist\n",
        "VAL_FLIST: /content/val/val.tflist\n",
        "TEST_FLIST: /content/val/val.tflist\n",
        "\n",
        "TRAIN_EDGE_FLIST: ./datasets/places2_edges_train.flist\n",
        "VAL_EDGE_FLIST: ./datasets/places2_edges_val.flist\n",
        "TEST_EDGE_FLIST: ./datasets/places2_edges_test.flist\n",
        "\n",
        "TRAIN_MASK_FLIST: /content/mask_train/mask_train.tflist\n",
        "VAL_MASK_FLIST: /content/mask_val/mask_val.tflist\n",
        "TEST_MASK_FLIST: /content/mask_val/mask_val.tflist\n",
        "\n",
        "LR: 0.0001                    # learning rate\n",
        "D2G_LR: 0.1                   # discriminator/generator learning rate ratio\n",
        "BETA1: 0.0                    # adam optimizer beta1\n",
        "BETA2: 0.9                    # adam optimizer beta2\n",
        "BATCH_SIZE: 1                 # input batch size for training\n",
        "INPUT_SIZE: 256               # input image size for training 0 for original size\n",
        "SIGMA: 2                      # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
        "MAX_ITERS: 1000000                # maximum number of iterations to train the model\n",
        "\n",
        "EDGE_THRESHOLD: 0.5           # edge detection threshold\n",
        "L1_LOSS_WEIGHT: 1             # l1 loss weight\n",
        "FM_LOSS_WEIGHT: 10            # feature-matching loss weight\n",
        "STYLE_LOSS_WEIGHT: 250        # style loss weight\n",
        "CONTENT_LOSS_WEIGHT: 0.1      # perceptual loss weight\n",
        "INPAINT_ADV_LOSS_WEIGHT: 0.1  # adversarial loss weight\n",
        "\n",
        "GAN_LOSS: nsgan               # nsgan | lsgan | hinge\n",
        "GAN_POOL_SIZE: 0              # fake images pool size\n",
        "\n",
        "SAVE_INTERVAL: 200           # how many iterations to wait before saving model (0: never)\n",
        "SAMPLE_INTERVAL: 1000         # how many iterations to wait before sampling (0: never)\n",
        "SAMPLE_SIZE: 1               # number of images to sample\n",
        "EVAL_INTERVAL: 0              # how many iterations to wait before model evaluation (0: never)\n",
        "LOG_INTERVAL: 10              # how many iterations to wait before logging training status (0: never)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSPXmERCWpJ4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create empty folders\n",
        "!mkdir /content/training-checkpoints/\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/\n",
        "!mkdir /content/mask_train/\n",
        "!mkdir /content/mask_val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H--bAmNAJdms",
        "colab_type": "text"
      },
      "source": [
        "Input all your data.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuxf2YHlw7z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install miniconda and dependencies\n",
        "%cd /content/\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install pytorch==1.1 cudatoolkit torchvision -c pytorch -y\n",
        "%cd /content/edge-connect\n",
        "!pip install -r requirements.txt\n",
        "!conda install ipykernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znym_DLBCF6g",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title differentialbe augmentation (experimental)\n",
        "%%writefile /content/edge-connect/src/models.py\n",
        "\n",
        "\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout' \n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from .networks import InpaintGenerator, EdgeGenerator, Discriminator\n",
        "from .loss import AdversarialLoss, PerceptualLoss, StyleLoss\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, name, config):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.config = config\n",
        "        self.iteration = 0\n",
        "\n",
        "        self.gen_weights_path = os.path.join(config.PATH, name + '_gen.pth')\n",
        "        self.dis_weights_path = os.path.join(config.PATH, name + '_dis.pth')\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.exists(self.gen_weights_path):\n",
        "            print('Loading %s generator...' % self.name)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = torch.load(self.gen_weights_path)\n",
        "            else:\n",
        "                data = torch.load(self.gen_weights_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "            self.generator.load_state_dict(data['generator'])\n",
        "            self.iteration = data['iteration']\n",
        "\n",
        "        # load discriminator only when training\n",
        "        if self.config.MODE == 1 and os.path.exists(self.dis_weights_path):\n",
        "            print('Loading %s discriminator...' % self.name)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = torch.load(self.dis_weights_path)\n",
        "            else:\n",
        "                data = torch.load(self.dis_weights_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "            self.discriminator.load_state_dict(data['discriminator'])\n",
        "\n",
        "    def save(self):\n",
        "        print('\\nsaving %s...\\n' % self.name)\n",
        "        torch.save({\n",
        "            'iteration': self.iteration,\n",
        "            'generator': self.generator.state_dict()\n",
        "        }, self.gen_weights_path)\n",
        "\n",
        "        torch.save({\n",
        "            'discriminator': self.discriminator.state_dict()\n",
        "        }, self.dis_weights_path)\n",
        "\n",
        "\n",
        "class EdgeModel(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super(EdgeModel, self).__init__('EdgeModel', config)\n",
        "\n",
        "        # generator input: [grayscale(1) + edge(1) + mask(1)]\n",
        "        # discriminator input: (grayscale(1) + edge(1))\n",
        "        generator = EdgeGenerator(use_spectral_norm=True)\n",
        "        discriminator = Discriminator(in_channels=2, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
        "        if len(config.GPU) > 1:\n",
        "            generator = nn.DataParallel(generator, config.GPU)\n",
        "            discriminator = nn.DataParallel(discriminator, config.GPU)\n",
        "        l1_loss = nn.L1Loss()\n",
        "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
        "\n",
        "        self.add_module('generator', generator)\n",
        "        self.add_module('discriminator', discriminator)\n",
        "\n",
        "        self.add_module('l1_loss', l1_loss)\n",
        "        self.add_module('adversarial_loss', adversarial_loss)\n",
        "\n",
        "        self.gen_optimizer = optim.Adam(\n",
        "            params=generator.parameters(),\n",
        "            lr=float(config.LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "        self.dis_optimizer = optim.Adam(\n",
        "            params=discriminator.parameters(),\n",
        "            lr=float(config.LR) * float(config.D2G_LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "    def process(self, images, edges, masks):\n",
        "        self.iteration += 1\n",
        "\n",
        "\n",
        "        # zero optimizers\n",
        "        self.gen_optimizer.zero_grad()\n",
        "        self.dis_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # process outputs\n",
        "        outputs = self(images, edges, masks)\n",
        "        gen_loss = 0\n",
        "        dis_loss = 0\n",
        "\n",
        "\n",
        "        # discriminator loss\n",
        "        dis_input_real = torch.cat((images, edges), dim=1)\n",
        "        dis_input_fake = torch.cat((images, outputs.detach()), dim=1)\n",
        "        #real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
        "        dis_real, dis_real_feat = self.discriminator(DiffAugment(dis_input_real, policy=policy))        # in: (grayscale(1) + edge(1))\n",
        "        dis_fake, dis_fake_feat = self.discriminator(DiffAugment(dis_input_fake, policy=policy))        # in: (grayscale(1) + edge(1))\n",
        "        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n",
        "        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n",
        "        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n",
        "\n",
        "\n",
        "        # generator adversarial loss\n",
        "        gen_input_fake = torch.cat((images, outputs), dim=1)\n",
        "        gen_fake, gen_fake_feat = self.discriminator(DiffAugment(gen_input_fake, policy=policy))         # in: (grayscale(1) + edge(1))\n",
        "        gen_gan_loss = self.adversarial_loss(gen_fake, True, False)\n",
        "        gen_loss += gen_gan_loss\n",
        "\n",
        "\n",
        "        # generator feature matching loss\n",
        "        gen_fm_loss = 0\n",
        "        for i in range(len(dis_real_feat)):\n",
        "            gen_fm_loss += self.l1_loss(gen_fake_feat[i], dis_real_feat[i].detach())\n",
        "        gen_fm_loss = gen_fm_loss * self.config.FM_LOSS_WEIGHT\n",
        "        gen_loss += gen_fm_loss\n",
        "\n",
        "\n",
        "        # create logs\n",
        "        logs = [\n",
        "            (\"l_d1\", dis_loss.item()),\n",
        "            (\"l_g1\", gen_gan_loss.item()),\n",
        "            (\"l_fm\", gen_fm_loss.item()),\n",
        "        ]\n",
        "\n",
        "        return outputs, gen_loss, dis_loss, logs\n",
        "\n",
        "    def forward(self, images, edges, masks):\n",
        "        edges_masked = (edges * (1 - masks))\n",
        "        images_masked = (images * (1 - masks)) + masks\n",
        "        inputs = torch.cat((images_masked, edges_masked, masks), dim=1)\n",
        "        outputs = self.generator(inputs)                                    # in: [grayscale(1) + edge(1) + mask(1)]\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, gen_loss=None, dis_loss=None):\n",
        "        if dis_loss is not None:\n",
        "            dis_loss.backward()\n",
        "        self.dis_optimizer.step()\n",
        "\n",
        "        if gen_loss is not None:\n",
        "            gen_loss.backward()\n",
        "        self.gen_optimizer.step()\n",
        "\n",
        "\n",
        "class InpaintingModel(BaseModel):\n",
        "    def __init__(self, config):\n",
        "        super(InpaintingModel, self).__init__('InpaintingModel', config)\n",
        "\n",
        "        # generator input: [rgb(3) + edge(1)]\n",
        "        # discriminator input: [rgb(3)]\n",
        "        generator = InpaintGenerator()\n",
        "        discriminator = Discriminator(in_channels=3, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
        "        if len(config.GPU) > 1:\n",
        "            generator = nn.DataParallel(generator, config.GPU)\n",
        "            discriminator = nn.DataParallel(discriminator , config.GPU)\n",
        "\n",
        "        l1_loss = nn.L1Loss()\n",
        "        perceptual_loss = PerceptualLoss()\n",
        "        style_loss = StyleLoss()\n",
        "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
        "\n",
        "        self.add_module('generator', generator)\n",
        "        self.add_module('discriminator', discriminator)\n",
        "\n",
        "        self.add_module('l1_loss', l1_loss)\n",
        "        self.add_module('perceptual_loss', perceptual_loss)\n",
        "        self.add_module('style_loss', style_loss)\n",
        "        self.add_module('adversarial_loss', adversarial_loss)\n",
        "\n",
        "        self.gen_optimizer = optim.Adam(\n",
        "            params=generator.parameters(),\n",
        "            lr=float(config.LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "        self.dis_optimizer = optim.Adam(\n",
        "            params=discriminator.parameters(),\n",
        "            lr=float(config.LR) * float(config.D2G_LR),\n",
        "            betas=(config.BETA1, config.BETA2)\n",
        "        )\n",
        "\n",
        "    def process(self, images, edges, masks):\n",
        "        self.iteration += 1\n",
        "\n",
        "        # zero optimizers\n",
        "        self.gen_optimizer.zero_grad()\n",
        "        self.dis_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # process outputs\n",
        "        outputs = self(images, edges, masks)\n",
        "        gen_loss = 0\n",
        "        dis_loss = 0\n",
        "\n",
        "\n",
        "        # discriminator loss\n",
        "        dis_input_real = images\n",
        "        dis_input_fake = outputs.detach()\n",
        "        dis_real, _ = self.discriminator(dis_input_real)                    # in: [rgb(3)]\n",
        "        dis_fake, _ = self.discriminator(dis_input_fake)                    # in: [rgb(3)]\n",
        "        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n",
        "        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n",
        "        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n",
        "\n",
        "\n",
        "        # generator adversarial loss\n",
        "        gen_input_fake = outputs\n",
        "        gen_fake, _ = self.discriminator(gen_input_fake)                    # in: [rgb(3)]\n",
        "        gen_gan_loss = self.adversarial_loss(gen_fake, True, False) * self.config.INPAINT_ADV_LOSS_WEIGHT\n",
        "        gen_loss += gen_gan_loss\n",
        "\n",
        "\n",
        "        # generator l1 loss\n",
        "        gen_l1_loss = self.l1_loss(outputs, images) * self.config.L1_LOSS_WEIGHT / torch.mean(masks)\n",
        "        gen_loss += gen_l1_loss\n",
        "\n",
        "\n",
        "        # generator perceptual loss\n",
        "        gen_content_loss = self.perceptual_loss(outputs, images)\n",
        "        gen_content_loss = gen_content_loss * self.config.CONTENT_LOSS_WEIGHT\n",
        "        gen_loss += gen_content_loss\n",
        "\n",
        "\n",
        "        # generator style loss\n",
        "        gen_style_loss = self.style_loss(outputs * masks, images * masks)\n",
        "        gen_style_loss = gen_style_loss * self.config.STYLE_LOSS_WEIGHT\n",
        "        gen_loss += gen_style_loss\n",
        "\n",
        "\n",
        "        # create logs\n",
        "        logs = [\n",
        "            (\"l_d2\", dis_loss.item()),\n",
        "            (\"l_g2\", gen_gan_loss.item()),\n",
        "            (\"l_l1\", gen_l1_loss.item()),\n",
        "            (\"l_per\", gen_content_loss.item()),\n",
        "            (\"l_sty\", gen_style_loss.item()),\n",
        "        ]\n",
        "\n",
        "        return outputs, gen_loss, dis_loss, logs\n",
        "\n",
        "    def forward(self, images, edges, masks):\n",
        "        images_masked = (images * (1 - masks).float()) + masks\n",
        "        inputs = torch.cat((images_masked, edges), dim=1)\n",
        "        outputs = self.generator(inputs)                                    # in: [rgb(3) + edge(1)]\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, gen_loss=None, dis_loss=None):\n",
        "        dis_loss.backward()\n",
        "        self.dis_optimizer.step()\n",
        "\n",
        "        gen_loss.backward()\n",
        "        self.gen_optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUxqJC30WlZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "%cd /content/edge-connect\n",
        "!python train.py --model 3 --checkpoints /content/training-checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InsJhcpA6Bhc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test model\n",
        "%cd /content/edge-connect\n",
        "!python test.py \\\n",
        "  --model 3 \\\n",
        "  --checkpoints /content/training-checkpoints \\\n",
        "  --input /content/image.jpg \\\n",
        "  --mask /content/mask.png \\\n",
        "  --output /content/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}